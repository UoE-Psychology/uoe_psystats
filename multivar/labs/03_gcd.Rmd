```{r, echo=FALSE}
HIDDEN_SOLS=TRUE
```

# Growth Curve Analysis

`r msmbstyle::question_begin()`
Load the `tidyverse`, `lme4` and `effects` packages, and some useful functions from Dan for getting p-values and coding polynomials.   
  
The `source()` function basically takes in R code and evaluates it. You can look at Dan's scripts online at the URLs ([here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R) and [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R)), but sourcing them will read them into your environment. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE, toggle=FALSE)`
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(lme4)
library(effects)
source('https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R')
source("https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R")
```
`r msmbstyle::solution_end()`

## Exercise 1
<div class="red">
`Az`: Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer's disease on three different kinds of tasks - Memory, complex ADL, and simple ADL. 
</div>

`r msmbstyle::question_begin()`
Read the data in to R from the following url: [https://edin.ac/35Njwpl](https://edin.ac/35Njwpl)
. The data is in .rda format. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
load("https://edin.ac/35Njwpl")
summary(Az)
```
`r msmbstyle::solution_end()`

* Plot the observed data
* (Why are natural polynomials more useful for these data?)
* Prep the data for GCA
* Fit the GCA model(s)
* Interpret the results: 
    + Which terms show significant effects of experimental factors? 
    + To what extent do model comparisons and the parameter-specific p-values yield the same results?
* Plot observed and model fit data

`r msmbstyle::question_begin()`
Plot the observed data
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + 
  stat_summary(fun.data=mean_se, geom="ribbon", color=NA, alpha=0.5) +
  stat_summary(fun.y=mean, geom="line")
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`
Why are natural polynomials more useful for these data?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
Because it's useful to know whether there are task differences at the starting baseline point
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Fit the GCA models
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
# prep for analysis
Az <- code_poly(Az, predictor="Time", poly.order=2, orthogonal=F, draw.poly = F)

# fit the full model incrementally
m.base <- lmer(Performance ~ (poly1 + poly2) +
                 (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
               data=Az, REML=F)
m.0 <- lmer(Performance ~ (poly1 + poly2) + Task +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.1 <- lmer(Performance ~ poly1*Task + poly2 +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.Az.full <- lmer(Performance ~ (poly1 + poly2)*Task + 
                  (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), 
                data=Az, REML=F)
anova(m.base, m.0, m.1, m.Az.full)
```
Get p-values:
```{r warning=FALSE, message=FALSE}
get_pvalues(m.Az.full)
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Interpret the results: 
`r msmbstyle::question_begin()`
Which terms show significant effects of experimental factors? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
+ Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials)
+ Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster.
+ Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?)
`r msmbstyle::solution_end()`
`r msmbstyle::question_begin()`
To what extent do model comparisons and the parameter-specific p-values yield the same results?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
jk insert
`r msmbstyle::solution_end()`
`r msmbstyle::question_end()` 

`r msmbstyle::question_begin()`
Plot model fit
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r fig.width=6, fig.height=4}
ggplot(Az, aes(Time, Performance, color=Task)) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(fun.y=mean, geom="line", aes(y=fitted(m.Az.full)))
```
`r msmbstyle::solution_end()`


## Exercise 2: Logistic GCA
<div class="red">
Re-analyze `TargetFix` data using logistic GCA
</div>



