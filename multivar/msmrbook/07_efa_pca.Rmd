```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
```

# EFA and PCA  

### Packages {-}  

+ psych  
+ GPArotation  
+ car  
+ GGally (optional)

### Lecture Slides {-}  

+ Coming soon  

### Background & Reading {-}  
  
+ Coming soon  

  
## Today's Exercises

<div class="noteBox"> 
#### Background {-}  

A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:  

1. Stealing
1. Lying
1. Skipping school
1. Vandalism
1. Breaking curfew
1. Threatening others
1. Bullying
1. Spreading malicious rumours
1. Using a weapon 
1. Fighting

Your task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales 
</div>

`r msmbstyle::question_begin(header="&#x25BA; Question 1")`
Load the `psych` package and read in the dataset ‘Conduct_problems.csv’.  
The first column is clearly an ID column, and it is easiest just to discard this for when we are doing factor analysis.  
  
Create a correlation matrix for *the items*.  
Inspect the items to check their suitability for exploratory factor analysis.   
  
+ You can use a function such as `corr.test(df)` from the psych package to create the correlation matrix.  
+ You can check the factorability of the correlation matrix using `KMO(df)`.  
+ You can check linearity of relations using `scatterplotMatrix(df)` (from the `car` package). If you add the argument `diagonal=histogram`  
+ You can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). 
+ You can do the same using the `ggpairs` function from the `GGally` package.
  
*NOTE. df=dataframe*
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
library(psych)
df <- read.csv("../labs/Conduct_problems.csv")

# discard the first column
df <- df[,-1]

corr.test(df)  

KMO(df)  

car::scatterplotMatrix(df)
```
or alternatively. 
```{r message=FALSE}
library(GGally)
ggpairs(data=df, diag=list(continuous="density"), axisLabels="show")
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 2")`
How many dimensions should be retained?   
  
Use a scree plot, parallel analysis, and MAP test to guide you.   
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can use `fa.parallel(df)` to conduct both parallel analysis and view a scree plot.   
```{r}
fa.parallel(df)
```
In this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors.  
  
We can conduct the MAP test using `vss(df)`.
```{r}
vss(df)
```
The MAP test suggests retaining 2 factors.  
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 3")`
Having decided how many dimensions to retain in the previous question, conduct an EFA to extract this many factors, using a suitable rotation and extraction method.  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can use the `fa()` function from the `psych` package, for example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method.  
```{r}
conduct_efa <- fa(df, nfactors=2, rotate='oblimin', fm='minres')
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 4")`
Inspect the loadings and give the factors you extracted labels based on the patterns of loadings.  
  
Look back to the description of the items, and suggest a name for you factors  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
You can inspect the loadings using:
```{r}
conduct_efa$loadings
```
We can see that the first five items have high loadings for one factor and the second five items have high loadings for the other.  
  
The first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 5")`
How correlated are your factors?  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can inspect the factor correlations (if we used an oblique rotation) using:
```{r}
conduct_efa$Phi
```
We can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. 
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header="&#x25BA; Question 6")`
Using the same data, conduct a PCA using the `principal()` function.  
  
What differences do you notice compared to your EFA?  
  
Do you think a PCA or an EFA is more appropriate in this particular case?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can use:
```{r}
principal(df, nfactors=2)
```
We can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low.  
  
In terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data.
`r msmbstyle::solution_end()`
