```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
```

# Basic Multilevel Regression 

We're going to start using multilevel modelling. There are a whole lot of different names people use for this sort of methodology (hierarchical linear models, linear mixed-effect models, mixed models, nested data models, random coefficient, random-effects models, random parameter models... and so on).   
What the idea boils down to is that **model parameters vary at more than one level.**  

### Packages {-}  

+ lme4  

### Lecture Slides {-}  

+ Coming soon  

### Background & Reading {-}  
  
+ [Winter, 2013](https://arxiv.org/abs/1308.5499)  
+ [Brauer & Curtin, 2018](http://dx.doi.org/10.1037/met0000159),  [(pdf)](https://dionysus.psych.wisc.edu/LabPubs/BrauerM2018a.pdf)  
+ [Luke, 2017](https://doi.org/10.3758/s13428-016-0809-y)  

  
## LME4  

We're going to use the `lme4` package, and specifically the functions `lmer()` and `glmer()`. 
"(g)lmer" here stands for "(generalised) linear mixed effects regression". 
<br>
You will have seen some use of these functions in the lectures. The broad syntax is:  
<br>
<center>lmer(*formula*, REML = *logical*, data = *dataframe*)</center>    
<br>
The *formula* bit is similar to what we did with a standard linear model (`lm()`) in that it has the **outcome ~ explanatory variables** structure. However, we now have the addition of the random effect terms, specified in parenthesis with the `|` operator separating parameters on the LHS and a grouping factor on the RHS. Below are a selection of different formulas for specifying different random effect structures.   
<br>  
<br>
  
|  Formula|  Alternative|  Meaning|
|--------:|------------:|--------:|
|  $\text{(1 | g)}$|  $\text{1 + (1 | g)}$|  Random intercept with fixed mean|
|  $\text{0 + offset(o) + (1 | g)}$|  $\text{-1 + offset(o) + (1 | g)}$|  Random intercept with *a priori* means|
|  $\text{(1 | g1/g2)}$|  $\text{(1 | g1) + (1 | g1:g2)}$|  Intercept varying among $g1$ and $g2$ within $g1$|
|  $\text{(1 | g1) + (1 | g2)}$|  $\text{1 + (1 | g1) + (1 | g2)}$|  Intercept varying among $g1$ and $g2$|
|  $\text{x + (x | g)}$|  $\text{1 + x + (1 + x | g)}$|  Correlated random intercept and slope|
|  $\text{x + (x || g)}$|  $\text{1 + x + (x | g) + (0 + x | g)}$|  Uncorrelated random intercept and slope|
**Table 1:** Examples of the right-hand-sides of mixed effects model formulas. $g$, $g1$, $g2$ are grouping factors, covariats and *a priori* known offsets are $x$ and $o$.

<div class="red">
##### Errors and warnings? {-}  
For large datasets and/or complex models (lots of random-effects terms), it is quite common to get a *convergence warning*.  There are lots of different ways to [deal with these](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html) (to try to rule out hypotheses about what is causing them).  
  
For now, if `lmer()` gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add `control = lmerControl(optimizer = "bobyqa")` when you run your model.
</div>


`r msmbstyle::question_begin()`
Load the `tidyverse`, `lme4` and `effects` packages (install them if you haven't already).    
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(lme4)
library(effects)
```
`r msmbstyle::solution_end()`

### Exercise 1  
County-level suicide rate data from Public Health England (PHE) is available at [https://edin.ac/36xdhas](https://edin.ac/36xdhas) and covers the period from 2001 to 2016. It contains information for a number of different indicators over this period for a selection of counties in England.  

`r msmbstyle::question_begin()`
Using multilevel regression, study the following:  
  
* Did the regions differ in their baseline (2001) suicide rates?  
* Did the regions differ in ther slopes of change of suidice rate?  
* Make a plot of the fitted values from the model  

Steps:  

1. First, get acquainted with the data. It contains various different indicators but we're only interested in one (suicide rates), so you might want to `filter()` the data, using the skills you learnt last week.   
1. You may also want to center your data on the year 2001, so that the intercept (e.g., at time 0) in your models is interpretable.   
1. Then think about what test you can do to answer the questions above.        
1. Lastly, passing your model to the `effect()` function (from the `effects` package) can give you all the data you need to construct your plot.

**Hint:** Try comparing models with and without different baseline/slopes of suicide rates for each region. Think also about the grouping of the data points, and how this is represented in your random effect structure.    
  
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
#### Loading {-}  
```{r}
load(url("https://edin.ac/36xdhas")) #load Public Health England data

unique(mh_phe[, 1:2]) #check list of mental health indicators: suicide is 41001

# select data and shift Year variable so baseline year (2001) is 0
suicide_dat <- filter(mh_phe, IndicatorID == 41001) %>%
  mutate(Time = Year - 2001)
```

#### Modelling {-}
```{r}
# base model: just change over time
m <- lmer(Value ~ Time + (Time | County), 
          data = suicide_dat, REML = F, control = lmerControl(optimizer = "bobyqa"))
# add baseline differences between regions
m.0 <- lmer(Value ~ Time + Region + (Time | County), 
            data = suicide_dat, REML = F, control = lmerControl(optimizer = "bobyqa"))
# add slope differences between regions
m.1 <- lmer(Value ~ Time * Region + (Time | County), 
            data = suicide_dat, REML = F, control = lmerControl(optimizer = "bobyqa"))

# compare models
anova(m, m.0, m.1)
```
  
It looks like regions differ in baseline suicide rate (addition of `Region` predictor in `m.0`) and in slope of change (addition of interaction in `m.1`)  

#### Plotting {-}  
```{r}
# you can extract the fitted values using the effect() function from the effects package.
ef <- as.data.frame(effect("Time:Region", m.1))
ggplot(ef, aes(Time, fit, color=Region)) + 
  geom_line() +
  theme_bw() + scale_color_brewer(palette = "Set1") +
  labs(y="Fitted values")
```
`r msmbstyle::solution_end()`

### Exercise 2  
The weight maintenance data (`WeightMaintain3`), a made-up data set based on Lowe et al. (2014, Obesity, 22, 94-100), contains information on overweight participants who completed a 12-week weight loss program, and were then randomly assigned to one of three weight maintenance conditions:

* None (Control)  
* MR (meal replacements): use MR to replace one meal and snack per day  
* ED (energy density intervention): book and educational materials on purchasing and preparing foods lower in ED (reducing fat content and/or increasing water content of foods)  

Weight was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post.  

```{r}
load(url("https://edin.ac/2tFIedK"))
summary(WeightMaintain3)
```

`r msmbstyle::question_begin()`
Overall, did the participants maintain their weight loss or did their weights change?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
m.null <- lmer(WeightChange ~ 1 + (Assessment | ID), data=WeightMaintain3, REML=F)
m.base <- lmer(WeightChange ~ Assessment + (Assessment | ID), data=WeightMaintain3, REML=F)
anova(m.null, m.base)
```
Weights changed over course of assessment period: $\chi^2(1)=56.5, p << 0.0001$
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin()`
Did the groups differ in baseline weight change and rate of weight gain (non-maintenance)?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
m.int <- lmer(WeightChange ~ Assessment + Condition + (Assessment | ID), 
              data=WeightMaintain3, REML=F, control = lmerControl(optimizer = "bobyqa"))
m.full <- lmer(WeightChange ~ Assessment*Condition + (Assessment | ID), 
               data=WeightMaintain3, REML=F, control = lmerControl(optimizer = "bobyqa"))
anova(m.null, m.base, m.int, m.full)
```
Yes: 

*  Baseline: $\chi^2(2)=9.4, p < 0.01$
*  Slope: $\chi^2(2)=40.4, p << 0.0001$

Note: `m.int` is difficult to interpret in light of the massive effect on slope

```{r}
coef(summary(m.full))
```

Compared to no intervention, weight (re)gain was 1.75 lbs/year slower for the ED intervention and 0.84 lbs/year slower for the MR intervention.

Note that baseline weight difference parameters are not significantly different from 0.
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin()`
Make a graph of the model fit *and* the observed data
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
There are lots of ways you can do this. 
  
1) Using the `effect()` function again (and then adding the means and SEs from the original data):  
```{r}
ef <- as.data.frame(effect("Assessment:Condition", m.full))
ggplot(ef, aes(Assessment, fit, color=Condition)) + 
  geom_line() +
  stat_summary(data=WeightMaintain3, aes(y=WeightChange), 
               fun.data=mean_se, geom="pointrange", size=1) +
  theme_bw() + scale_color_brewer(palette = "Set1")
```
2) Using the `fitted()` function to extract and plot fitted values from the model:  
```{r}
ggplot(WeightMaintain3, aes(Assessment, WeightChange, color=Condition)) + 
  stat_summary(fun.data=mean_se, geom="pointrange", size=1) + 
  stat_summary(aes(y=fitted(m.full)), fun=mean, geom="line") + 
  theme_bw(base_size=12) + scale_color_manual(values=c("black", "red", "blue"))
```

3) Or, alternatively, using `fortify()`:
```{r}
ggplot(fortify(m.full), aes(Assessment, WeightChange, color=Condition)) + 
  stat_summary(fun.data=mean_se, geom="pointrange", size=1) + 
  stat_summary(aes(y=.fitted), fun=mean, geom="line") + 
  theme_bw(base_size=12) + scale_color_manual(values=c("black", "red", "blue"))
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin()`
Examine the parameter estimates and interpret them (i.e., what does each parameter represent?)
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
round(coef(summary(m.full)), 3)
```

* `(Intercept)` ==> baseline weight change in None group
* `Assessment`  ==> slope of weight change in None group
* `ConditionED` ==> baseline weight change in ED group relative to None group
* `ConditionMR` ==> baseline weight change in MR group relative to None group
* `Assessment:ConditionED`  ==> slope of weight change in ED group relative to None group
* `Assessment:ConditionMR`  ==> slope of weight change in MR groups relative to None group
`r msmbstyle::solution_end()`


## Logistic MLR

### Exercise 3  

```{r}
load(url("https://edin.ac/2QwG7SG"))
```
In the `nwl` data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. 

Compare the two groups (those with anterior vs. posterior lesions) with respect to their responses. 
**Tip:** Remember that you can use `cbind()` to specify the numbers of successes and failures as the outcome in a *binomial* regression (and remember to specify the *family* argument in `glmer()`).  

`r msmbstyle::question_begin()`
Is the learning rate (training data) different between these two groups?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
m.base <- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), 
                data = filter(nwl, block < 8, !is.na(lesion_location)),
                family=binomial)
m.loc0 <- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), 
                data=filter(nwl, block < 8, !is.na(lesion_location)),
                family=binomial)
m.loc1 <- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), 
                data=filter(nwl, block < 8, !is.na(lesion_location)),
                family=binomial)
#summary(m.loc1)
anova(m.base, m.loc0, m.loc1, test="Chisq")
```
No significant difference in learning rate between groups: $\chi^2(2)=2.2, p = 0.138$
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin()`
Does their follow-up test performance differ, and does their retention from immediate to follow-up test differ?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
m.recall.loc <- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID), 
                  data=filter(nwl, block > 7, !is.na(lesion_location)),
                  family="binomial")
summary(m.recall.loc)
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin()`
**Extra:** Create a visualisation showing the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up)
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
ggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, 
                                            color=lesion_location, 
                                            shape=lesion_location)) +
  #geom_line(aes(group=ID),alpha=.2) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(data=filter(nwl, !is.na(lesion_location), block <= 7), 
                           fun=mean, geom="line") + 
  geom_hline(yintercept=0.5, linetype="dashed") + 
  geom_vline(xintercept=c(7.5, 8.5), linetype="dashed") + 
  scale_x_continuous(breaks=1:9, labels=c(1:7, "Test", "Follow-Up")) + 
  theme_bw(base_size=10) + 
  labs(x="Block", y="Proportion Correct", shape="Lesion\nLocation", color="Lesion\nLocation")
```
`r msmbstyle::solution_end()`
