```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
```

# Growth Curve Analysis

### Packages {-}  

+ lme4  

### Lecture Slides {-}  

+ Coming soon  

### Background & Reading {-}  
  
+ Coming soon  
  
## Introduction    

`r msmbstyle::question_begin()`
Load the `tidyverse`, `lme4` and `effects` packages, and some useful functions from Dan for getting p-values and coding polynomials.   
  
The `source()` function basically takes in R code and evaluates it. You can look at Dan's scripts online at the URLs ([here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R) and [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R)), but sourcing them will read them into your environment. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE, toggle=FALSE)`
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(lme4)
library(effects)
source('https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R')
source("https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R")
```
`r msmbstyle::solution_end()`

## Exercise 1
<div class="red">
#### The task {-}  

Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer's disease on three different kinds of tasks - Memory, complex ADL, and simple ADL. 
</div>

`r msmbstyle::question_begin()`
Read the data in to R from the following url: [https://edin.ac/35Njwpl](https://edin.ac/35Njwpl)
. The data is in .rda format. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
load(url("https://edin.ac/35Njwpl"))
summary(Az)
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Plot the observed data (the performance over time for each type of task).
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + 
  stat_summary(fun.data=mean_se, geom="ribbon", color=NA, alpha=0.5) +
  stat_summary(fun=mean, geom="line")
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`
Why are natural polynomials more useful for these data?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
Because it's useful to know whether there are task differences at the starting baseline point
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Fit the GCA model(s).

Steps required:  

1. Add 1st and 2nd order natural polynomials to the data using the `code_poly()` function. 
1. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested?
1. Create a new model with a fixed effect of Task
1. Create a new model in which performance varies linearly over time between Task type.
1. Create a new model in which linear and quadratic performance over time varies between Task type.
1. Run model comparisons.

`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
# prep for analysis
Az <- code_poly(Az, predictor="Time", poly.order=2, orthogonal=F, draw.poly = F)

# fit the full model incrementally
m.base <- lmer(Performance ~ (poly1 + poly2) +
                 (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
               data=Az, REML=F)
m.0 <- lmer(Performance ~ (poly1 + poly2) + Task +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.1 <- lmer(Performance ~ poly1*Task + poly2 +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.Az.full <- lmer(Performance ~ (poly1 + poly2)*Task + 
                  (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), 
                data=Az, REML=F)
anova(m.base, m.0, m.1, m.Az.full)
```
Get p-values for your full model:
```{r warning=FALSE, message=FALSE}
get_pvalues(m.Az.full)
```
`r msmbstyle::solution_end()`

#### Interpret the results of your full model
`r msmbstyle::question_begin()`
Look at the `summary()` of your full model, and try using the `get_pvalues()` function on it.  
  
Which terms show significant effects of experimental factors? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
+ Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials)
+ Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster.
+ Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?)
`r msmbstyle::solution_end()`
`r msmbstyle::question_begin()`
To what extent do model comparisons and the parameter-specific p-values yield the same results?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
Model comparisons suggest:
+ Linear slopes are different: $\chi^2(2)=30.56, p << 0.0001$ (comparison `m.0` and `m.1` above).
+ Quadratic terms is different: $\chi^2(2)=172.36, p << 0.0001$ (comparison `m.1` and `m.Az.full` above).


*Note:* We can't investigate the intercept difference via the model comparisons above. Comparison between `m.base` and `m.0` indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0).
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
Plot model fit
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r fig.width=6, fig.height=4}
ggplot(Az, aes(Time, Performance, color=Task)) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(fun=mean, geom="line", aes(y=fitted(m.Az.full)))
```
`r msmbstyle::solution_end()`


## Exercise 2: Logistic GCA
<div class="red">
Re-analyze `TargetFix` data using logistic GCA. 

The data (.rda format) is available at [https://edin.ac/2TieJK0](https://edin.ac/2TieJK0)
</div>

## Exercise 2: Solution
```{r}
load(url("https://edin.ac/2TieJK0"))
#make 3rd-order orth poly
TargetFix <- code_poly(TargetFix, predictor="timeBin", poly.order=3, draw.poly=F)
# fit logisitc GCA model
m.log <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 | Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log)
```

Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them:
```{r}
m.log_zc <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 || Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log_zc)
```

Plot model fit

```{r fig.height=4, fig.width=6}
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  stat_summary(aes(y=fitted(m.log)), fun=mean, geom="line") +
  stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom="line", linetype="dashed") +
  theme_bw() + expand_limits(y=c(0,1)) + 
  labs(y="Fixation Proportion", x="Time since word onset (ms)")
```


