```{r, echo=FALSE}
HIDDEN_SOLS=TRUE
```

# MLR for longitudinal data (growth curve analysis)

### Packages {-}  

+ lme4  
+ tidyverse  
+ effects  

We will also be needing to access some useful functions from Dan for getting p-values and coding polynomials.   
<br>
The `source()` function basically takes in R code and evaluates it. You can download R scripts with Dan's code [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R) and [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R).   
However, you can also source them directly from the URLs, and read them into your environment:    
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(lme4)
library(effects)
source('https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R')
source("https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R")
```

### Lecture Slides {-}  

The lecture slides can be accessed [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/Lecture03_MLRforLDA.html). 
The data for the lecture can be found at [https://edin.ac/2TieJK0](https://edin.ac/2TieJK0).  

### Background & Reading {-}  
  
+ [Curran et al., 2010](https://doi.org/10.1080/15248371003699969)  
+ [Winter & Wieling, 2016](https://doi.org/10.1093/jole/lzv003)  
  
## Introduction    

This week we are going to look at how we might use MLR to study longitudinal data. That is, data in which repeated measurements are taken over a continuous domain, with the potential for observations to be unevenly spaced, or missing at certain points, and which are likely to display **non-linear patterns**. The lab will focus on including higher-order polynomials in MLR to capture non-linearity. 
<br>

<div class="red">
##### Solutions are available {-}  
Solutions are already available for the first half of today's lab.  
We encourage you to try working through the questions yourself before looking at the solutions. 
</div>

## Exercise 1

**Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer's disease on three different kinds of tasks - Memory, complex activities of daily living (ADL), and simple activities of daily living.** 

`r msmbstyle::question_begin(header = "&#x25BA; Question 1")`
Read the data in to R from the following url: [https://edin.ac/35Njwpl](https://edin.ac/35Njwpl)
. The data is in .rda format. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
load(url("https://edin.ac/35Njwpl"))
summary(Az)
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 2")`
Plot the observed data (the performance over time for each type of task).
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + 
  stat_summary(fun.data=mean_se, geom="ribbon", color=NA, alpha=0.5) +
  stat_summary(fun=mean, geom="line")
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 3")`
Why are natural polynomials more useful for these data?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Because it's useful to know whether there are task differences at the starting baseline point
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 4")`
Fit the GCA model(s).

Steps required:  

1. Add 1st and 2nd order natural polynomials to the data using the `code_poly()` function. 
1. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested?
1. Create a new model with a fixed effect of Task
1. Create a new model in which performance varies linearly over time between Task type.
1. Create a new model in which linear and quadratic performance over time varies between Task type.
1. Run model comparisons.

`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
# prep for analysis
Az <- code_poly(Az, predictor="Time", poly.order=2, orthogonal=F, draw.poly = F)

# fit the full model incrementally
m.base <- lmer(Performance ~ (poly1 + poly2) +
                 (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
               data=Az, REML=F)
m.0 <- lmer(Performance ~ (poly1 + poly2) + Task +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.1 <- lmer(Performance ~ poly1*Task + poly2 +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.Az.full <- lmer(Performance ~ (poly1 + poly2)*Task + 
                  (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), 
                data=Az, REML=F)
anova(m.base, m.0, m.1, m.Az.full)
```
`r msmbstyle::solution_end()`

### Interpret the results of your full model {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 5")`
Look at the `summary()` of your full model, and try using the `get_pvalues()` function on it.  
  
Which terms show significant effects of experimental factors? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Get p-values for your full model:
```{r warning=FALSE, message=FALSE}
get_pvalues(m.Az.full)
```

+ Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials)
+ Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster.
+ Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?)
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 6")`
To what extent do model comparisons and the parameter-specific p-values yield the same results?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Model comparisons suggest:  

+ Linear slopes are different: $\chi^2(2)=30.56, p << 0.0001$ (comparison `m.0` and `m.1` above).  
+ Quadratic term is different: $\chi^2(2)=172.36, p << 0.0001$ (comparison `m.1` and `m.Az.full` above).  


*Note:* We can't investigate the intercept difference via the model comparisons above. Comparison between `m.base` and `m.0` indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0).**

<div class="red">
<br>
**Please note:** There was an error in last week's lab about assessing 'baseline' differences between groups.
Model comparison between `y~time` and `y~time+g` does *not* assess the difference in `y` between groups in `g` where `time==0`, but instead assesses the difference in `y` between groups in `g` holding time constant. 
<br>
Last week's lab has been amended. Apologies for any confusion!
</div>
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 7")`
Plot model fit
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r fig.width=6, fig.height=4}
ggplot(Az, aes(Time, Performance, color=Task)) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(fun=mean, geom="line", aes(y=fitted(m.Az.full)))
```
`r msmbstyle::solution_end()`

## Exercise 2: Logistic GCA

We saw in the [lecture](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/Lecture03_MLRforLDA.html#(4)) a walk-through of using GCA to model the eye-tracking data from a spoken word-to-picture matching task.  
<br>
We analysed the proportion of fixations to the target picture in a given 50~ms time bin (the `meanFix` variable). We can express this differently, in terms of the number of samples in each 50~ms bin in which there were fixations to the target, and the total number of samples. This can lend itself to being modelled as a binomial (where success is fixation on the target). 
<br>
In the data, the `sumFix` variable contains the number of samples in which the target was fixated upon, and the `N` variable contains the total number of samples in that bin. Like we saw last week, we can model a binomial using `cbind(num_successes, num_failures)`, so here we can use `cbind(sumFix, N-sumFix)~ ...`
<br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 8")`
**Re-analyze `TargetFix` data using logistic GCA.** 

The data (.rda format) is available at [https://edin.ac/2TieJK0](https://edin.ac/2TieJK0)  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
load(url("https://edin.ac/2TieJK0"))
#make 3rd-order orth poly
TargetFix <- code_poly(TargetFix, predictor="timeBin", poly.order=3, draw.poly=F)
# fit logisitc GCA model
m.log <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 | Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log)
```

Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them:
```{r}
m.log_zc <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 || Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log_zc)
```

Plot model fit
```{r fig.height=4, fig.width=6}
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  stat_summary(aes(y=fitted(m.log)), fun=mean, geom="line") +
  stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom="line", linetype="dashed") +
  theme_bw() + expand_limits(y=c(0,1)) + 
  labs(y="Fixation Proportion", x="Time since word onset (ms)")
```
`r msmbstyle::solution_end()`

