```{r, echo=FALSE}
HIDDEN_SOLS=TRUE
```

# MLR for longitudinal data (growth curve analysis)

### Packages {-}  

+ lme4  
+ tidyverse  
+ effects  

We will also be needing to access some useful functions from Dan for getting p-values and coding polynomials.   
<br>
The `source()` function basically takes in R code and evaluates it. You can download R scripts with Dan's code [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R) and [here](https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R).   
However, you can also source them directly from the URLs, and read them into your environment:    
```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(lme4)
library(effects)
source('https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R')
source("https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R")
```

### Lecture Slides {-}  

The lecture slides can be accessed [here](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/Lecture03_MLRforLDA.html). 
The data for the lecture can be found at [https://edin.ac/2TieJK0](https://edin.ac/2TieJK0).  

### Background & Reading {-}  
  
+ [Curran et al., 2010](https://doi.org/10.1080/15248371003699969)  
+ [Winter & Wieling, 2016](https://doi.org/10.1093/jole/lzv003)  
  
## Introduction    

This week we are going to look at how we might use MLR to study longitudinal data. That is, data in which repeated measurements are taken over a continuous domain, with the potential for observations to be unevenly spaced, or missing at certain points, and which are likely to display **non-linear patterns**. The lab will focus on including higher-order polynomials in MLR to capture non-linearity. 
<br>

<div class="red">
##### Solutions are available {-}  
Solutions are already available for the first half of today's lab.  
We encourage you to try working through the questions yourself before looking at the solutions. 
</div>

## Exercise 1

**Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer's disease on three different kinds of tasks - Memory, complex activities of daily living (ADL), and simple activities of daily living.** 

`r msmbstyle::question_begin(header = "&#x25BA; Question 1")`
Read the data in to R from the following url: [https://edin.ac/35Njwpl](https://edin.ac/35Njwpl)
. The data is in .rda format. 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
load(url("https://edin.ac/35Njwpl"))
summary(Az)
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 2")`
Plot the observed data (the performance over time for each type of task).
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + 
  stat_summary(fun.data=mean_se, geom="ribbon", color=NA, alpha=0.5) +
  stat_summary(fun=mean, geom="line")
```
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 3")`
Why are natural polynomials more useful for these data?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Because it's useful to know whether there are task differences at the starting baseline point
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 4")`
Fit the GCA model(s).

Steps required:  

1. Add 1st and 2nd order natural polynomials to the data using the `code_poly()` function. 
1. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested?
1. Create a new model with a fixed effect of Task
1. Create a new model in which performance varies linearly over time between Task type.
1. Create a new model in which linear and quadratic performance over time varies between Task type.
1. Run model comparisons.

`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r}
# prep for analysis
Az <- code_poly(Az, predictor="Time", poly.order=2, orthogonal=F, draw.poly = F)

# fit the full model incrementally
m.base <- lmer(Performance ~ (poly1 + poly2) +
                 (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
               data=Az, REML=F)
m.0 <- lmer(Performance ~ (poly1 + poly2) + Task +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.1 <- lmer(Performance ~ poly1*Task + poly2 +
              (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task),
            data=Az, REML=F)
m.Az.full <- lmer(Performance ~ (poly1 + poly2)*Task + 
                  (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), 
                data=Az, REML=F)
anova(m.base, m.0, m.1, m.Az.full)
```

<div class="red">
In these solutions we are using Dan's `code_poly()` function. All that this does is add polynomial terms to your dataframe (it can also give you a plot if you want one by setting `draw.poly = TRUE`). 
<br>
As always with R, you can do the same thing in lots of different ways.  
We could, for example, pass the `poly()` function directly to our model (without making storing them as variables in our dataframe), for instance:
```{r eval=FALSE}
m.base <- lmer(Performance ~ poly(Time, degree = 2, raw = TRUE) +
                 (poly(Time, degree = 2, raw = TRUE) | Subject) + 
                 (poly(Time, degree = 2, raw = TRUE) | Subject:Task),
               data=Az, REML=F)
```
<br>
If you're struggling with the idea of polynomials, try looking at the output of the following commands:
```{r eval=FALSE}
# note, the "raw" here is the same as "natural".
poly(1:10, 3, raw = TRUE)
poly(1:10, 3, raw = FALSE)

# let's plot them quickly using matplot() 
matplot(poly(1:10, 3, raw = TRUE), type = 'l')
matplot(poly(1:10, 3, raw = FALSE), type = 'l')

```
</div>


`r msmbstyle::solution_end()`

### Interpret the results of your full model {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 5")`
Look at the `summary()` of your full model, and try using the `get_pvalues()` function on it.  
  
Which terms show significant effects of experimental factors? 
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Get p-values for your full model:
```{r warning=FALSE, message=FALSE}
get_pvalues(m.Az.full)
```

+ Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials)
+ Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster.
+ Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?)
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 6")`
To what extent do model comparisons and the parameter-specific p-values yield the same results?
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
Model comparisons suggest:  

+ Linear slopes are different: $\chi^2(2)=30.56, p << 0.0001$ (comparison `m.0` and `m.1` above).  
+ Quadratic term is different: $\chi^2(2)=172.36, p << 0.0001$ (comparison `m.1` and `m.Az.full` above).  


*Note:* We can't investigate the intercept difference via the model comparisons above. Comparison between `m.base` and `m.0` indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0).**

<div class="red">
<br>
**Please note:** There was an error in last week's lab about assessing 'baseline' differences between groups.
Model comparison between `y~time` and `y~time+g` does *not* assess the difference in `y` between groups in `g` where `time==0`, but instead assesses the difference in `y` between groups in `g` holding time constant. 
<br>
Last week's lab has been amended. Apologies for any confusion!
</div>
`r msmbstyle::solution_end()`
<br>
<hr />
<br><br>
`r msmbstyle::question_begin(header = "&#x25BA; Question 7")`
Plot model fit
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
```{r fig.width=6, fig.height=4}
ggplot(Az, aes(Time, Performance, color=Task)) + 
  stat_summary(fun.data=mean_se, geom="pointrange") + 
  stat_summary(fun=mean, geom="line", aes(y=fitted(m.Az.full)))
```
`r msmbstyle::solution_end()`

## Exercise 2: Writing up, and Logistic GCA

We saw in the [lecture](https://uoe-psychology.github.io/uoe_psystats/multivar/lectures/Lecture03_MLRforLDA.html#(4)) a walk-through of using GCA to model the eye-tracking data from a spoken word-to-picture matching task.  

`r msmbstyle::question_begin(header = "&#x25BA; Question 8")`
The model we saw in the lecture had the following structure and results:
```
> m.full <- lmer(meanFix ~ (poly1+poly2+poly3)*Condition + 
                 (poly1+poly2+poly3 | Subject) + 
                 (poly1+poly2 | Subject:Condition), 
               control = lmerControl(optimizer="bobyqa"),
               data=TargetFix, REML=F)

> coef(summary(m.full))
                      Estimate Std. Error      t value
(Intercept)       0.4773227513 0.01385240 34.457775306
poly1               0.6385603705 0.05993519 10.654181583
poly2              -0.1095979256 0.03848819 -2.847573180
poly3              -0.0932611870 0.02041640 -4.567955536
ConditionLow     -0.0581122429 0.01901291 -3.056462582
poly1:ConditionLow  0.0003188189 0.06330556  0.005036191
poly2:ConditionLow  0.1635455113 0.05426498  3.013831365
poly3:ConditionLow -0.0020869051 0.02014728 -0.103582452
```
<br>
Write up the results of the model
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=FALSE)`
There are two rules of thumb for reporting growth curve analysis results:

+ **Clearly describe each of the three key components of the model**: the functional form (third-order orthogonal polynomial), the fixed effects (effect of Condition on all time terms), and the random effects (effect of Subject on each of the time terms and nested effects of Subject-by-Condition on each of the time terms except the cubic). Depending on the circumstances and complexity of the model, you may want to include additional information about the factors and why they were included or not. It's also a good idea to report which method was used for computing p-values.  
+ **For key findings, report parameter estimates and standard errors along with significance tests**. In some cases the model comparison is going to be enough, but for key findings, the readers should want to see the parameter estimates. The parameter estimate standard errors are critical for interpreting the estimates, so those should be reported as well. The t-values are not critical to report (they are just Estimate divided by the Std Error, so they can always be computed from the reported estimates and standard errors). If there are many estimated parameters, it may be a good idea to focus the main text discussion on the most important ones and report the full set in a table or appendix.  
<br>
<br>
Here is how we might report the results from the example above:<br>
<small>[Note, we haven't included Table 1 here. If you want a nice way of creating tables, try installing the `sjPlot` package, and using `tab_model()` on your model.]</small>
<br>
<br>
<div class="noteBox">
Growth curve analysis (Mirman, 2014) was used to analyze the target gaze data from 300ms to 1000ms after word onset. The overall time course of target fixations was modeled with a third-order (cubic) orthogonal polynomial and fixed effects of Condition (Low vs. High frequency; within-participants) on all time terms. The model also included participant random effects on all time terms and participant-by-condition random effects on all time terms except the cubic (estimating random effects is “expensive” in terms of the number of observation required, so this cubic term was excluded because it tends to capture less-relevant effects in the tails). There was a significant effect of Condition on the intercept term, indicating lower overall target fixation proportions for the Low condition relative to the High condition (Estimate = -0.058, SE = 0.019, p < 0.01). There was also a significant effect on the quadratic term, indicating shallower curvature - slower word recognition - in the Low condition relative to the High condition (Estimate = 0.16, SE = 0.054, p < 0.01). All other effects of Condition were not significant (see Table 1 for full results).
</div>
<br>
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(header = "&#x25BA; Question 9")`
Above, we analysed the proportion of fixations to the target picture in a given 50~ms time bin (the `meanFix` variable). We can express this differently, in terms of the number of samples in each 50~ms bin in which there were fixations to the target, and the total number of samples. This can lend itself to being modelled as a binomial (where success is fixation on the target). 
<br>
In the data, the `sumFix` variable contains the number of samples in which the target was fixated upon, and the `N` variable contains the total number of samples in that bin. Like we saw last week, we can model a binomial using `cbind(num_successes, num_failures)`, so here we can use `cbind(sumFix, N-sumFix)~ ...`
<br>
<br>
**Re-analyze `TargetFix` data using logistic GCA.** 
<br>
<br>
The data (.rda format) is available at [https://edin.ac/2TieJK0](https://edin.ac/2TieJK0)  
`r msmbstyle::question_end()` 
`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
load(url("https://edin.ac/2TieJK0"))
#make 3rd-order orth poly
TargetFix <- code_poly(TargetFix, predictor="timeBin", poly.order=3, draw.poly=F)
# fit logisitc GCA model
m.log <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 | Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log)
```

Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them:
```{r}
m.log_zc <- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition +
                 (poly1+poly2+poly3 || Subject) +
                 (poly1+poly2 | Subject:Condition),
               data=TargetFix, family=binomial, control = glmerControl(optimizer = "bobyqa"))
summary(m.log_zc)
```

Plot model fit
```{r fig.height=4, fig.width=6}
ggplot(TargetFix, aes(Time, meanFix, color=Condition)) +
  stat_summary(fun.data=mean_se, geom="pointrange") +
  stat_summary(aes(y=fitted(m.log)), fun=mean, geom="line") +
  stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom="line", linetype="dashed") +
  theme_bw() + expand_limits(y=c(0,1)) + 
  labs(y="Fixation Proportion", x="Time since word onset (ms)")
```
`r msmbstyle::solution_end()`



