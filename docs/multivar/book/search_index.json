[
["index.html", "Multivariate Statistics and Methodology using R Overview of the Course The team R stuff", " Multivariate Statistics and Methodology using R Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Multivariate Statistics and Methodology using R extends what you learnt last semester in USMR to provide an advanced level overview of statistical analysis techniques and methodology issues relevant to psychological research, introducing analysis tools that extend to cases where multiple outcome variables are being studied simultaneously, and hierarchical data structures (e.g., children nested in classes nested in schools). On this page you will find the weekly lab exercises, along with links to useful content online, walkthroughs, etc. The labs will begin to require a little more initiative than those from USMR, so be prepared to do some googling! Each week, solutions (where available) will be made available here for the previous weeks’ lab. The team Lecturers: Dr Aja Murray: aja.murray@ed.ac.uk (Course Organiser) Dr Dan Mirman: daniel.mirman@ed.ac.uk Senior Teching Coordinators: pg.ppls.stats@ed.ac.uk Dr Umberto Noe Dr Josiah King R stuff We will be getting to grips with a lot of new tools for data manipulation and visualisation, using some of the packages pictured below. Some of you may be familiar with some of these already, but don’t worry if not! R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ Some key ones for this course are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["week-1-tidyverse-markdown.html", "Chapter 1 Week 1: Tidyverse &amp; Markdown 1.1 Data Visualization with ggplot 1.2 Data management with the Tidyverse 1.3 Reproducible research with RMarkdown", " Chapter 1 Week 1: Tidyverse &amp; Markdown This week, we’re going to introduce you to some really nice packages and tools which will help you to make your analysis and reporting more efficient, aesthetically pleasing, and (importantly) reproducible. ► Question If you haven’t previously installed them, install the following packages tidyverse rmarkdown haven (this one is just for reading in data from other software like SPSS or SAS) Background &amp; Reading R for Data Science: https://r4ds.had.co.nz/index.html Data visualization: Chapters 3 and 28 Data management (tidyverse): Chapters 5 and 12 R Markdown: Chapter 27 Extras: Kieran Healey has a brilliant book on getting started with ggplot: Data Visualisation; a practical introduction Another great one is Fundamentals of Data Visualisation by Claus O. Wilke 1.1 Data Visualization with ggplot For plotting, you may be familiar with the popular ggplot2 package from some of the USMR labs last semester. We’re going to be using this more and more, so the first part of today’s lab will focus on ggplot. Visualization is the first step in analysis 1.1.1 Geoms To learn about some of the different functionalities of ggplot, we’re first going to need some data… ► Question Load the ggplot2 package, read in the data using load() and url(), and extract some summary statistics. The data can be found at https://edin.ac/2Erg9ZW. ► Solution library(ggplot2) load(url(&quot;https://edin.ac/2Erg9ZW&quot;)) summary(speech_ses) ## ResponseId Category Accuracy ## Length:912 Social Class:228 Min. : 3.704 ## Class :character Race :228 1st Qu.: 55.556 ## Mode :character Age :228 Median : 66.667 ## Gender :228 Mean : 69.547 ## 3rd Qu.: 82.407 ## Max. :100.000 Data overview Kraus et al. (2019) Evidence for the reproduction of social class in brief speech, Proc. Natl. Acad. Sci. U.S.A. (Study 1) N=189 speakers from the International Dialects of (North American) English Archive. Narrative speech and reading stories. Extracted 7 individual words that were produced by all speakers: “And”, “From”, “Thought”, “Beautiful”, “Imagine”, “Yellow”, and “The”. Participants (N=229, from AMT) Listened to the 7 one-word clips Estimated the speaker’s race, gender, age, and educational attainment Each participant completed this for a random subset of 27 speakers ► Question Make a summary plot showing mean accuracy for each category of judgment hint: try ?stat_summary ► Solution #one way of doing this: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + stat_summary(fun=mean, geom=&quot;bar&quot;) ► Question Explore the different ways of showing variability. Construct a plot using each of the following geoms: ► Solution # * Boxplot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_boxplot() # * Jitter ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) # * Violin plot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_violin() # * Errorbar ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;) # * Pointrange ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) ► Question Combine two of the geoms used above to create a visualization of the mean accuracy, a measure of variability, and all of the data points. ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) ► Question Refine the plot by, for example, removing unnecessary elements, adding useful annotations (e.g., chance performance = 50%), selecting a good color scheme, etc. tip: This is where google becomes really helpful, for example ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) + guides(colour = FALSE) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + geom_hline(yintercept=50, linetype=&quot;dashed&quot;) 1.1.2 Recreating a plot ► Question Recreate the graph below using ggplot (if you like, try to make it better!). Women in computer science The data (in .csv format) can be downloaded from https://edin.ac/2qYA0wr. You can use read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) to read it directly into R. ► Solution women_cs&lt;-read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) ggplot(women_cs, aes(x=date, y=pct_women_majors, color=field))+ labs(x=NULL,y=NULL, title=&quot;What happened to women in computer science?&quot;)+ geom_line()+ scale_color_manual(values=c(&#39;#11605E&#39;, &#39;#17807E&#39;, &#39;#8BC0BF&#39;,&#39;#D8472B&#39;))+ scale_y_continuous(label=scales::percent)+ theme_minimal(base_family=&quot;Helvetica&quot;)+ theme(legend.title=element_blank()) # If you want to get fancier, and add the labels at the end of the lines, check out the gghighlight package! 1.2 Data management with the Tidyverse A collection of R packages known as the tidyverse provides so many incredibly useful functions that can speed up your workflow. They are often contrasted to Base R (which is what you have been working with so far) in that they provide an alternative grammar which is aimed at being more predictable and consistent. Some people find the tidyverse a lot more intuitive, but others don’t, and the transition can sometimes be difficult! 1.2.1 Piping! It may look a bit weird (%&gt;%), but the pipe operator in R is incredibly useful. Its fundamental role is to ‘chain’ functions together. Previously we wrapped functions around one another, with lots of brackets, but with %&gt;% we can link the intermediate output of one function and take it as the input of another. The two functions f and g, when used in combination like g(f(x)), can now be written as x %&gt;% f() %&gt;% g(). You don’t even always need the brackets, and coulde write x %&gt;% f %&gt;% g! The default behaviour of %&gt;% is to put the output of the LHS (left hand side) in as the first argument in the RHS. However, you can change this by using %&gt;% in combination with a ., to specify which argument you want it to be inputted as: 100 %&gt;% rnorm(10, ., 1) is equal to rnorm(10, 100, 1) The default behaviour: 100 %&gt;% rnorm(0, 1) is implicitly saying 100 %&gt;% rnorm(., 0, 1), which is equal to rnorm(100, 0, 1). ► Question Translate the following statements between Base R and sequences of pipes. The first is shown for you. 1 Base R: round(mean(rnorm(100,0,1))) Pipes : rnorm(100,0,1) %&gt;% mean() %&gt;% round() 2 Base R: x&lt;-10:100 round(exp(diff(log(x))), 2) Pipes: ► Solution 10:100 %&gt;% log() %&gt;% diff() %&gt;% exp() %&gt;% round(2) 3 Pipes: 6 %&gt;% round(pi, digits=.) Base R: ► Solution round(pi, digits=6) 1.2.2 Grouping, summarising, filtering, mutating and selecting Tidyverse also gives us really useful functions for wrangling data. There are many, but some of the key ones we’ll learn here are: select() extracts columns filter() subsets data based on conditions mutate() adds new variables group_by() group related rows together summarise()/summarize() reduces values down to a single summary For a quick example, if we want to calculate the median accuracy for each category, but only after removing those with an accuracy &lt;50, we could use: speech_ses %&gt;% filter(Accuracy&gt;50) %&gt;% group_by(Category) %&gt;% summarise( mdn_accuracy = median(Accuracy) ) And if we wanted to also calculate the mean accuracy for each category, we could add: speech_ses %&gt;% group_by(Category) %&gt;% summarise( n = n(), mean_acc = mean(Accuracy) ) ► Question Load the tidyverse, and haven package, and read in the data using read_sav() (.sav is the type of file which comes out of another stats software, SPSS). You can download the data from https://edin.ac/34n6AWA to your computer, and then read it in. library(tidyverse) exam &lt;- haven::read_sav(&quot;data/exam.sav&quot;) Using the exam.sav data: ► Question Calculate the mean score for each exam ► Solution exam %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42.9 ## 2 2 37.9 ## 3 3 38.9 ► Question Calculate the mean score for each exam for female students only ► Solution exam %&gt;% filter(gender==&quot;f&quot;) %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 ## 2 2 36.6 ## 3 3 38.1 ► Question Make a new dataframe containing only the exam scores for males for exam number 1, with a new variable indicating whether they passed or not (pass = a score of 40) ► Solution exam_m1 &lt;- exam %&gt;% filter(exam == 1, gender == &quot;m&quot;) %&gt;% mutate(pass = ifelse(scores&gt;40,&quot;pass&quot;,&quot;fail&quot;)) ► Question Calculate the average score for each exam for male and female students&quot;)` ► Solution exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) ## # A tibble: 6 x 3 ## # Groups: exam [3] ## exam gender M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 f 43.1 ## 2 1 m 42.7 ## 3 2 f 36.6 ## 4 2 m 39.1 ## 5 3 f 38.1 ## 6 3 m 39.7 # use spread() to make it easier to compare exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) %&gt;% spread(gender, M) ## # A tibble: 3 x 3 ## # Groups: exam [3] ## exam f m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 42.7 ## 2 2 36.6 39.1 ## 3 3 38.1 39.7 1.2.3 Reshaping The same data can be represented in many different ways. We often discern between long and wide formats, and each of these are useful in different ways. Consider, the below example, showing the same data in long format on the left, and in wide on the right. There are some useful functions which we can use to move between these formats: gather() and spread(). Check out the explanation of them in the reading, section 12.31 Data overview The USArrests data set (comes with R) contains violent crime arrests (per 100,000 residents) in each of the 50 states in the USA in 1973 and the percent of the population of each state that lived in urban areas. You can see it by just typing USArrests in R. ► Question Convert the USArrests data set from a wide to a long format so that instead of separate variables for each crime type (Murder, Assault, Rape), there is one variable that identifies the crime type and one variable that contains the rates for each crime type for each state. ► Solution x &lt;- gather(USArrests, key=&quot;CrimeType&quot;, value=&quot;Rate&quot;, Murder, Assault, Rape) ► Question Make a scatterplot showing the relationship between each type of violent crime rate and percent of population living in urban areas. ► Solution ggplot(x, aes(UrbanPop, Rate)) + facet_wrap(~CrimeType, scales=&quot;free&quot;, nrow=1) + geom_point() + stat_smooth(method=&quot;lm&quot;) Less guidance Data overview The ability data set in the psych package contains accuracy of 1525 subjects on 16 multiple choice IQ-test-type questions. The questions are of 4 types: basic reasoning, letter sequence, matrix reasoning, and spatial rotation. There are four questions of each type. You can see the by typing psych::ability (those :: are just a way of accessing something from inside a package without loading it). ► Question Tidy the data and make a graph of average accuracy for each question type. Hint: the separate() function may come in handy at some point. ► Solution iq &lt;- as_tibble(psych::ability) %&gt;% gather(key=&quot;Item&quot;, value=&quot;Correct&quot;, 1:16) %&gt;% separate(Item, c(&quot;Domain&quot;, &quot;Number&quot;)) ggplot(iq, aes(Domain, Correct)) + stat_summary(fun = mean, geom=&quot;bar&quot;) 1.3 Reproducible research with RMarkdown We’re also going to start to use RMarkdown. This is a really useful means of making a report reproducible. Essentially, it is a combination of R code and normal text. It will require learning a few new formatting rules (the “markdown” bit), but it means that in one file you can read in and analyse your data, and compile it to a pdf. Which essentially means that if your data or analysis changes, then the results you report change too without having to edit them! 1.3.1 Convert a script into a R Notebook Open your script from the exercises so far. Compile a HTML report from that script. ► Question Create a new R Notebook file, fill it in with the content of your script from the exercises so far.&quot;)` Hint: R code goes into R chunks, add some text in between chunks. Add formatting to make it look nicer: headers, bold, italics, etc (see the cheat-sheet) Add chunk options to suppress extraneous messages and warnings, and to control the size of figures. ► Question Knit the notebook into a HTML file. A newer version of these are pivot_longer() and pivot_wider(), but gather() and spread() will continue to be available.↩ "],
["week-2-basic-mlr.html", "Chapter 2 Week 2: Basic MLR 2.1 Logistic MLR 2.2 Effect of brain lesion location on learning and memory", " Chapter 2 Week 2: Basic MLR ► Question Load the tidyverse, lme4 and effects packages (install them if you haven’t already). ► Solution library(tidyverse) library(lme4) library(effects) 2.0.1 Analyzing county-level suicide rate data from Public Health England (PHE). The data is available at https://edin.ac/36xdhas and covers the period from 2001 to 2016. It contains information for a number of different indicators over this period for a selection of counties in England. ► Question Did the regions differ in their baseline (2001) suicide rates? Did the regions differ in ther slopes of change of suidice rate? Make a plot of the fitted values from the model ► Solution Loading load(url(&quot;https://edin.ac/36xdhas&quot;)) #load Public Health England data unique(mh_phe[, 1:2]) #check list of mental health indicators: suicide is 41001 ## IndicatorID IndicatorName ## 1 848 Depression: Recorded prevalence (aged 18+) ## 760 41001 Suicide rate ## 8056 90275 Percentage of physically active adults - historical method ## 8664 90646 Depression: QOF incidence (18+) - new diagnosis # select data and shift Year variable so baseline year (2001) is 0 suicide_dat &lt;- filter(mh_phe, IndicatorID == 41001) %&gt;% mutate(Time = Year - 2001) Modelling # base model: just change over time m &lt;- lmer(Value ~ Time + (Time | County), data = suicide_dat, REML = F) # add baseline differences between regions m.0 &lt;- lmer(Value ~ Time + Region + (Time | County), data = suicide_dat, REML = F) # add slope differences between regions m.1 &lt;- lmer(Value ~ Time * Region + (Time | County), data = suicide_dat, REML = F) # compare models anova(m, m.0, m.1) ## Data: suicide_dat ## Models: ## m: Value ~ Time + (Time | County) ## m.0: Value ~ Time + Region + (Time | County) ## m.1: Value ~ Time * Region + (Time | County) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m 6 39961 40002 -19974 39949 ## m.0 14 39924 40019 -19948 39896 53.136 8 1.015e-08 *** ## m.1 22 39918 40068 -19937 39874 21.277 8 0.006447 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like regions differ in baseline suicide rate (addition of Region predictor in m.0) and in slope of change (addition of interaction in m.1) Plotting # you can extract the fitted values using the effect() function from the effects package. ef &lt;- as.data.frame(effect(&quot;Time:Region&quot;, m.1)) ggplot(ef, aes(Time, fit, color=Region)) + geom_line() + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + labs(y=&quot;Fitted values&quot;) 2.0.2 Analyze the weight maintenance data (WeightMaintain3), a made-up data set based on Lowe et al. (2014, Obesity, 22, 94-100). Overweight participants completed a 12-week weight loss program, then were randomly assigned to one of three weight maintenance conditions: None (Control) MR (meal replacements): use MR to replace one meal and snack per day ED (energy density intervention): book and educational materials on purchasing and preparing foods lower in ED (reducing fat content and/or increasing water content of foods) Weight was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post. load(url(&quot;https://edin.ac/2tFIedK&quot;)) summary(WeightMaintain3) ## ID Condition Assessment WeightChange ## 101 : 4 None:240 Min. :0.00 Min. :-8.3781 ## 102 : 4 ED :240 1st Qu.:0.75 1st Qu.:-0.5024 ## 103 : 4 MR :240 Median :1.50 Median : 0.7050 ## 104 : 4 Mean :1.50 Mean : 1.4438 ## 105 : 4 3rd Qu.:2.25 3rd Qu.: 2.8806 ## 106 : 4 Max. :3.00 Max. :14.9449 ## (Other):696 ► Question Overall, did the participants maintain their weight loss or did their weights change? ► Solution m.null &lt;- lmer(WeightChange ~ 1 + (Assessment | ID), data=WeightMaintain3, REML=F) m.base &lt;- lmer(WeightChange ~ Assessment + (Assessment | ID), data=WeightMaintain3, REML=F) anova(m.null, m.base) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.66 1 6.782e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Weights changed over course of assessment period: \\(\\chi^2(1)=56.5, p &lt;&lt; 0.0001\\) ► Question Did the groups differ in baseline weight change and rate of weight gain (non-maintenance)? ► Solution m.int &lt;- lmer(WeightChange ~ Assessment + Condition + (Assessment | ID), data=WeightMaintain3, REML=F) m.full &lt;- lmer(WeightChange ~ Assessment*Condition + (Assessment | ID), data=WeightMaintain3, REML=F) anova(m.null, m.base, m.int, m.full) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## m.int: WeightChange ~ Assessment + Condition + (Assessment | ID) ## m.full: WeightChange ~ Assessment * Condition + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.6605 1 6.782e-15 *** ## m.int 8 2573.9 2610.6 -1279.0 2557.9 9.4418 2 0.008907 ** ## m.full 10 2537.5 2583.3 -1258.8 2517.5 40.3813 2 1.703e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes: Baseline: \\(\\chi^2(2)=9.4, p &lt; 0.01\\) Slope: \\(\\chi^2(2)=40.4, p &lt;&lt; 0.0001\\) Note: m.int is difficult to interpret in light of the massive effect on slope coef(summary(m.full)) ## Estimate Std. Error t value ## (Intercept) 0.06038642 0.09807154 0.6157384 ## Assessment 1.84917936 0.18399472 10.0501761 ## ConditionED -0.14303302 0.13869410 -1.0312841 ## ConditionMR -0.14944649 0.13869410 -1.0775259 ## Assessment:ConditionED -1.74949968 0.26020783 -6.7234705 ## Assessment:ConditionMR -0.83624053 0.26020783 -3.2137408 Compared to no intervention, weight (re)gain was 1.75 lbs/year slower for the ED intervention and 0.84 lbs/year slower for the MR intervention. Note that baseline weight difference parameters are not significantly different from 0. ► Question Make a graph of the model fit ► Solution ggplot(WeightMaintain3, aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=fitted(m.full)), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Alternatively, you can do the same using fortify(). ggplot(fortify(m.full), aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=.fitted), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) ► Question Examine the parameter estimates and interpret them ► Solution round(coef(summary(m.full)), 3) ## Estimate Std. Error t value ## (Intercept) 0.060 0.098 0.616 ## Assessment 1.849 0.184 10.050 ## ConditionED -0.143 0.139 -1.031 ## ConditionMR -0.149 0.139 -1.078 ## Assessment:ConditionED -1.749 0.260 -6.723 ## Assessment:ConditionMR -0.836 0.260 -3.214 (Intercept) ==&gt; baseline weight change in None group Assessment ==&gt; slope of weight change in None group ConditionED ==&gt; baseline weight change in ED group relative to None group ConditionMR ==&gt; baseline weight change in MR group relative to None group Assessment:ConditionED ==&gt; slope of weight change in ED group relative to None group Assessment:ConditionMR ==&gt; slope of weight change in MR groups relative to None group 2.1 Logistic MLR 2.2 Effect of brain lesion location on learning and memory load(url(&quot;https://edin.ac/2QwG7SG&quot;)) In the nwl data set, the participants with aphasia are also separated into two groups based on the general location of their brain lesion: anterior vs. posterior. Compare these two groups: ► Question Is the learning rate (training data) different between these two groups? Tip: Remember that you can use cbind() to specify the numbers of successes and failures as the outcome in a binomial regression. ► Solution m.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), data = filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) #summary(m.loc1) anova(m.base, m.loc0, m.loc1, test=&quot;Chisq&quot;) ## Data: filter(nwl, block &lt; 8, !is.na(lesion_location)) ## Models: ## m.base: cbind(NumCorrect, NumError) ~ block + (block | ID) ## m.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ## m.loc0: ID) ## m.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ## m.loc1: ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 5 454.12 466.27 -222.06 444.12 ## m.loc0 6 454.66 469.25 -221.33 442.66 1.4572 1 0.2274 ## m.loc1 7 454.47 471.48 -220.23 440.47 2.1974 1 0.1382 No significant difference in learning rate between groups: \\(\\chi^2(2)=2.2, p = 0.138\\) ► Question Does their immediate test performance differ, and does their retention from immediate to follow-up test differ? ► Solution m.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID), data=filter(nwl, block &gt; 7, !is.na(lesion_location)), family=&quot;binomial&quot;) summary(m.recall.loc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID) ## Data: filter(nwl, block &gt; 7, !is.na(lesion_location)) ## ## AIC BIC logLik deviance df.resid ## 142.6 150.9 -64.3 128.6 17 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.31708 -0.45013 0.03288 0.46923 1.09355 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 0.28200 0.5310 ## PhaseImmediate 0.02539 0.1593 1.00 ## Number of obs: 24, groups: ID, 12 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.13826 0.28978 -0.477 0.6333 ## PhaseImmediate 0.02453 0.24634 0.100 0.9207 ## lesion_locationposterior 0.74483 0.38409 1.939 0.0525 . ## PhaseImmediate:lesion_locationposterior 0.25437 0.33904 0.750 0.4531 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) PhsImm lsn_lc ## PhaseImmedt -0.149 ## lsn_lctnpst -0.755 0.113 ## PhsImmdt:l_ 0.109 -0.729 -0.164 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question Extra: Create a visualisation showing the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up) ► Solution ggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, color=lesion_location, shape=lesion_location)) + #geom_line(aes(group=ID),alpha=.2) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), fun=mean, geom=&quot;line&quot;) + geom_hline(yintercept=0.5, linetype=&quot;dashed&quot;) + geom_vline(xintercept=c(7.5, 8.5), linetype=&quot;dashed&quot;) + scale_x_continuous(breaks=1:9, labels=c(1:7, &quot;Test&quot;, &quot;Follow-Up&quot;)) + theme_bw(base_size=10) + labs(x=&quot;Block&quot;, y=&quot;Proportion Correct&quot;, shape=&quot;Lesion\\nLocation&quot;, color=&quot;Lesion\\nLocation&quot;) "]
]
