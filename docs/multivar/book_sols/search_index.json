[
["index.html", "Multivariate Statistics and Methodology using R Overview of the Course The team R stuff", " Multivariate Statistics and Methodology using R Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Multivariate Statistics and Methodology using R extends what you learnt last semester in USMR to provide an advanced level overview of statistical analysis techniques and methodology issues relevant to psychological research, introducing analysis tools that extend to cases where multiple outcome variables are being studied simultaneously, and hierarchical data structures (e.g., children nested in classes nested in schools). On this page you will find the weekly lab exercises, along with links to useful content online, walkthroughs, etc. The labs will begin to require a little more initiative than those from USMR, so be prepared to do some googling! Each week, solutions (where available) will be made available here for the previous weeks’ lab. The team Lecturers: Dr Aja Murray: aja.murray@ed.ac.uk (Course Organiser) Dr Dan Mirman: daniel.mirman@ed.ac.uk Senior Teching Coordinators: pg.ppls.stats@ed.ac.uk Dr Umberto Noe Dr Josiah King R stuff We will be getting to grips with a lot of new tools for data manipulation and visualisation, using some of the packages pictured below. Some of you may be familiar with some of these already, but don’t worry if not! R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ Some key ones for this course are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["tidyverse-markdown.html", "Chapter 1 Tidyverse &amp; Markdown 1.1 Data Visualization with ggplot 1.2 Data management with the Tidyverse 1.3 Reproducible research with RMarkdown", " Chapter 1 Tidyverse &amp; Markdown This week, we’re going to introduce you to some really nice packages and tools which will help you to make your analysis and reporting more efficient, aesthetically pleasing, and (importantly) reproducible. Packages If you haven’t previously installed them, install the following packages tidyverse rmarkdown haven (this one is just for reading in data from other software like SPSS or SAS) Lecture slides The lecture slides can be accessed here. The data (in .RData format) for the lecture can be found at https://edin.ac/2suU8XW Background &amp; Reading R for Data Science: https://r4ds.had.co.nz/index.html Data visualization: Chapters 3 and 28 Data management (tidyverse): Chapters 5 and 12 R Markdown: Chapter 27 Extras: Kieran Healey has a brilliant book on getting started with ggplot: Data Visualisation; a practical introduction Another great one is Fundamentals of Data Visualisation by Claus O. Wilke 1.1 Data Visualization with ggplot For plotting, you may be familiar with the popular ggplot2 package from some of the USMR labs last semester. We’re going to be using this more and more, so the first part of today’s lab will focus on ggplot. Visualization is the first step in analysis 1.1.1 Geoms To learn about some of the different functionalities of ggplot, we’re first going to need some data… ► Question Load the ggplot2 package, read in the data using load() and url(), and extract some summary statistics. The data can be found at https://edin.ac/2Erg9ZW. ► Solution library(ggplot2) load(url(&quot;https://edin.ac/2Erg9ZW&quot;)) summary(speech_ses) ## ResponseId Category Accuracy ## Length:912 Social Class:228 Min. : 3.704 ## Class :character Race :228 1st Qu.: 55.556 ## Mode :character Age :228 Median : 66.667 ## Gender :228 Mean : 69.547 ## 3rd Qu.: 82.407 ## Max. :100.000 Data overview Kraus et al. (2019) Evidence for the reproduction of social class in brief speech, Proc. Natl. Acad. Sci. U.S.A. (Study 1) N=189 speakers from the International Dialects of (North American) English Archive. Narrative speech and reading stories. Extracted 7 individual words that were produced by all speakers: “And”, “From”, “Thought”, “Beautiful”, “Imagine”, “Yellow”, and “The”. Participants (N=229, from AMT) Listened to the 7 one-word clips Estimated the speaker’s race, gender, age, and educational attainment Each participant completed this for a random subset of 27 speakers ► Question Make a summary plot showing mean accuracy for each category of judgment hint: try ?stat_summary ► Solution We need to use stat_summary because we want to summarise the y values on our plot into summary value(s) (in the case the mean). We could also calculate the mean accuracy for each category first, and then plot them using geom_bar directly, but stat_summary can be pretty useful #one way of doing this: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + stat_summary(fun.y=mean, geom=&quot;bar&quot;) We should also note that stat_summary(fun.y=mean, geom=&quot;bar&quot;) and geom_bar(stat=&quot;summary&quot;,fun.y=mean) are exactly the same! You could get the same plot using: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_bar(stat=&quot;summary&quot;,fun.y=mean) ► Question Explore the different ways of showing variability. Construct a plot using each of the following geoms: The top three plots (jitter, boxplots and violins) all show all of the data, so we don’t need to use stat_summary for these. However, the bottom two (errorbars and pointranges) require us to summarise the data into means and standard errors, so we need to use stat_summary(fun.data=mean_se). ► Solution # * Boxplot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_boxplot() # * Jitter ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) # * Violin plot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_violin() # * Errorbar ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;) # * Pointrange ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) ► Question Combine two of the geoms used above to create a visualization of the mean accuracy, a measure of variability, and all of the data points. ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) ► Question Refine the plot by, for example, removing unnecessary elements, adding useful annotations (e.g., chance performance = 50%), selecting a good color scheme, etc. tip: This is where google becomes really helpful, for example ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) + guides(colour = FALSE) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + geom_hline(yintercept=50, linetype=&quot;dashed&quot;) 1.1.2 Recreating a plot ► Question Recreate the graph below using ggplot (if you like, try to make it better!). Women in computer science The data (in .csv format) can be downloaded from https://edin.ac/2qYA0wr. You can use read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) to read it directly into R. ► Solution women_cs&lt;-read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) ggplot(women_cs, aes(x=date, y=pct_women_majors, color=field))+ labs(x=NULL,y=NULL, title=&quot;What happened to women in computer science?&quot;)+ geom_line()+ scale_color_manual(values=c(&#39;#11605E&#39;, &#39;#17807E&#39;, &#39;#8BC0BF&#39;,&#39;#D8472B&#39;))+ scale_y_continuous(label=scales::percent)+ theme_minimal(base_family=&quot;Helvetica&quot;)+ theme(legend.title=element_blank()) # If you want to get fancier, and add the labels at the end of the lines, check out the gghighlight package! 1.2 Data management with the Tidyverse A collection of R packages known as the tidyverse provides so many incredibly useful functions that can speed up your workflow. They are often contrasted to Base R (which is what you have been working with so far) in that they provide an alternative grammar which is aimed at being more predictable and consistent. Some people find the tidyverse a lot more intuitive, but others don’t, and the transition can sometimes be difficult! 1.2.1 Piping! It may look a bit weird (%&gt;%), but the pipe operator in R is incredibly useful. Its fundamental role is to ‘chain’ functions together. Previously we wrapped functions around one another, with lots of brackets, but with %&gt;% we can link the intermediate output of one function and take it as the input of another. The two functions f and g, when used in combination like g(f(x)), can now be written as x %&gt;% f() %&gt;% g(). You don’t even always need the brackets, and coulde write x %&gt;% f %&gt;% g! The default behaviour of %&gt;% is to put the output of the LHS (left hand side) in as the first argument in the RHS. However, you can change this by using %&gt;% in combination with a ., to specify which argument you want it to be inputted as: 100 %&gt;% rnorm(10, ., 1) is equal to rnorm(10, 100, 1) The default behaviour: 100 %&gt;% rnorm(0, 1) is implicitly saying 100 %&gt;% rnorm(., 0, 1), which is equal to rnorm(100, 0, 1). ► Question Translate the following statements between Base R and sequences of pipes. The first is shown for you. 1 Base R: round(mean(rnorm(100,0,1))) Pipes : rnorm(100,0,1) %&gt;% mean() %&gt;% round() 2 Base R: x&lt;-10:100 round(exp(diff(log(x))), 2) Pipes: ► Solution 10:100 %&gt;% log() %&gt;% diff() %&gt;% exp() %&gt;% round(2) 3 Pipes: 6 %&gt;% round(pi, digits=.) Base R: ► Solution round(pi, digits=6) 1.2.2 Grouping, summarising, filtering, mutating and selecting Tidyverse also gives us really useful functions for wrangling data. There are many, but some of the key ones we’ll learn here are: select() extracts columns filter() subsets data based on conditions mutate() adds new variables group_by() group related rows together summarise()/summarize() reduces values down to a single summary For a quick example, if we want to calculate the median accuracy for each category, but only after removing those with an accuracy &lt;50, we could use: speech_ses %&gt;% filter(Accuracy&gt;50) %&gt;% group_by(Category) %&gt;% summarise( mdn_accuracy = median(Accuracy) ) And if we wanted to also calculate the mean accuracy for each category, we could add: speech_ses %&gt;% group_by(Category) %&gt;% summarise( n = n(), mean_acc = mean(Accuracy) ) ► Question Load the tidyverse, and haven package, and read in the data using read_sav() (.sav is the type of file which comes out of another stats software, SPSS). You can download the data from https://edin.ac/34n6AWA to your computer, and then read it in. library(tidyverse) exam &lt;- haven::read_sav(&quot;data/exam.sav&quot;) Using the exam.sav data: ► Question Calculate the mean score for each exam ► Solution exam %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42.9 ## 2 2 37.9 ## 3 3 38.9 ► Question Calculate the mean score for each exam for female students only ► Solution exam %&gt;% filter(gender==&quot;f&quot;) %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 ## 2 2 36.6 ## 3 3 38.1 ► Question Make a new dataframe containing only the exam scores for males for exam number 1, with a new variable indicating whether they passed or not (pass = a score of 40) ► Solution exam_m1 &lt;- exam %&gt;% filter(exam == 1, gender == &quot;m&quot;) %&gt;% mutate(pass = ifelse(scores&gt;40,&quot;pass&quot;,&quot;fail&quot;)) ► Question Calculate the average score for each exam for male and female students&quot;)` ► Solution exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) ## # A tibble: 6 x 3 ## # Groups: exam [3] ## exam gender M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 f 43.1 ## 2 1 m 42.7 ## 3 2 f 36.6 ## 4 2 m 39.1 ## 5 3 f 38.1 ## 6 3 m 39.7 # use spread() to make it easier to compare exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) %&gt;% spread(gender, M) ## # A tibble: 3 x 3 ## # Groups: exam [3] ## exam f m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 42.7 ## 2 2 36.6 39.1 ## 3 3 38.1 39.7 1.2.3 Reshaping The same data can be represented in many different ways. We often discern between long and wide formats, and each of these are useful in different ways. Consider, the below example, showing the same data in long format on the left, and in wide on the right. There are some useful functions which we can use to move between these formats: gather() and spread(). Check out the explanation of them in the reading, section 12.3 A newer version of these are pivot_longer() and pivot_wider(), but gather() and spread() will continue to be available if you want them. Data overview The USArrests data set (comes with R) contains violent crime arrests (per 100,000 residents) in each of the 50 states in the USA in 1973 and the percent of the population of each state that lived in urban areas. You can see it by just typing USArrests in R. ► Question Convert the USArrests data set from a wide to a long format so that instead of separate variables for each crime type (Murder, Assault, Rape), there is one variable that identifies the crime type and one variable that contains the rates for each crime type for each state. ► Solution x &lt;- gather(USArrests, key=&quot;CrimeType&quot;, value=&quot;Rate&quot;, Murder, Assault, Rape) # or x &lt;- pivot_longer(USArrests, cols = c(Murder, Assault, Rape), names_to = &quot;CrimeType&quot;, values_to = &quot;Rate&quot;) ► Question Make a scatterplot showing the relationship between each type of violent crime rate and percent of population living in urban areas. ► Solution ggplot(x, aes(UrbanPop, Rate)) + facet_wrap(~CrimeType, scales=&quot;free&quot;, nrow=1) + geom_point() + stat_smooth(method=&quot;lm&quot;) Less guidance Data overview The ability data set in the psych package contains accuracy of 1525 subjects on 16 multiple choice IQ-test-type questions. The questions are of 4 types: basic reasoning, letter sequence, matrix reasoning, and spatial rotation. There are four questions of each type. You can see the by typing psych::ability (those :: are just a way of accessing something from inside a package without loading it). ► Question Tidy the data and make a graph of average accuracy for each question type. You might have to use as_tibble(ability) or as.data.frame(ability) because it is initially stored as a matrix. Hint: the separate() function may come in handy at some point. ► Solution iq &lt;- as_tibble(psych::ability) %&gt;% pivot_longer(., cols=1:16, names_to = &quot;Item&quot;, values_to = &quot;Correct&quot;) %&gt;% # gather(key=&quot;Item&quot;, value=&quot;Correct&quot;, 1:16) %&gt;% # The gather() alternative... separate(Item, c(&quot;Domain&quot;, &quot;Number&quot;)) ggplot(iq, aes(Domain, Correct)) + stat_summary(fun.y = mean, geom=&quot;bar&quot;) 1.3 Reproducible research with RMarkdown We’re also going to start to use RMarkdown. This is a really useful means of making a report reproducible. Essentially, it is a combination of R code and normal text. It will require learning a few new formatting rules (the “markdown” bit), but it means that in one file you can read in and analyse your data, and compile it to a pdf. Which essentially means that if your data or analysis changes, then the results you report change too without having to edit them! 1.3.1 Convert a script into a R Notebook Open your script from the exercises so far. Compile a HTML report from that script. ► Question Create a new R Notebook file, fill it in with the content of your script from the exercises so far.&quot;)` Hint: R code goes into R chunks, add some text in between chunks. Add formatting to make it look nicer: headers, bold, italics, etc (see the cheat-sheet) Add chunk options to suppress extraneous messages and warnings, and to control the size of figures. ► Question Knit the notebook into a HTML file. "],
["basic-multilevel-regression.html", "Chapter 2 Basic Multilevel Regression 2.1 LME4 2.2 Introduction to MLR 2.3 Logistic MLR", " Chapter 2 Basic Multilevel Regression We’re going to start using multilevel modelling. There are a whole lot of different names people use for this sort of methodology (hierarchical linear models, linear mixed-effect models, mixed models, nested data models, random coefficient, random-effects models, random parameter models… and so on). What the idea boils down to is that model parameters vary at more than one level. Packages lme4 tidyverse effects Load the tidyverse, lme4 and effects packages (install them if you haven’t already). library(tidyverse) library(lme4) library(effects) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/36bD1s0 (Visual search data) and https://edin.ac/2QwG7SG (Novel world learning data). Background &amp; Reading Winter, 2013 Brauer &amp; Curtin, 2018, (pdf) Luke, 2017 2.1 LME4 We’re going to use the lme4 package, and specifically the functions lmer() and glmer(). “(g)lmer” here stands for “(generalised) linear mixed effects regression”. You will have seen some use of these functions in the lectures. The broad syntax is: lmer(formula, REML = logical, data = dataframe) The formula bit is similar to what we did with a standard linear model (lm()) in that it has the outcome ~ explanatory variables structure. However, we now have the addition of the random effect terms, specified in parenthesis with the | operator separating parameters on the LHS and a grouping factor on the RHS. Below are a selection of different formulas for specifying different random effect structures. Formula Alternative Meaning \\(\\text{(1 | g)}\\) \\(\\text{1 + (1 | g)}\\) Random intercept with fixed mean \\(\\text{0 + offset(o) + (1 | g)}\\) \\(\\text{-1 + offset(o) + (1 | g)}\\) Random intercept with a priori means \\(\\text{(1 | g1/g2)}\\) \\(\\text{(1 | g1) + (1 | g1:g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) within \\(g1\\) \\(\\text{(1 | g1) + (1 | g2)}\\) \\(\\text{1 + (1 | g1) + (1 | g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) \\(\\text{x + (x | g)}\\) \\(\\text{1 + x + (1 + x | g)}\\) Correlated random intercept and slope \\(\\text{x + (x || g)}\\) \\(\\text{1 + x + (x | g) + (0 + x | g)}\\) Uncorrelated random intercept and slope Table 1: Examples of the right-hand-sides of mixed effects model formulas. \\(g\\), \\(g1\\), \\(g2\\) are grouping factors, covariates and a priori known offsets are \\(x\\) and \\(o\\). Errors and warnings? For large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning. There are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them). For now, if lmer() gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = &quot;bobyqa&quot;) when you run your model. 2.2 Introduction to MLR 2.2.1 Exercise 1 County-level suicide rate data from Public Health England (PHE) is available at https://edin.ac/36xdhas and covers the period from 2001 to 2016. It contains information for a number of different indicators over this period for a selection of counties in England. ► Question Using multilevel regression, study the following: Did the regions differ in their baseline (2001) suicide rates? Did the regions differ in ther slopes of change of suidice rate? Make a plot of the fitted values from the model Steps: First, get acquainted with the data. It contains various different indicators but we’re only interested in one (suicide rates), so you might want to filter() the data, using the skills you learnt last week. You may also want to center your data on the year 2001, so that the intercept (e.g., at time 0) in your models is interpretable. Then think about what test you can do to answer the questions above. Lastly, passing your model to the effect() function (from the effects package) can give you all the data you need to construct your plot. Hint: Try comparing models with and without different baseline/slopes of suicide rates for each region. Think also about the grouping of the data points, and how this is represented in your random effect structure. ► Solution Loading load(url(&quot;https://edin.ac/36xdhas&quot;)) #load Public Health England data unique(mh_phe[, 1:2]) #check list of mental health indicators: suicide is 41001 ## IndicatorID IndicatorName ## 1 848 Depression: Recorded prevalence (aged 18+) ## 760 41001 Suicide rate ## 8056 90275 Percentage of physically active adults - historical method ## 8664 90646 Depression: QOF incidence (18+) - new diagnosis # select data and shift Year variable so baseline year (2001) is 0 suicide_dat &lt;- filter(mh_phe, IndicatorID == 41001) %&gt;% mutate(Time = Year - 2001) Modelling # base model: just change over time m &lt;- lmer(Value ~ Time + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add baseline differences between regions m.0 &lt;- lmer(Value ~ Time + Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add slope differences between regions m.1 &lt;- lmer(Value ~ Time * Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # compare models anova(m, m.0, m.1) ## Data: suicide_dat ## Models: ## m: Value ~ Time + (Time | County) ## m.0: Value ~ Time + Region + (Time | County) ## m.1: Value ~ Time * Region + (Time | County) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m 6 39961 40002 -19974 39949 ## m.0 14 39924 40019 -19948 39896 53.136 8 1.015e-08 *** ## m.1 22 39918 40068 -19937 39874 21.277 8 0.006447 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like regions differ in baseline suicide rate (addition of Region predictor in m.0) and in slope of change (addition of interaction in m.1) Plotting # you can extract the fitted values using the effect() function from the effects package. ef &lt;- as.data.frame(effect(&quot;Time:Region&quot;, m.1)) ggplot(ef, aes(Time, fit, color=Region)) + geom_line() + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + labs(y=&quot;Fitted values&quot;) 2.2.2 Exercise 2 The weight maintenance data (WeightMaintain3), a made-up data set based on Lowe et al. (2014, Obesity, 22, 94-100), contains information on overweight participants who completed a 12-week weight loss program, and were then randomly assigned to one of three weight maintenance conditions: None (Control) MR (meal replacements): use MR to replace one meal and snack per day ED (energy density intervention): book and educational materials on purchasing and preparing foods lower in ED (reducing fat content and/or increasing water content of foods) Weight was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post. load(url(&quot;https://edin.ac/2tFIedK&quot;)) summary(WeightMaintain3) ## ID Condition Assessment WeightChange ## 101 : 4 None:240 Min. :0.00 Min. :-8.3781 ## 102 : 4 ED :240 1st Qu.:0.75 1st Qu.:-0.5024 ## 103 : 4 MR :240 Median :1.50 Median : 0.7050 ## 104 : 4 Mean :1.50 Mean : 1.4438 ## 105 : 4 3rd Qu.:2.25 3rd Qu.: 2.8806 ## 106 : 4 Max. :3.00 Max. :14.9449 ## (Other):696 ► Question Overall, did the participants maintain their weight loss or did their weights change? ► Solution m.null &lt;- lmer(WeightChange ~ 1 + (Assessment | ID), data=WeightMaintain3, REML=F) m.base &lt;- lmer(WeightChange ~ Assessment + (Assessment | ID), data=WeightMaintain3, REML=F) anova(m.null, m.base) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.66 1 6.782e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Weights changed over course of assessment period: \\(\\chi^2(1)=56.5, p &lt;&lt; 0.0001\\) ► Question Did the groups differ in baseline weight change and rate of weight gain (non-maintenance)? ► Solution m.int &lt;- lmer(WeightChange ~ Assessment + Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) m.full &lt;- lmer(WeightChange ~ Assessment*Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) anova(m.null, m.base, m.int, m.full) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## m.int: WeightChange ~ Assessment + Condition + (Assessment | ID) ## m.full: WeightChange ~ Assessment * Condition + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.6605 1 6.782e-15 *** ## m.int 8 2573.9 2610.6 -1279.0 2557.9 9.4418 2 0.008907 ** ## m.full 10 2537.5 2583.3 -1258.8 2517.5 40.3814 2 1.703e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes: Baseline: \\(\\chi^2(2)=9.4, p &lt; 0.01\\) Slope: \\(\\chi^2(2)=40.4, p &lt;&lt; 0.0001\\) Note: m.int is difficult to interpret in light of the massive effect on slope coef(summary(m.full)) ## Estimate Std. Error t value ## (Intercept) 0.06038642 0.09807901 0.6156916 ## Assessment 1.84917936 0.18394628 10.0528229 ## ConditionED -0.14303302 0.13870466 -1.0312056 ## ConditionMR -0.14944649 0.13870466 -1.0774439 ## Assessment:ConditionED -1.74949968 0.26013932 -6.7252412 ## Assessment:ConditionMR -0.83624053 0.26013932 -3.2145872 Compared to no intervention, weight (re)gain was 1.75 lbs/year slower for the ED intervention and 0.84 lbs/year slower for the MR intervention. Note that baseline weight difference parameters are not significantly different from 0. ► Question Make a graph of the model fit and the observed data ► Solution There are lots of ways you can do this. Using the effect() function again (and then adding the means and SEs from the original data): ef &lt;- as.data.frame(effect(&quot;Assessment:Condition&quot;, m.full)) ggplot(ef, aes(Assessment, fit, color=Condition)) + geom_line() + stat_summary(data=WeightMaintain3, aes(y=WeightChange), fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2) Using the fitted() function to extract and plot fitted values from the model: ggplot(WeightMaintain3, aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=fitted(m.full)), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Or, alternatively, using fortify(): ggplot(fortify(m.full), aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=.fitted), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) ► Question Examine the parameter estimates and interpret them (i.e., what does each parameter represent?) ► Solution round(coef(summary(m.full)), 3) ## Estimate Std. Error t value ## (Intercept) 0.060 0.098 0.616 ## Assessment 1.849 0.184 10.053 ## ConditionED -0.143 0.139 -1.031 ## ConditionMR -0.149 0.139 -1.077 ## Assessment:ConditionED -1.749 0.260 -6.725 ## Assessment:ConditionMR -0.836 0.260 -3.215 (Intercept) ==&gt; baseline weight change in None group Assessment ==&gt; slope of weight change in None group ConditionED ==&gt; baseline weight change in ED group relative to None group ConditionMR ==&gt; baseline weight change in MR group relative to None group Assessment:ConditionED ==&gt; slope of weight change in ED group relative to None group Assessment:ConditionMR ==&gt; slope of weight change in MR groups relative to None group 2.3 Logistic MLR 2.3.1 Exercise 3 load(url(&quot;https://edin.ac/2QwG7SG&quot;)) In the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. Figure 2.1 shows the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up) Figure 2.1: Differences between groups in the average proportion of correct responses at each block Compare the two groups (those with anterior vs. posterior lesions) with respect to their responses. Tip: Remember that you can use cbind() to specify the numbers of successes and failures as the outcome in a binomial regression (and remember to specify the family argument in glmer()). ► Question Is the learning rate (training data) different between these two groups? ► Solution m.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), data = filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) #summary(m.loc1) anova(m.base, m.loc0, m.loc1, test=&quot;Chisq&quot;) ## Data: filter(nwl, block &lt; 8, !is.na(lesion_location)) ## Models: ## m.base: cbind(NumCorrect, NumError) ~ block + (block | ID) ## m.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ## m.loc0: ID) ## m.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ## m.loc1: ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 5 454.12 466.27 -222.06 444.12 ## m.loc0 6 454.66 469.25 -221.33 442.66 1.4572 1 0.2274 ## m.loc1 7 454.47 471.48 -220.23 440.47 2.1974 1 0.1382 No significant difference in learning rate between groups: \\(\\chi^2(2)=2.2, p = 0.138\\) ► Question Does follow-up test performance differ between lesion location groups, and does their retention from immediate to follow-up test differ? ► Solution m.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID), data=filter(nwl, block &gt; 7, !is.na(lesion_location)), family=&quot;binomial&quot;) summary(m.recall.loc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID) ## Data: filter(nwl, block &gt; 7, !is.na(lesion_location)) ## ## AIC BIC logLik deviance df.resid ## 142.6 150.9 -64.3 128.6 17 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.31708 -0.45013 0.03288 0.46923 1.09355 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 0.28200 0.5310 ## PhaseImmediate 0.02539 0.1593 1.00 ## Number of obs: 24, groups: ID, 12 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.13826 0.28978 -0.477 0.6333 ## PhaseImmediate 0.02453 0.24634 0.100 0.9207 ## lesion_locationposterior 0.74483 0.38409 1.939 0.0525 . ## PhaseImmediate:lesion_locationposterior 0.25437 0.33904 0.750 0.4531 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) PhsImm lsn_lc ## PhaseImmedt -0.149 ## lsn_lctnpst -0.755 0.113 ## PhsImmdt:l_ 0.109 -0.729 -0.164 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question Extra: Recreate the visualisation in Figure 2.1. ► Solution ggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, color=lesion_location, shape=lesion_location)) + #geom_line(aes(group=ID),alpha=.2) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), fun=mean, geom=&quot;line&quot;) + geom_hline(yintercept=0.5, linetype=&quot;dashed&quot;) + geom_vline(xintercept=c(7.5, 8.5), linetype=&quot;dashed&quot;) + scale_x_continuous(breaks=1:9, labels=c(1:7, &quot;Test&quot;, &quot;Follow-Up&quot;)) + theme_bw(base_size=10) + labs(x=&quot;Block&quot;, y=&quot;Proportion Correct&quot;, shape=&quot;Lesion\\nLocation&quot;, color=&quot;Lesion\\nLocation&quot;) "],
["mlr-for-longitudinal-data-growth-curve-analysis.html", "Chapter 3 MLR for longitudinal data (growth curve analysis) 3.1 Introduction 3.2 Exercise 1 3.3 Exercise 2: Logistic GCA 3.4 Exercise 2: Solution", " Chapter 3 MLR for longitudinal data (growth curve analysis) Packages lme4 tidyverse effects We will also be needing to access some useful functions from Dan for getting p-values and coding polynomials. The source() function basically takes in R code and evaluates it. You can download R scripts with Dan’s code here and here. However, you can also source them directly from the URLs, and read them into your environment: library(tidyverse) library(lme4) library(effects) source(&#39;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R&#39;) source(&quot;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R&quot;) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/2TieJK0. Background &amp; Reading Curran et al., 2010 Winter &amp; Wieling, 2016 3.1 Introduction 3.2 Exercise 1 The task Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer’s disease on three different kinds of tasks - Memory, complex ADL, and simple ADL. ► Question Read the data in to R from the following url: https://edin.ac/35Njwpl . The data is in .rda format. ► Solution load(url(&quot;https://edin.ac/35Njwpl&quot;)) summary(Az) ## Subject Time Task Performance ## 1 : 30 Min. : 1.0 cADL :300 Min. : 2.00 ## 2 : 30 1st Qu.: 3.0 sADL :300 1st Qu.:40.00 ## 3 : 30 Median : 5.5 Memory:300 Median :52.00 ## 4 : 30 Mean : 5.5 Mean :49.27 ## 5 : 30 3rd Qu.: 8.0 3rd Qu.:61.00 ## 6 : 30 Max. :10.0 Max. :85.00 ## (Other):720 ► Question Plot the observed data (the performance over time for each type of task). ► Solution ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, color=NA, alpha=0.5) + stat_summary(fun=mean, geom=&quot;line&quot;) ► Question Why are natural polynomials more useful for these data? ► Solution Because it’s useful to know whether there are task differences at the starting baseline point ► Question Fit the GCA model(s). Steps required: Add 1st and 2nd order natural polynomials to the data using the code_poly() function. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested? Create a new model with a fixed effect of Task Create a new model in which performance varies linearly over time between Task type. Create a new model in which linear and quadratic performance over time varies between Task type. Run model comparisons. ► Solution # prep for analysis Az &lt;- code_poly(Az, predictor=&quot;Time&quot;, poly.order=2, orthogonal=F, draw.poly = F) # fit the full model incrementally m.base &lt;- lmer(Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.0 &lt;- lmer(Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.1 &lt;- lmer(Performance ~ poly1*Task + poly2 + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.Az.full &lt;- lmer(Performance ~ (poly1 + poly2)*Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) anova(m.base, m.0, m.1, m.Az.full) ## Data: Az ## Models: ## m.base: Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + ## m.base: poly2 | Subject:Task) ## m.0: Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + ## m.0: (poly1 + poly2 | Subject:Task) ## m.1: Performance ~ poly1 * Task + poly2 + (poly1 + poly2 | Subject) + ## m.1: (poly1 + poly2 | Subject:Task) ## m.Az.full: Performance ~ (poly1 + poly2) * Task + (poly1 + poly2 | Subject) + ## m.Az.full: (poly1 + poly2 | Subject:Task) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 16 3804.6 3881.5 -1886.3 3772.6 ## m.0 18 3807.8 3894.2 -1885.9 3771.8 0.8242 2 0.6623 ## m.1 20 3781.2 3877.3 -1870.6 3741.2 30.5611 2 2.311e-07 *** ## m.Az.full 22 3612.9 3718.5 -1784.4 3568.9 172.3620 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Get p-values for your full model: get_pvalues(m.Az.full) ## Estimate Std..Error t.value p.normal p.normal.star ## (Intercept) 67.161666666 0.92669108 72.4747097 0.000000e+00 *** ## poly1 -3.287803030 0.34111997 -9.6382602 0.000000e+00 *** ## poly2 0.009469697 0.01243576 0.7614891 4.463650e-01 ## TasksADL 0.095000000 0.81069066 0.1171840 9.067142e-01 ## TaskMemory 1.240000000 0.81069066 1.5295600 1.261257e-01 ## poly1:TasksADL 1.362196970 0.26753689 5.0916229 3.550117e-07 *** ## poly1:TaskMemory -3.977070707 0.26753689 -14.8655038 0.000000e+00 *** ## poly2:TasksADL -0.013257576 0.01748015 -0.7584360 4.481900e-01 ## poly2:TaskMemory 0.338888889 0.01748015 19.3870696 0.000000e+00 *** Interpret the results of your full model ► Question Look at the summary() of your full model, and try using the get_pvalues() function on it. Which terms show significant effects of experimental factors? ► Solution Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials) Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster. Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?) ► Question To what extent do model comparisons and the parameter-specific p-values yield the same results? ► Solution Model comparisons suggest: Linear slopes are different: \\(\\chi^2(2)=30.56, p &lt;&lt; 0.0001\\) (comparison m.0 and m.1 above). Quadratic terms is different: \\(\\chi^2(2)=172.36, p &lt;&lt; 0.0001\\) (comparison m.1 and m.Az.full above). Note: We can’t investigate the intercept difference via the model comparisons above. Comparison between m.base and m.0 indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0). ► Question Plot model fit ► Solution ggplot(Az, aes(Time, Performance, color=Task)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(fun=mean, geom=&quot;line&quot;, aes(y=fitted(m.Az.full))) 3.3 Exercise 2: Logistic GCA Re-analyze TargetFix data using logistic GCA. The data (.rda format) is available at https://edin.ac/2TieJK0 3.4 Exercise 2: Solution load(url(&quot;https://edin.ac/2TieJK0&quot;)) #make 3rd-order orth poly TargetFix &lt;- code_poly(TargetFix, predictor=&quot;timeBin&quot;, poly.order=3, draw.poly=F) # fit logisitc GCA model m.log &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 | Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 | Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1419.1 1508.0 -685.6 1371.1 276 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.75431 -0.40973 -0.00307 0.37868 2.06240 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject:Condition (Intercept) 0.032340 0.17983 ## poly1 0.401862 0.63393 -0.68 ## poly2 0.147989 0.38469 -0.23 0.73 ## Subject (Intercept) 0.001751 0.04185 ## poly1 0.343615 0.58619 1.00 ## poly2 0.001991 0.04462 -1.00 -1.00 ## poly3 0.027493 0.16581 -1.00 -1.00 1.00 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11675 0.06548 -1.783 0.074589 . ## poly1 2.81834 0.29833 9.447 &lt; 2e-16 *** ## poly2 -0.55911 0.16952 -3.298 0.000973 *** ## poly3 -0.32075 0.12771 -2.512 0.012017 * ## ConditionLow -0.26157 0.09095 -2.876 0.004030 ** ## poly1:ConditionLow 0.06399 0.33134 0.193 0.846851 ## poly2:ConditionLow 0.69502 0.23977 2.899 0.003747 ** ## poly3:ConditionLow -0.07066 0.16617 -0.425 0.670679 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.288 ## poly2 -0.128 0.272 ## poly3 -0.100 -0.228 -0.015 ## ConditionLw -0.690 0.297 0.081 0.012 ## ply1:CndtnL 0.372 -0.552 -0.292 -0.024 -0.541 ## ply2:CndtnL 0.080 -0.230 -0.701 0.034 -0.116 0.415 ## ply3:CndtnL 0.013 -0.020 0.037 -0.637 -0.003 0.031 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them: m.log_zc &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 || Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log_zc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 || Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1411.6 1478.3 -687.8 1375.6 282 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.69598 -0.41491 -0.00141 0.33691 2.07563 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject.Condition (Intercept) 0.03404 0.1845 ## poly1 0.42307 0.6504 -0.63 ## poly2 0.15312 0.3913 -0.25 0.70 ## Subject poly3 0.00000 0.0000 ## Subject.1 poly2 0.00000 0.0000 ## Subject.2 poly1 0.44471 0.6669 ## Subject.3 (Intercept) 0.00000 0.0000 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11770 0.06544 -1.798 0.07210 . ## poly1 2.82162 0.31822 8.867 &lt; 2e-16 *** ## poly2 -0.55892 0.17054 -3.277 0.00105 ** ## poly3 -0.31340 0.11646 -2.691 0.00712 ** ## ConditionLow -0.26066 0.09280 -2.809 0.00497 ** ## poly1:ConditionLow 0.06593 0.33782 0.195 0.84527 ## poly2:ConditionLow 0.69049 0.24206 2.853 0.00434 ** ## poly3:ConditionLow -0.06654 0.16627 -0.400 0.68903 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.379 ## poly2 -0.129 0.301 ## poly3 -0.018 0.029 -0.054 ## ConditionLw -0.705 0.267 0.092 0.012 ## ply1:CndtnL 0.357 -0.528 -0.284 -0.027 -0.509 ## ply2:CndtnL 0.092 -0.212 -0.703 0.038 -0.131 0.402 ## ply3:CndtnL 0.012 -0.020 0.037 -0.699 -0.003 0.033 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Plot model fit ggplot(TargetFix, aes(Time, meanFix, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=fitted(m.log)), fun=mean, geom=&quot;line&quot;) + stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom=&quot;line&quot;, linetype=&quot;dashed&quot;) + theme_bw() + expand_limits(y=c(0,1)) + labs(y=&quot;Fixation Proportion&quot;, x=&quot;Time since word onset (ms)&quot;) "],
["other-random-effects-structures.html", "Chapter 4 Other random effects structures", " Chapter 4 Other random effects structures Packages lme4 Lecture Slides Coming soon Background &amp; Reading Coming soon "],
["individual-differences.html", "Chapter 5 Individual differences", " Chapter 5 Individual differences Packages lme4 Lecture Slides Coming soon Background &amp; Reading Coming soon "],
["break-week.html", "Chapter 6 Break Week", " Chapter 6 Break Week "],
["efa-and-pca.html", "Chapter 7 EFA and PCA 7.1 Today’s Exercises", " Chapter 7 EFA and PCA Packages psych GPArotation car GGally (optional) Lecture Slides Coming soon Background &amp; Reading Coming soon 7.1 Today’s Exercises Background A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours: Stealing Lying Skipping school Vandalism Breaking curfew Threatening others Bullying Spreading malicious rumours Using a weapon Fighting Your task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales ► Question 1 Load the psych package and read in the dataset ‘Conduct_problems.csv’. The first column is clearly an ID column, and it is easiest just to discard this for when we are doing factor analysis. Create a correlation matrix for the items. Inspect the items to check their suitability for exploratory factor analysis. You can use a function such as corr.test(df) from the psych package to create the correlation matrix. You can check the factorability of the correlation matrix using KMO(df). You can check linearity of relations using scatterplotMatrix(df) (from the car package). If you add the argument diagonal=histogram You can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). You can do the same using the ggpairs function from the GGally package. NOTE. df=dataframe ► Solution library(psych) df &lt;- read.csv(&quot;../labs/Conduct_problems.csv&quot;) # discard the first column df &lt;- df[,-1] corr.test(df) ## Call:corr.test(x = df) ## Correlation matrix ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 1.00 0.59 0.49 0.48 0.60 0.17 0.30 0.32 0.26 0.20 ## item2 0.59 1.00 0.53 0.51 0.66 0.20 0.33 0.30 0.29 0.19 ## item3 0.49 0.53 1.00 0.49 0.55 0.15 0.25 0.24 0.25 0.15 ## item4 0.48 0.51 0.49 1.00 0.65 0.23 0.29 0.32 0.28 0.25 ## item5 0.60 0.66 0.55 0.65 1.00 0.21 0.30 0.29 0.27 0.21 ## item6 0.17 0.20 0.15 0.23 0.21 1.00 0.54 0.57 0.41 0.44 ## item7 0.30 0.33 0.25 0.29 0.30 0.54 1.00 0.83 0.61 0.58 ## item8 0.32 0.30 0.24 0.32 0.29 0.57 0.83 1.00 0.61 0.59 ## item9 0.26 0.29 0.25 0.28 0.27 0.41 0.61 0.61 1.00 0.44 ## item10 0.20 0.19 0.15 0.25 0.21 0.44 0.58 0.59 0.44 1.00 ## Sample Size ## [1] 450 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 0 0 0 0 0 0 0 0 0 0 ## item2 0 0 0 0 0 0 0 0 0 0 ## item3 0 0 0 0 0 0 0 0 0 0 ## item4 0 0 0 0 0 0 0 0 0 0 ## item5 0 0 0 0 0 0 0 0 0 0 ## item6 0 0 0 0 0 0 0 0 0 0 ## item7 0 0 0 0 0 0 0 0 0 0 ## item8 0 0 0 0 0 0 0 0 0 0 ## item9 0 0 0 0 0 0 0 0 0 0 ## item10 0 0 0 0 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option KMO(df) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = df) ## Overall MSA = 0.87 ## MSA for each item = ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## 0.90 0.88 0.92 0.88 0.84 0.94 0.82 0.81 0.95 0.94 car::scatterplotMatrix(df) or alternatively. library(GGally) ggpairs(data=df, diag=list(continuous=&quot;density&quot;), axisLabels=&quot;show&quot;) ► Question 2 How many dimensions should be retained? Use a scree plot, parallel analysis, and MAP test to guide you. ► Solution You can use fa.parallel(df) to conduct both parallel analysis and view a scree plot. fa.parallel(df) ## Parallel analysis suggests that the number of factors = 2 and the number of components = 2 In this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors. We can conduct the MAP test using vss(df). vss(df) ## ## Very Simple Structure ## Call: vss(x = df) ## Although the VSS complexity 1 shows 7 factors, it is probably more reasonable to think about 2 factors ## VSS complexity 2 achieves a maximimum of 0.92 with 2 factors ## ## The Velicer MAP achieves a minimum of 0.03 with 2 factors ## BIC achieves a minimum of NA with 2 factors ## Sample Size adjusted BIC achieves a minimum of NA with 3 factors ## ## Statistics by number of factors ## vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq SRMR eCRMS ## 1 0.77 0.00 0.106 35 8.8e+02 3.6e-161 6.0 0.77 0.233 662 773 1.0 1.0e+03 1.6e-01 0.179 ## 2 0.78 0.92 0.034 26 4.0e+01 3.9e-02 2.1 0.92 0.035 -119 -36 1.1 1.4e+01 1.8e-02 0.024 ## 3 0.78 0.91 0.058 18 1.2e+01 8.6e-01 1.9 0.93 0.000 -98 -41 1.2 3.4e+00 9.1e-03 0.014 ## 4 0.63 0.90 0.104 11 6.1e+00 8.7e-01 1.4 0.95 0.000 -61 -26 1.3 1.6e+00 6.3e-03 0.013 ## 5 0.69 0.87 0.149 5 2.4e+00 7.9e-01 1.6 0.94 0.000 -28 -12 1.3 7.5e-01 4.3e-03 0.013 ## 6 0.71 0.89 0.252 0 1.2e-01 NA 1.4 0.95 NA NA NA 1.4 2.9e-02 8.4e-04 NA ## 7 0.79 0.89 0.397 -4 1.1e-07 NA 1.5 0.94 NA NA NA 1.3 2.2e-08 7.4e-07 NA ## 8 0.79 0.89 0.455 -7 1.6e-08 NA 1.5 0.94 NA NA NA 1.4 2.8e-09 2.6e-07 NA ## eBIC ## 1 797 ## 2 -145 ## 3 -107 ## 4 -66 ## 5 -30 ## 6 NA ## 7 NA ## 8 NA The MAP test suggests retaining 2 factors. ► Question 3 Having decided how many dimensions to retain in the previous question, conduct an EFA to extract this many factors, using a suitable rotation and extraction method. ► Solution You can use the fa() function from the psych package, for example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method. conduct_efa &lt;- fa(df, nfactors=2, rotate=&#39;oblimin&#39;, fm=&#39;minres&#39;) ► Question 4 Inspect the loadings and give the factors you extracted labels based on the patterns of loadings. Look back to the description of the items, and suggest a name for you factors ► Solution You can inspect the loadings using: conduct_efa$loadings ## ## Loadings: ## MR1 MR2 ## item1 0.706 ## item2 0.772 ## item3 0.681 ## item4 0.676 ## item5 0.872 ## item6 0.634 ## item7 0.890 ## item8 0.924 ## item9 0.629 ## item10 0.669 ## ## MR1 MR2 ## SS loadings 2.90 2.784 ## Proportion Var 0.29 0.278 ## Cumulative Var 0.29 0.568 We can see that the first five items have high loadings for one factor and the second five items have high loadings for the other. The first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems. ► Question 5 How correlated are your factors? ► Solution We can inspect the factor correlations (if we used an oblique rotation) using: conduct_efa$Phi ## MR1 MR2 ## MR1 1.0000000 0.4299169 ## MR2 0.4299169 1.0000000 We can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. ► Question 6 Using the same data, conduct a PCA using the principal() function. What differences do you notice compared to your EFA? Do you think a PCA or an EFA is more appropriate in this particular case? ► Solution We can use: principal(df, nfactors=2) ## Principal Components Analysis ## Call: principal(r = df, nfactors = 2) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## item1 0.17 0.77 0.62 0.38 1.1 ## item2 0.17 0.81 0.68 0.32 1.1 ## item3 0.11 0.75 0.58 0.42 1.0 ## item4 0.21 0.74 0.60 0.40 1.2 ## item5 0.16 0.85 0.75 0.25 1.1 ## item6 0.73 0.08 0.53 0.47 1.0 ## item7 0.87 0.20 0.80 0.20 1.1 ## item8 0.88 0.19 0.82 0.18 1.1 ## item9 0.72 0.21 0.56 0.44 1.2 ## item10 0.75 0.09 0.57 0.43 1.0 ## ## RC1 RC2 ## SS loadings 3.29 3.22 ## Proportion Var 0.33 0.32 ## Cumulative Var 0.33 0.65 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 166.43 with prob &lt; 1.9e-22 ## ## Fit based upon off diagonal values = 0.98 We can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low. In terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data. "]
]
