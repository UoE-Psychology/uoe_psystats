[
["index.html", "Multivariate Statistics and Methodology using R Overview of the Course The team R stuff", " Multivariate Statistics and Methodology using R Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Multivariate Statistics and Methodology using R extends what you learnt last semester in USMR to provide an advanced level overview of statistical analysis techniques and methodology issues relevant to psychological research, introducing analysis tools that extend to cases where multiple outcome variables are being studied simultaneously, and hierarchical data structures (e.g., children nested in classes nested in schools). On this page you will find the weekly lab exercises, along with links to useful content online, walkthroughs, etc. The labs will begin to require a little more initiative than those from USMR, so be prepared to do some googling! Each week, solutions (where available) will be made available here for the previous weeks’ lab. The team Lecturers: Dr Aja Murray: aja.murray@ed.ac.uk (Course Organiser) Dr Dan Mirman: daniel.mirman@ed.ac.uk Senior Teching Coordinators: pg.ppls.stats@ed.ac.uk Dr Umberto Noe Dr Josiah King R stuff We will be getting to grips with a lot of new tools for data manipulation and visualisation, using some of the packages pictured below. Some of you may be familiar with some of these already, but don’t worry if not! R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ Some key ones for this course are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community "],
["tidyverse-markdown.html", "Chapter 1 Tidyverse &amp; Markdown 1.1 Data Visualization with ggplot 1.2 Data management with the Tidyverse 1.3 Reproducible research with RMarkdown", " Chapter 1 Tidyverse &amp; Markdown This week, we’re going to introduce you to some really nice packages and tools which will help you to make your analysis and reporting more efficient, aesthetically pleasing, and (importantly) reproducible. Packages If you haven’t previously installed them, install the following packages tidyverse rmarkdown haven (this one is just for reading in data from other software like SPSS or SAS) Lecture slides The lecture slides can be accessed here. The data (in .RData format) for the lecture can be found at https://edin.ac/2suU8XW Background &amp; Reading R for Data Science: https://r4ds.had.co.nz/index.html Data visualization: Chapters 3 and 28 Data management (tidyverse): Chapters 5 and 12 R Markdown: Chapter 27 Extras: Kieran Healey has a brilliant book on getting started with ggplot: Data Visualisation; a practical introduction Another great one is Fundamentals of Data Visualisation by Claus O. Wilke 1.1 Data Visualization with ggplot For plotting, you may be familiar with the popular ggplot2 package from some of the USMR labs last semester. We’re going to be using this more and more, so the first part of today’s lab will focus on ggplot. Visualization is the first step in analysis 1.1.1 Geoms To learn about some of the different functionalities of ggplot, we’re first going to need some data… ► Question Load the ggplot2 package, read in the data using load() and url(), and extract some summary statistics. The data can be found at https://edin.ac/2Erg9ZW. ► Solution library(ggplot2) load(url(&quot;https://edin.ac/2Erg9ZW&quot;)) summary(speech_ses) ## ResponseId Category Accuracy ## Length:912 Social Class:228 Min. : 3.704 ## Class :character Race :228 1st Qu.: 55.556 ## Mode :character Age :228 Median : 66.667 ## Gender :228 Mean : 69.547 ## 3rd Qu.: 82.407 ## Max. :100.000 Data overview Kraus et al. (2019) Evidence for the reproduction of social class in brief speech, Proc. Natl. Acad. Sci. U.S.A. (Study 1) N=189 speakers from the International Dialects of (North American) English Archive. Narrative speech and reading stories. Extracted 7 individual words that were produced by all speakers: “And”, “From”, “Thought”, “Beautiful”, “Imagine”, “Yellow”, and “The”. Participants (N=229, from AMT) Listened to the 7 one-word clips Estimated the speaker’s race, gender, age, and educational attainment Each participant completed this for a random subset of 27 speakers ► Question Make a summary plot showing mean accuracy for each category of judgment hint: try ?stat_summary ► Solution We need to use stat_summary because we want to summarise the y values on our plot into summary value(s) (in the case the mean). We could also calculate the mean accuracy for each category first, and then plot them using geom_bar directly, but stat_summary can be pretty useful #one way of doing this: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + stat_summary(fun.y=mean, geom=&quot;bar&quot;) We should also note that stat_summary(fun.y=mean, geom=\"bar\") and geom_bar(stat=\"summary\",fun.y=mean) are exactly the same! You could get the same plot using: ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_bar(stat=&quot;summary&quot;,fun.y=mean) ► Question Explore the different ways of showing variability. Construct a plot using each of the following geoms: The top three plots (jitter, boxplots and violins) all show all of the data, so we don’t need to use stat_summary for these. However, the bottom two (errorbars and pointranges) require us to summarise the data into means and standard errors, so we need to use stat_summary(fun.data=mean_se). ► Solution # * Boxplot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_boxplot() # * Jitter ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) # * Violin plot ggplot(speech_ses, aes(x = Category, y = Accuracy, fill = Category)) + geom_violin() # * Errorbar ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;) # * Pointrange ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) ► Question Combine two of the geoms used above to create a visualization of the mean accuracy, a measure of variability, and all of the data points. ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) ► Question Refine the plot by, for example, removing unnecessary elements, adding useful annotations (e.g., chance performance = 50%), selecting a good color scheme, etc. tip: This is where google becomes really helpful, for example ► Solution ggplot(speech_ses, aes(x = Category, y = Accuracy, colour = Category)) + geom_jitter(width=0.2, alpha=0.5) + stat_summary(fun.data=mean_se, geom=&quot;errorbar&quot;, colour=&quot;black&quot;, width=0.4, size=1.5) + guides(colour = FALSE) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + geom_hline(yintercept=50, linetype=&quot;dashed&quot;) 1.1.2 Recreating a plot ► Question Recreate the graph below using ggplot (if you like, try to make it better!). Women in computer science The data (in .csv format) can be downloaded from https://edin.ac/2qYA0wr. You can use read.csv(url(\"https://edin.ac/2qYA0wr\")) to read it directly into R. ► Solution women_cs&lt;-read.csv(url(&quot;https://edin.ac/2qYA0wr&quot;)) ggplot(women_cs, aes(x=date, y=pct_women_majors, color=field))+ labs(x=NULL,y=NULL, title=&quot;What happened to women in computer science?&quot;)+ geom_line()+ scale_color_manual(values=c(&#39;#11605E&#39;, &#39;#17807E&#39;, &#39;#8BC0BF&#39;,&#39;#D8472B&#39;))+ scale_y_continuous(label=scales::percent)+ theme_minimal(base_family=&quot;Helvetica&quot;)+ theme(legend.title=element_blank()) # If you want to get fancier, and add the labels at the end of the lines, check out the gghighlight package! 1.2 Data management with the Tidyverse A collection of R packages known as the tidyverse provides so many incredibly useful functions that can speed up your workflow. They are often contrasted to Base R (which is what you have been working with so far) in that they provide an alternative grammar which is aimed at being more predictable and consistent. Some people find the tidyverse a lot more intuitive, but others don’t, and the transition can sometimes be difficult! 1.2.1 Piping! It may look a bit weird (%&gt;%), but the pipe operator in R is incredibly useful. Its fundamental role is to ‘chain’ functions together. Previously we wrapped functions around one another, with lots of brackets, but with %&gt;% we can link the intermediate output of one function and take it as the input of another. The two functions f and g, when used in combination like g(f(x)), can now be written as x %&gt;% f() %&gt;% g(). You don’t even always need the brackets, and coulde write x %&gt;% f %&gt;% g! The default behaviour of %&gt;% is to put the output of the LHS (left hand side) in as the first argument in the RHS. However, you can change this by using %&gt;% in combination with a ., to specify which argument you want it to be inputted as: 100 %&gt;% rnorm(10, ., 1) is equal to rnorm(10, 100, 1) The default behaviour: 100 %&gt;% rnorm(0, 1) is implicitly saying 100 %&gt;% rnorm(., 0, 1), which is equal to rnorm(100, 0, 1). ► Question Translate the following statements between Base R and sequences of pipes. The first is shown for you. 1 Base R: round(mean(rnorm(100,0,1))) Pipes : rnorm(100,0,1) %&gt;% mean() %&gt;% round() 2 Base R: x&lt;-10:100 round(exp(diff(log(x))), 2) Pipes: ► Solution 10:100 %&gt;% log() %&gt;% diff() %&gt;% exp() %&gt;% round(2) 3 Pipes: 6 %&gt;% round(pi, digits=.) Base R: ► Solution round(pi, digits=6) 1.2.2 Grouping, summarising, filtering, mutating and selecting Tidyverse also gives us really useful functions for wrangling data. There are many, but some of the key ones we’ll learn here are: select() extracts columns filter() subsets data based on conditions mutate() adds new variables group_by() group related rows together summarise()/summarize() reduces values down to a single summary For a quick example, if we want to calculate the median accuracy for each category, but only after removing those with an accuracy &lt;50, we could use: speech_ses %&gt;% filter(Accuracy&gt;50) %&gt;% group_by(Category) %&gt;% summarise( mdn_accuracy = median(Accuracy) ) And if we wanted to also calculate the mean accuracy for each category, we could add: speech_ses %&gt;% group_by(Category) %&gt;% summarise( n = n(), mean_acc = mean(Accuracy) ) ► Question Load the tidyverse, and haven package, and read in the data using read_sav() (.sav is the type of file which comes out of another stats software, SPSS). You can download the data from https://edin.ac/34n6AWA to your computer, and then read it in. library(tidyverse) exam &lt;- haven::read_sav(&quot;data/exam.sav&quot;) Using the exam.sav data: ► Question Calculate the mean score for each exam ► Solution exam %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42.9 ## 2 2 37.9 ## 3 3 38.9 ► Question Calculate the mean score for each exam for female students only ► Solution exam %&gt;% filter(gender==&quot;f&quot;) %&gt;% group_by(exam) %&gt;% summarize(M = mean(scores)) ## # A tibble: 3 x 2 ## exam M ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 ## 2 2 36.6 ## 3 3 38.1 ► Question Make a new dataframe containing only the exam scores for males for exam number 1, with a new variable indicating whether they passed or not (pass = a score of 40) ► Solution exam_m1 &lt;- exam %&gt;% filter(exam == 1, gender == &quot;m&quot;) %&gt;% mutate(pass = ifelse(scores&gt;40,&quot;pass&quot;,&quot;fail&quot;)) ► Question Calculate the average score for each exam for male and female students\")` ► Solution exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) ## # A tibble: 6 x 3 ## # Groups: exam [3] ## exam gender M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 f 43.1 ## 2 1 m 42.7 ## 3 2 f 36.6 ## 4 2 m 39.1 ## 5 3 f 38.1 ## 6 3 m 39.7 # use spread() to make it easier to compare exam %&gt;% group_by(exam, gender) %&gt;% summarize(M = mean(scores)) %&gt;% spread(gender, M) ## # A tibble: 3 x 3 ## # Groups: exam [3] ## exam f m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 43.1 42.7 ## 2 2 36.6 39.1 ## 3 3 38.1 39.7 1.2.3 Reshaping The same data can be represented in many different ways. We often discern between long and wide formats, and each of these are useful in different ways. Consider, the below example, showing the same data in long format on the left, and in wide on the right. There are some useful functions which we can use to move between these formats: gather() and spread(). Check out the explanation of them in the reading, section 12.3 A newer version of these are pivot_longer() and pivot_wider(), but gather() and spread() will continue to be available if you want them. Data overview The USArrests data set (comes with R) contains violent crime arrests (per 100,000 residents) in each of the 50 states in the USA in 1973 and the percent of the population of each state that lived in urban areas. You can see it by just typing USArrests in R. ► Question Convert the USArrests data set from a wide to a long format so that instead of separate variables for each crime type (Murder, Assault, Rape), there is one variable that identifies the crime type and one variable that contains the rates for each crime type for each state. ► Solution x &lt;- gather(USArrests, key=&quot;CrimeType&quot;, value=&quot;Rate&quot;, Murder, Assault, Rape) # or x &lt;- pivot_longer(USArrests, cols = c(Murder, Assault, Rape), names_to = &quot;CrimeType&quot;, values_to = &quot;Rate&quot;) ► Question Make a scatterplot showing the relationship between each type of violent crime rate and percent of population living in urban areas. ► Solution ggplot(x, aes(UrbanPop, Rate)) + facet_wrap(~CrimeType, scales=&quot;free&quot;, nrow=1) + geom_point() + stat_smooth(method=&quot;lm&quot;) Less guidance Data overview The ability data set in the psych package contains accuracy of 1525 subjects on 16 multiple choice IQ-test-type questions. The questions are of 4 types: basic reasoning, letter sequence, matrix reasoning, and spatial rotation. There are four questions of each type. You can see the by typing psych::ability (those :: are just a way of accessing something from inside a package without loading it). ► Question Tidy the data and make a graph of average accuracy for each question type. You might have to use as_tibble(ability) or as.data.frame(ability) because it is initially stored as a matrix. Hint: the separate() function may come in handy at some point. ► Solution iq &lt;- as_tibble(psych::ability) %&gt;% pivot_longer(., cols=1:16, names_to = &quot;Item&quot;, values_to = &quot;Correct&quot;) %&gt;% # gather(key=&quot;Item&quot;, value=&quot;Correct&quot;, 1:16) %&gt;% # The gather() alternative... separate(Item, c(&quot;Domain&quot;, &quot;Number&quot;)) ggplot(iq, aes(Domain, Correct)) + stat_summary(fun.y = mean, geom=&quot;bar&quot;) 1.3 Reproducible research with RMarkdown We’re also going to start to use RMarkdown. This is a really useful means of making a report reproducible. Essentially, it is a combination of R code and normal text. It will require learning a few new formatting rules (the “markdown” bit), but it means that in one file you can read in and analyse your data, and compile it to a pdf. Which essentially means that if your data or analysis changes, then the results you report change too without having to edit them! 1.3.1 Convert a script into a R Notebook Open your script from the exercises so far. Compile a HTML report from that script. ► Question Create a new R Notebook file, fill it in with the content of your script from the exercises so far.\")` Hint: R code goes into R chunks, add some text in between chunks. Add formatting to make it look nicer: headers, bold, italics, etc (see the cheat-sheet) Add chunk options to suppress extraneous messages and warnings, and to control the size of figures. ► Question Knit the notebook into a HTML file. "],
["basic-multilevel-regression.html", "Chapter 2 Basic Multilevel Regression 2.1 LME4 2.2 Introduction to MLR 2.3 Logistic MLR", " Chapter 2 Basic Multilevel Regression We’re going to start using multilevel modelling. There are a whole lot of different names people use for this sort of methodology (hierarchical linear models, linear mixed-effect models, mixed models, nested data models, random coefficient, random-effects models, random parameter models… and so on). What the idea boils down to is that model parameters vary at more than one level. Packages lme4 tidyverse effects Load the tidyverse, lme4 and effects packages (install them if you haven’t already). library(tidyverse) library(lme4) library(effects) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/36bD1s0 (Visual search data) and https://edin.ac/2QwG7SG (Novel world learning data). Background &amp; Reading Winter, 2013 Brauer &amp; Curtin, 2018, (pdf) Luke, 2017 2.1 LME4 We’re going to use the lme4 package, and specifically the functions lmer() and glmer(). “(g)lmer” here stands for “(generalised) linear mixed effects regression”. You will have seen some use of these functions in the lectures. The broad syntax is: lmer(formula, REML = logical, data = dataframe) The formula bit is similar to what we did with a standard linear model (lm()) in that it has the outcome ~ explanatory variables structure. However, we now have the addition of the random effect terms, specified in parenthesis with the | operator separating parameters on the LHS and a grouping factor on the RHS. Below are a selection of different formulas for specifying different random effect structures. Formula Alternative Meaning \\(\\text{(1 | g)}\\) \\(\\text{1 + (1 | g)}\\) Random intercept with fixed mean \\(\\text{0 + offset(o) + (1 | g)}\\) \\(\\text{-1 + offset(o) + (1 | g)}\\) Random intercept with a priori means \\(\\text{(1 | g1/g2)}\\) \\(\\text{(1 | g1) + (1 | g1:g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) within \\(g1\\) \\(\\text{(1 | g1) + (1 | g2)}\\) \\(\\text{1 + (1 | g1) + (1 | g2)}\\) Intercept varying among \\(g1\\) and \\(g2\\) \\(\\text{x + (x | g)}\\) \\(\\text{1 + x + (1 + x | g)}\\) Correlated random intercept and slope \\(\\text{x + (x || g)}\\) \\(\\text{1 + x + (x | g) + (0 + x | g)}\\) Uncorrelated random intercept and slope Table 1: Examples of the right-hand-sides of mixed effects model formulas. \\(g\\), \\(g1\\), \\(g2\\) are grouping factors, covariates and a priori known offsets are \\(x\\) and \\(o\\). Errors and warnings? For large datasets and/or complex models (lots of random-effects terms), it is quite common to get a convergence warning. There are lots of different ways to deal with these (to try to rule out hypotheses about what is causing them). For now, if lmer() gives you convergence errors, you could try changing the optimizer. Bobyqa is a good one: add control = lmerControl(optimizer = \"bobyqa\") when you run your model. 2.2 Introduction to MLR 2.2.1 Exercise 1 County-level suicide rate data from Public Health England (PHE) is available at https://edin.ac/36xdhas and covers the period from 2001 to 2016. It contains information for a number of different indicators over this period for a selection of counties in England. ► Question 1 Using multilevel regression, study the following: Did the regions differ in their suicide rates? Did the regions differ in ther slopes of change of suidice rate? Make a plot of the fitted values from the model Steps: First, get acquainted with the data. It contains various different indicators but we’re only interested in one (suicide rates), so you might want to filter() the data, using the skills you learnt last week. You may also want to center your data on the year 2001, so that the intercept (e.g., at time 0) in your models is interpretable. Then think about what test you can do to answer the questions above. Lastly, passing your model to the effect() function (from the effects package) can give you all the data you need to construct your plot. Hint: Try comparing models with and without different differences in/slopes of suicide rates for each region. Think also about the grouping of the data points, and how this is represented in your random effect structure. ► Solution Loading load(url(&quot;https://edin.ac/36xdhas&quot;)) #load Public Health England data unique(mh_phe[, 1:2]) #check list of mental health indicators: suicide is 41001 ## IndicatorID IndicatorName ## 1 848 Depression: Recorded prevalence (aged 18+) ## 760 41001 Suicide rate ## 8056 90275 Percentage of physically active adults - historical method ## 8664 90646 Depression: QOF incidence (18+) - new diagnosis # select data and shift Year variable so baseline year (2001) is 0 suicide_dat &lt;- filter(mh_phe, IndicatorID == 41001) %&gt;% mutate(Time = Year - 2001) Modelling # base model: just change over time m &lt;- lmer(Value ~ Time + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add differences between regions m.0 &lt;- lmer(Value ~ Time + Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # add slope differences between regions m.1 &lt;- lmer(Value ~ Time * Region + (Time | County), data = suicide_dat, REML = F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) # compare models anova(m, m.0, m.1) ## Data: suicide_dat ## Models: ## m: Value ~ Time + (Time | County) ## m.0: Value ~ Time + Region + (Time | County) ## m.1: Value ~ Time * Region + (Time | County) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m 6 39961 40002 -19974 39949 ## m.0 14 39924 40019 -19948 39896 53.136 8 1.015e-08 *** ## m.1 22 39918 40068 -19937 39874 21.277 8 0.006447 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It looks like regions differ overall in their suicide rates (addition of Region predictor in m.0) and in slope of change (addition of interaction in m.1) Plotting # you can extract the fitted values using the effect() function from the effects package. ef &lt;- as.data.frame(effect(&quot;Time:Region&quot;, m.1)) ggplot(ef, aes(Time, fit, color=Region)) + geom_line() + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) + labs(y=&quot;Fitted values&quot;) 2.2.2 Exercise 2 The weight maintenance data (WeightMaintain3), a made-up data set based on Lowe et al. (2014, Obesity, 22, 94-100), contains information on overweight participants who completed a 12-week weight loss program, and were then randomly assigned to one of three weight maintenance conditions: None (Control) MR (meal replacements): use MR to replace one meal and snack per day ED (energy density intervention): book and educational materials on purchasing and preparing foods lower in ED (reducing fat content and/or increasing water content of foods) Weight was assessed at baseline (start of maintenance), 12 months post, 24 months post, and 36 months post. load(url(&quot;https://edin.ac/2tFIedK&quot;)) summary(WeightMaintain3) ## ID Condition Assessment WeightChange ## 101 : 4 None:240 Min. :0.00 Min. :-8.3781 ## 102 : 4 ED :240 1st Qu.:0.75 1st Qu.:-0.5024 ## 103 : 4 MR :240 Median :1.50 Median : 0.7050 ## 104 : 4 Mean :1.50 Mean : 1.4438 ## 105 : 4 3rd Qu.:2.25 3rd Qu.: 2.8806 ## 106 : 4 Max. :3.00 Max. :14.9449 ## (Other):696 ► Question 2 Overall, did the participants maintain their weight loss or did their weights change? ► Solution m.null &lt;- lmer(WeightChange ~ 1 + (Assessment | ID), data=WeightMaintain3, REML=F) m.base &lt;- lmer(WeightChange ~ Assessment + (Assessment | ID), data=WeightMaintain3, REML=F) anova(m.null, m.base) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.66 1 6.782e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Weights changed over course of assessment period: \\(\\chi^2(1)=56.5, p &lt;&lt; 0.0001\\) ► Question 3 Did the groups differ in overall weight change and rate of weight gain (non-maintenance)? ► Solution m.int &lt;- lmer(WeightChange ~ Assessment + Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) m.full &lt;- lmer(WeightChange ~ Assessment*Condition + (Assessment | ID), data=WeightMaintain3, REML=F, control = lmerControl(optimizer = &quot;bobyqa&quot;)) anova(m.null, m.base, m.int, m.full) ## Data: WeightMaintain3 ## Models: ## m.null: WeightChange ~ 1 + (Assessment | ID) ## m.base: WeightChange ~ Assessment + (Assessment | ID) ## m.int: WeightChange ~ Assessment + Condition + (Assessment | ID) ## m.full: WeightChange ~ Assessment * Condition + (Assessment | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.null 5 2638.0 2660.9 -1314.0 2628.0 ## m.base 6 2579.4 2606.8 -1283.7 2567.4 60.6605 1 6.782e-15 *** ## m.int 8 2573.9 2610.6 -1279.0 2557.9 9.4418 2 0.008907 ** ## m.full 10 2537.5 2583.3 -1258.8 2517.5 40.3814 2 1.703e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes: Group difference: \\(\\chi^2(2)=9.4, p &lt; 0.01\\) Group slope difference: \\(\\chi^2(2)=40.4, p &lt;&lt; 0.0001\\) Note: m.int is difficult to interpret in light of the massive effect on slope coef(summary(m.full)) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.06038642 0.09807901 517.4922 0.6156916 5.383688e-01 ## Assessment 1.84917936 0.18394628 180.4609 10.0528229 3.677753e-19 ## ConditionED -0.14303302 0.13870466 517.4922 -1.0312056 3.029261e-01 ## ConditionMR -0.14944649 0.13870466 517.4922 -1.0774439 2.817840e-01 ## Assessment:ConditionED -1.74949968 0.26013932 180.4609 -6.7252412 2.234561e-10 ## Assessment:ConditionMR -0.83624053 0.26013932 180.4609 -3.2145872 1.547766e-03 Compared to no intervention, weight (re)gain was 1.75 lbs/year slower for the ED intervention and 0.84 lbs/year slower for the MR intervention. Note that baseline weight difference parameters are not significantly different from 0. ► Question 4 Make a graph of the model fit and the observed data ► Solution There are lots of ways you can do this. Using the effect() function again (and then adding the means and SEs from the original data): ef &lt;- as.data.frame(effect(&quot;Assessment:Condition&quot;, m.full)) ggplot(ef, aes(Assessment, fit, color=Condition)) + geom_line() + stat_summary(data=WeightMaintain3, aes(y=WeightChange), fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + theme_bw() + scale_color_brewer(palette = &quot;Set1&quot;) 2) Using the fitted() function to extract and plot fitted values from the model: ggplot(WeightMaintain3, aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=fitted(m.full)), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) Or, alternatively, using fortify(): ggplot(fortify(m.full), aes(Assessment, WeightChange, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;, size=1) + stat_summary(aes(y=.fitted), fun=mean, geom=&quot;line&quot;) + theme_bw(base_size=12) + scale_color_manual(values=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;)) ► Question 5 Examine the parameter estimates and interpret them (i.e., what does each parameter represent?) ► Solution round(coef(summary(m.full)), 3) ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.060 0.098 517.492 0.616 0.538 ## Assessment 1.849 0.184 180.461 10.053 0.000 ## ConditionED -0.143 0.139 517.492 -1.031 0.303 ## ConditionMR -0.149 0.139 517.492 -1.077 0.282 ## Assessment:ConditionED -1.749 0.260 180.461 -6.725 0.000 ## Assessment:ConditionMR -0.836 0.260 180.461 -3.215 0.002 (Intercept) ==&gt; baseline weight change in None group Assessment ==&gt; slope of weight change in None group ConditionED ==&gt; baseline weight change in ED group relative to None group ConditionMR ==&gt; baseline weight change in MR group relative to None group Assessment:ConditionED ==&gt; slope of weight change in ED group relative to None group Assessment:ConditionMR ==&gt; slope of weight change in MR groups relative to None group 2.3 Logistic MLR 2.3.1 Exercise 3 load(url(&quot;https://edin.ac/2QwG7SG&quot;)) In the nwl data set (accessed using the code above), participants with aphasia are separated into two groups based on the general location of their brain lesion: anterior vs. posterior. There is data on the numbers of correct and incorrect responses participants gave in each of a series of experimental blocks. There were 7 learning blocks, immediately followed by a test. Finally, participants also completed a follow-up test. Figure 2.1 shows the differences between groups in the average proportion of correct responses at each point in time (i.e., each block, test, and follow-up) Figure 2.1: Differences between groups in the average proportion of correct responses at each block Compare the two groups (those with anterior vs. posterior lesions) with respect to their responses. Tip: Remember that you can use cbind() to specify the numbers of successes and failures as the outcome in a binomial regression (and remember to specify the family argument in glmer()). ► Question 5 Is the learning rate (training data) different between these two groups? ► Solution m.base &lt;- glmer(cbind(NumCorrect, NumError) ~ block + (block | ID), data = filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc0 &lt;- glmer(cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) m.loc1 &lt;- glmer(cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ID), data=filter(nwl, block &lt; 8, !is.na(lesion_location)), family=binomial) #summary(m.loc1) anova(m.base, m.loc0, m.loc1, test=&quot;Chisq&quot;) ## Data: filter(nwl, block &lt; 8, !is.na(lesion_location)) ## Models: ## m.base: cbind(NumCorrect, NumError) ~ block + (block | ID) ## m.loc0: cbind(NumCorrect, NumError) ~ block + lesion_location + (block | ## m.loc0: ID) ## m.loc1: cbind(NumCorrect, NumError) ~ block * lesion_location + (block | ## m.loc1: ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 5 454.12 466.27 -222.06 444.12 ## m.loc0 6 454.66 469.25 -221.33 442.66 1.4572 1 0.2274 ## m.loc1 7 454.47 471.48 -220.23 440.47 2.1974 1 0.1382 No significant difference in learning rate between groups: \\(\\chi^2(2)=2.2, p = 0.138\\) ► Question 6 Does immediate test performance differ between lesion location groups, and does their retention from immediate to follow-up test differ? ► Solution nwl_test &lt;- filter(nwl, block &gt; 7, !is.na(lesion_location)) %&gt;% mutate( Phase = fct_relevel(factor(Phase),&quot;Immediate&quot;) ) m.recall.loc &lt;- glmer(cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID), nwl_test,family=&quot;binomial&quot;) summary(m.recall.loc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(NumCorrect, NumError) ~ Phase * lesion_location + (Phase | ID) ## Data: nwl_test ## ## AIC BIC logLik deviance df.resid ## 142.6 150.9 -64.3 128.6 17 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.31708 -0.45014 0.03291 0.46924 1.09355 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 0.47660 0.6904 ## PhaseFollow-up 0.02539 0.1593 -1.00 ## Number of obs: 24, groups: ID, 12 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11375 0.35128 -0.324 0.746 ## PhaseFollow-up -0.02452 0.24634 -0.100 0.921 ## lesion_locationposterior 0.99918 0.46868 2.132 0.033 * ## PhaseFollow-up:lesion_locationposterior -0.25437 0.33904 -0.750 0.453 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) PhsFl- lsn_lc ## PhaseFllw-p -0.578 ## lsn_lctnpst -0.750 0.435 ## PhsFllw-p:_ 0.421 -0.729 -0.589 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular (Intercept) ==&gt; Anterior lesion group performance in immediate test PhaseFollow-up ==&gt; Change in performance (anterior lesion group) from immediate to follow-up test lesion_locationposterior ==&gt; Posterior lesion group performance in immediate test relative to anterior lesion group performance in immediate test PhaseFollow-up:lesion_locationposterior ==&gt; Change in performance from immediate to follow-up test, posterior lesion group relative to anterior lesion group ► Question 6 Extra: Recreate the visualisation in Figure 2.1. ► Solution ggplot(filter(nwl, !is.na(lesion_location)), aes(block, PropCorrect, color=lesion_location, shape=lesion_location)) + #geom_line(aes(group=ID),alpha=.2) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(data=filter(nwl, !is.na(lesion_location), block &lt;= 7), fun=mean, geom=&quot;line&quot;) + geom_hline(yintercept=0.5, linetype=&quot;dashed&quot;) + geom_vline(xintercept=c(7.5, 8.5), linetype=&quot;dashed&quot;) + scale_x_continuous(breaks=1:9, labels=c(1:7, &quot;Test&quot;, &quot;Follow-Up&quot;)) + theme_bw(base_size=10) + labs(x=&quot;Block&quot;, y=&quot;Proportion Correct&quot;, shape=&quot;Lesion\\nLocation&quot;, color=&quot;Lesion\\nLocation&quot;) "],
["mlr-for-longitudinal-data-growth-curve-analysis.html", "Chapter 3 MLR for longitudinal data (growth curve analysis) 3.1 Introduction 3.2 Exercise 1 3.3 Exercise 2: Writing up, and Logistic GCA", " Chapter 3 MLR for longitudinal data (growth curve analysis) Packages lme4 tidyverse effects We will also be needing to access some useful functions from Dan for getting p-values and coding polynomials. The source() function basically takes in R code and evaluates it. You can download R scripts with Dan’s code here and here. However, you can also source them directly from the URLs, and read them into your environment: library(tidyverse) library(lme4) library(effects) source(&#39;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/get_pvalues.R&#39;) source(&quot;https://uoe-psychology.github.io/uoe_psystats/multivar/functions/code_poly.R&quot;) Lecture Slides The lecture slides can be accessed here. The data for the lecture can be found at https://edin.ac/2TieJK0. Background &amp; Reading Curran et al., 2010 Winter &amp; Wieling, 2016 3.1 Introduction This week we are going to look at how we might use MLR to study longitudinal data. That is, data in which repeated measurements are taken over a continuous domain, with the potential for observations to be unevenly spaced, or missing at certain points, and which are likely to display non-linear patterns. The lab will focus on including higher-order polynomials in MLR to capture non-linearity. Solutions are available Solutions are already available for the first half of today’s lab. We encourage you to try working through the questions yourself before looking at the solutions. 3.2 Exercise 1 Use natural (not orthogonal) polynomials to analyze decline in performance of 30 individuals with probable Alzheimer’s disease on three different kinds of tasks - Memory, complex activities of daily living (ADL), and simple activities of daily living. ► Question 1 Read the data in to R from the following url: https://edin.ac/35Njwpl . The data is in .rda format. ► Solution load(url(&quot;https://edin.ac/35Njwpl&quot;)) summary(Az) ## Subject Time Task Performance ## 1 : 30 Min. : 1.0 cADL :300 Min. : 2.00 ## 2 : 30 1st Qu.: 3.0 sADL :300 1st Qu.:40.00 ## 3 : 30 Median : 5.5 Memory:300 Median :52.00 ## 4 : 30 Mean : 5.5 Mean :49.27 ## 5 : 30 3rd Qu.: 8.0 3rd Qu.:61.00 ## 6 : 30 Max. :10.0 Max. :85.00 ## (Other):720 ► Question 2 Plot the observed data (the performance over time for each type of task). ► Solution ggplot(Az, aes(Time, Performance, color=Task, fill=Task)) + stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, color=NA, alpha=0.5) + stat_summary(fun=mean, geom=&quot;line&quot;) ► Question 3 Why are natural polynomials more useful for these data? ► Solution Because it’s useful to know whether there are task differences at the starting baseline point ► Question 4 Fit the GCA model(s). Steps required: Add 1st and 2nd order natural polynomials to the data using the code_poly() function. Create a baseline model, in which performance varies over time, but no differences in Task are estimated. Think about random effect structure - what are the observations grouped by? Are observations nested? Create a new model with a fixed effect of Task Create a new model in which performance varies linearly over time between Task type. Create a new model in which linear and quadratic performance over time varies between Task type. Run model comparisons. ► Solution # prep for analysis Az &lt;- code_poly(Az, predictor=&quot;Time&quot;, poly.order=2, orthogonal=F, draw.poly = F) # fit the full model incrementally m.base &lt;- lmer(Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.0 &lt;- lmer(Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.1 &lt;- lmer(Performance ~ poly1*Task + poly2 + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) m.Az.full &lt;- lmer(Performance ~ (poly1 + poly2)*Task + (poly1 + poly2 | Subject) + (poly1 + poly2 | Subject:Task), data=Az, REML=F) anova(m.base, m.0, m.1, m.Az.full) ## Data: Az ## Models: ## m.base: Performance ~ (poly1 + poly2) + (poly1 + poly2 | Subject) + (poly1 + ## m.base: poly2 | Subject:Task) ## m.0: Performance ~ (poly1 + poly2) + Task + (poly1 + poly2 | Subject) + ## m.0: (poly1 + poly2 | Subject:Task) ## m.1: Performance ~ poly1 * Task + poly2 + (poly1 + poly2 | Subject) + ## m.1: (poly1 + poly2 | Subject:Task) ## m.Az.full: Performance ~ (poly1 + poly2) * Task + (poly1 + poly2 | Subject) + ## m.Az.full: (poly1 + poly2 | Subject:Task) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## m.base 16 3804.6 3881.5 -1886.3 3772.6 ## m.0 18 3807.8 3894.2 -1885.9 3771.8 0.8242 2 0.6623 ## m.1 20 3781.4 3877.4 -1870.7 3741.4 30.4282 2 2.469e-07 *** ## m.Az.full 22 3612.9 3718.5 -1784.4 3568.9 172.4920 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In these solutions we are using Dan’s code_poly() function. All that this does is add polynomial terms to your dataframe (it can also give you a plot if you want one by setting draw.poly = TRUE). As always with R, you can do the same thing in lots of different ways. We could, for example, pass the poly() function directly to our model (without making storing them as variables in our dataframe), for instance: m.base &lt;- lmer(Performance ~ poly(Time, degree = 2, raw = TRUE) + (poly(Time, degree = 2, raw = TRUE) | Subject) + (poly(Time, degree = 2, raw = TRUE) | Subject:Task), data=Az, REML=F) If you’re struggling with the idea of polynomials, try looking at the output of the following commands: # note, the &quot;raw&quot; here is the same as &quot;natural&quot;. poly(1:10, 3, raw = TRUE) poly(1:10, 3, raw = FALSE) # let&#39;s plot them quickly using matplot() matplot(poly(1:10, 3, raw = TRUE), type = &#39;l&#39;) matplot(poly(1:10, 3, raw = FALSE), type = &#39;l&#39;) Interpret the results of your full model ► Question 5 Look at the summary() of your full model, and try using the get_pvalues() function on it. Which terms show significant effects of experimental factors? ► Solution Get p-values for your full model: get_pvalues(m.Az.full) ## Estimate Std..Error df t.value Pr...t.. p.normal p.normal.star ## (Intercept) 67.161666667 0.9307397 49.93913 72.1594510 3.604254e-52 0.000000e+00 *** ## poly1 -3.287803030 0.3417915 45.46197 -9.6193228 1.556027e-12 0.000000e+00 *** ## poly2 0.009469697 0.0124348 89.99977 0.7615478 4.483209e-01 4.463300e-01 ## TasksADL 0.095000000 0.8117243 60.06639 0.1170348 9.072229e-01 9.068325e-01 ## TaskMemory 1.240000000 0.8117243 60.06639 1.5276124 1.318580e-01 1.266088e-01 ## poly1:TasksADL 1.362196970 0.2675361 61.08255 5.0916376 3.654585e-06 3.549840e-07 *** ## poly1:TaskMemory -3.977070707 0.2675361 61.08255 -14.8655469 5.829334e-22 0.000000e+00 *** ## poly2:TasksADL -0.013257576 0.0174789 89.67383 -0.7584903 4.501465e-01 4.481575e-01 ## poly2:TaskMemory 0.338888889 0.0174789 89.67383 19.3884563 7.876776e-34 0.000000e+00 *** Intercepts are not different: performance in all tasks starts out the same (thanks, natural polynomials) Linear slopes are different: compared to complex ADL tasks, decline in simple ADL tasks is slower and decline in Memory is faster. Quadratic term is different for Memory: decline in cADL and sADL tasks is approximately linear, decline in Memory has more curvature (reaching floor?) ► Question 6 To what extent do model comparisons and the parameter-specific p-values yield the same results? ► Solution Model comparisons suggest: Linear slopes are different: \\(\\chi^2(2)=30.56, p &lt;&lt; 0.0001\\) (comparison m.0 and m.1 above). Quadratic term is different: \\(\\chi^2(2)=172.36, p &lt;&lt; 0.0001\\) (comparison m.1 and m.Az.full above). Note: We can’t investigate the intercept difference via the model comparisons above. Comparison between m.base and m.0 indicates difference holding polynomial terms constant (not the conditional effect where poly1 and poly2 are 0).** Please note: There was an error in last week’s lab about assessing ‘baseline’ differences between groups. Model comparison between y~time and y~time+g does not assess the difference in y between groups in g where time==0, but instead assesses the difference in y between groups in g holding time constant. Last week’s lab has been amended. Apologies for any confusion! ► Question 7 Plot model fit ► Solution ggplot(Az, aes(Time, Performance, color=Task)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(fun=mean, geom=&quot;line&quot;, aes(y=fitted(m.Az.full))) 3.3 Exercise 2: Writing up, and Logistic GCA We saw in the lecture a walk-through of using GCA to model the eye-tracking data from a spoken word-to-picture matching task. ► Question 8 The model we saw in the lecture had the following structure and results: &gt; m.full &lt;- lmer(meanFix ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 | Subject) + (poly1+poly2 | Subject:Condition), control = lmerControl(optimizer=&quot;bobyqa&quot;), data=TargetFix, REML=F) &gt; coef(summary(m.full)) Estimate Std. Error t value (Intercept) 0.4773227513 0.01385240 34.457775306 poly1 0.6385603705 0.05993519 10.654181583 poly2 -0.1095979256 0.03848819 -2.847573180 poly3 -0.0932611870 0.02041640 -4.567955536 ConditionLow -0.0581122429 0.01901291 -3.056462582 poly1:ConditionLow 0.0003188189 0.06330556 0.005036191 poly2:ConditionLow 0.1635455113 0.05426498 3.013831365 poly3:ConditionLow -0.0020869051 0.02014728 -0.103582452 Write up the results of the model ► Solution There are two rules of thumb for reporting growth curve analysis results: Clearly describe each of the three key components of the model: the functional form (third-order orthogonal polynomial), the fixed effects (effect of Condition on all time terms), and the random effects (effect of Subject on each of the time terms and nested effects of Subject-by-Condition on each of the time terms except the cubic). Depending on the circumstances and complexity of the model, you may want to include additional information about the factors and why they were included or not. It’s also a good idea to report which method was used for computing p-values. For key findings, report parameter estimates and standard errors along with significance tests. In some cases the model comparison is going to be enough, but for key findings, the readers should want to see the parameter estimates. The parameter estimate standard errors are critical for interpreting the estimates, so those should be reported as well. The t-values are not critical to report (they are just Estimate divided by the Std Error, so they can always be computed from the reported estimates and standard errors). If there are many estimated parameters, it may be a good idea to focus the main text discussion on the most important ones and report the full set in a table or appendix. Here is how we might report the results from the example above: [Note, we haven’t included Table 1 here. If you want a nice way of creating tables, try installing the sjPlot package, and using tab_model() on your model.] Growth curve analysis (Mirman, 2014) was used to analyze the target gaze data from 300ms to 1000ms after word onset. The overall time course of target fixations was modeled with a third-order (cubic) orthogonal polynomial and fixed effects of Condition (Low vs. High frequency; within-participants) on all time terms. The model also included participant random effects on all time terms and participant-by-condition random effects on all time terms except the cubic (estimating random effects is “expensive” in terms of the number of observation required, so this cubic term was excluded because it tends to capture less-relevant effects in the tails). There was a significant effect of Condition on the intercept term, indicating lower overall target fixation proportions for the Low condition relative to the High condition (Estimate = -0.058, SE = 0.019, p &lt; 0.01). There was also a significant effect on the quadratic term, indicating shallower curvature - slower word recognition - in the Low condition relative to the High condition (Estimate = 0.16, SE = 0.054, p &lt; 0.01). All other effects of Condition were not significant (see Table 1 for full results). ► Question 9 Above, we analysed the proportion of fixations to the target picture in a given 50~ms time bin (the meanFix variable). We can express this differently, in terms of the number of samples in each 50~ms bin in which there were fixations to the target, and the total number of samples. This can lend itself to being modelled as a binomial (where success is fixation on the target). In the data, the sumFix variable contains the number of samples in which the target was fixated upon, and the N variable contains the total number of samples in that bin. Like we saw last week, we can model a binomial using cbind(num_successes, num_failures), so here we can use cbind(sumFix, N-sumFix)~ ... Re-analyze TargetFix data using logistic GCA. The data (.rda format) is available at https://edin.ac/2TieJK0 ► Solution load(url(&quot;https://edin.ac/2TieJK0&quot;)) #make 3rd-order orth poly TargetFix &lt;- code_poly(TargetFix, predictor=&quot;timeBin&quot;, poly.order=3, draw.poly=F) # fit logisitc GCA model m.log &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 | Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 | Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1419.1 1508.0 -685.6 1371.1 276 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.75430 -0.40973 -0.00307 0.37868 2.06240 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject:Condition (Intercept) 0.032340 0.17983 ## poly1 0.401864 0.63393 -0.68 ## poly2 0.147989 0.38469 -0.23 0.73 ## Subject (Intercept) 0.001751 0.04185 ## poly1 0.343612 0.58618 1.00 ## poly2 0.001991 0.04462 -1.00 -1.00 ## poly3 0.027493 0.16581 -1.00 -1.00 1.00 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11675 0.06548 -1.783 0.074591 . ## poly1 2.81834 0.29834 9.447 &lt; 2e-16 *** ## poly2 -0.55911 0.16952 -3.298 0.000973 *** ## poly3 -0.32075 0.12771 -2.512 0.012017 * ## ConditionLow -0.26157 0.09095 -2.876 0.004030 ** ## poly1:ConditionLow 0.06400 0.33134 0.193 0.846849 ## poly2:ConditionLow 0.69503 0.23977 2.899 0.003747 ** ## poly3:ConditionLow -0.07066 0.16617 -0.425 0.670684 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.288 ## poly2 -0.128 0.272 ## poly3 -0.100 -0.228 -0.015 ## ConditionLw -0.690 0.297 0.081 0.012 ## ply1:CndtnL 0.372 -0.552 -0.292 -0.024 -0.541 ## ply2:CndtnL 0.080 -0.230 -0.701 0.034 -0.116 0.415 ## ply3:CndtnL 0.013 -0.020 0.037 -0.637 -0.003 0.031 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Simpler random effects: note that the correlations between Subject-level random effects are all +1.00 or -1.00, so can simplify the structure by removing them: m.log_zc &lt;- glmer(cbind(sumFix, N-sumFix) ~ (poly1+poly2+poly3)*Condition + (poly1+poly2+poly3 || Subject) + (poly1+poly2 | Subject:Condition), data=TargetFix, family=binomial, control = glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m.log_zc) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: cbind(sumFix, N - sumFix) ~ (poly1 + poly2 + poly3) * Condition + ## (poly1 + poly2 + poly3 || Subject) + (poly1 + poly2 | Subject:Condition) ## Data: TargetFix ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 1411.6 1478.3 -687.8 1375.6 282 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.69598 -0.41491 -0.00141 0.33691 2.07563 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject.Condition (Intercept) 0.03404 0.1845 ## poly1 0.42307 0.6504 -0.63 ## poly2 0.15312 0.3913 -0.25 0.70 ## Subject poly3 0.00000 0.0000 ## Subject.1 poly2 0.00000 0.0000 ## Subject.2 poly1 0.44471 0.6669 ## Subject.3 (Intercept) 0.00000 0.0000 ## Number of obs: 300, groups: Subject:Condition, 20; Subject, 10 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.11770 0.06544 -1.798 0.07210 . ## poly1 2.82162 0.31822 8.867 &lt; 2e-16 *** ## poly2 -0.55892 0.17054 -3.277 0.00105 ** ## poly3 -0.31340 0.11646 -2.691 0.00712 ** ## ConditionLow -0.26066 0.09280 -2.809 0.00497 ** ## poly1:ConditionLow 0.06593 0.33782 0.195 0.84526 ## poly2:ConditionLow 0.69049 0.24206 2.853 0.00434 ** ## poly3:ConditionLow -0.06654 0.16627 -0.400 0.68904 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) poly1 poly2 poly3 CndtnL pl1:CL pl2:CL ## poly1 -0.379 ## poly2 -0.129 0.301 ## poly3 -0.018 0.029 -0.054 ## ConditionLw -0.705 0.267 0.092 0.012 ## ply1:CndtnL 0.357 -0.528 -0.284 -0.027 -0.509 ## ply2:CndtnL 0.092 -0.212 -0.703 0.038 -0.131 0.402 ## ply3:CndtnL 0.012 -0.020 0.037 -0.699 -0.003 0.033 -0.056 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Plot model fit ggplot(TargetFix, aes(Time, meanFix, color=Condition)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=fitted(m.log)), fun=mean, geom=&quot;line&quot;) + stat_summary(aes(y=fitted(m.log_zc)), fun=mean, geom=&quot;line&quot;, linetype=&quot;dashed&quot;) + theme_bw() + expand_limits(y=c(0,1)) + labs(y=&quot;Fixation Proportion&quot;, x=&quot;Time since word onset (ms)&quot;) "],
["other-random-effects-structures.html", "Chapter 4 Other random effects structures 4.1 Crossed random effects 4.2 Dealing with issue of non-convergence and singular fits", " Chapter 4 Other random effects structures Packages library(tidyverse) library(lme4) library(lmerTest) library(effects) Lecture Slides The lecture slides can be accessed here. Data from the lecture can be found at https://edin.ac/36UYA00 (active_math_sim.csv) and https://edin.ac/2ShRCNl (problem_solving.Rdata). Background &amp; Reading Baayen et al., 2008 Barr et al., 2013 Matuschek et al., 2017 4.1 Crossed random effects 4.1.1 Exercise 1 The data load(url(&quot;https://edin.ac/2ShRCNl&quot;)) str(problem_solving) ## &#39;data.frame&#39;: 2864 obs. of 5 variables: ## $ Item : Factor w/ 30 levels &quot;AA&quot;,&quot;AB&quot;,&quot;AC&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Prob_Type: Factor w/ 2 levels &quot;Hard&quot;,&quot;Easy&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;contrasts&quot;)= chr &quot;contr.sum&quot; ## $ Subject : Factor w/ 120 levels &quot;S101&quot;,&quot;S102&quot;,..: 119 25 1 102 65 105 56 53 84 86 ... ## $ Condition: Factor w/ 2 levels &quot;Control&quot;,&quot;Treatment&quot;: 2 1 1 1 2 2 1 1 1 2 ... ## ..- attr(*, &quot;contrasts&quot;)= chr &quot;contr.sum&quot; ## $ RT : int 3366 1584 2008 3089 1831 2863 3444 4102 2251 1950 ... Item: word problem, can be Hard or Easy Prob_Type: difficulty level of word problem (16 hard problems, 14 easy problems) Subject: Participant ID, N=120 Condition: whether the participant received the Treatment or not RT: time to solve the problem Note: there is some missing data because trials where the participants failed to solve the problem are excluded. ► Question 1 Check the contrasts of the Condition and Prob_Type variables. ► Solution contrasts(problem_solving$Condition) ## [,1] ## Control 1 ## Treatment -1 contrasts(problem_solving$Prob_Type) ## [,1] ## Hard 1 ## Easy -1 They are sum-coded. Remember what this means when you are interpreting the intercept and coefficients! ► Question 2 Does Prob_Type vary within subjects? Does Prob_Type vary within items? Does Condition vary within subjects? Does Condition vary within items? ► Solution Yes: Each subject solved both Easy and Hard problems. No: Each item was either Easy or Hard. No: Each subject was either in the control condition or the treatment condition. Yes: Each item was solved both by subjects in the control condition and subjects in the treatment condition. ► Question 3 Conduct MLR with crossed random effects of subjects and items to answer these research questions Did the treatment improve problem solving ability? Did the treatment effect differ between hard and easy problems? Hint: You can do this with one model, given what you know about the contrasts. ► Solution mod_ps &lt;- lmer(RT ~ Prob_Type*Condition + (Prob_Type | Subject) + (Condition | Item), data=problem_solving, REML=FALSE) summary(mod_ps) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: RT ~ Prob_Type * Condition + (Prob_Type | Subject) + (Condition | Item) ## Data: problem_solving ## ## AIC BIC logLik deviance df.resid ## 46245.1 46310.7 -23111.6 46223.1 2853 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.8354 -0.5429 -0.1105 0.3643 12.3008 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 74023 272.07 ## Prob_Type1 8912 94.40 1.00 ## Item (Intercept) 153298 391.53 ## Condition1 1391 37.29 1.00 ## Residual 541953 736.17 ## Number of obs: 2864, groups: Subject, 120; Item, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1753.51 77.16 37.28 22.726 &lt; 2e-16 *** ## Prob_Type1 325.24 73.50 30.88 4.425 0.000111 *** ## Condition1 58.34 29.43 111.64 1.982 0.049891 * ## Prob_Type1:Condition1 36.14 17.73 86.76 2.038 0.044607 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Prb_T1 Cndtn1 ## Prob_Type1 -0.023 ## Condition1 0.217 -0.015 ## Prb_Typ1:C1 -0.024 0.379 0.398 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular Because both Condition and Prob_Type are sum-coded, the intercept term is the grand mean. The Prob_Type1 coefficient corresponds to the mean for Prob_Type == \"Hard\" minus the grand mean. Note that sum-coding means that we no longer interpret this as the effect of Prob_Type at a reference level of Condition - instead it may help to think of it as the effect of Prob_Type ‘when Condition == 0’. In the default coding, 0 corresponds to the reference level, but in sum-coding, where Control = 1 and Treatment = -1, what is 0? It’s mid-way between the two (it may help to think of sum-coding similar to what happens when we ‘center’ continuous variables). The Condition1 coefficient corresponds to the mean for Condition == \"Control\" minus the grand mean. The Prob_Type1:Condition1 coefficient corresponds to the effect of Prob_Type when Condition == \"Control\" relative to the effect of Prob_Type at the grand mean. Or, equivalently, the effect of Condition when Prob_Type == \"Hard\" relative to the effect of Condition at the grand mean. Treatment improves problem solving ability (slower reaction times in the control group across both easy and hard problems), \\(\\beta = 58.34, SE = 29.43, p = 0.0499\\) Treatment effect differs between easy and hard problems, with a greater effect for harder problems (greater difference in reaction times between control and treatment groups for hard problems), \\(\\beta = 36.14, SE = 17.73, p = 0.0446\\) ► Question 4 Make a graph of the condition means and variability based on your analysis model. Hint: start withas.data.frame(effect(........)), and pass to it the parameter you’re interested in and your model. ► Solution efx &lt;- as.data.frame(effect(&quot;Prob_Type:Condition&quot;, mod_ps)) ggplot(efx, aes(Condition, fit, color=Prob_Type, ymin=fit-se, ymax=fit+se)) + geom_pointrange() + theme_light() + scale_color_manual(values=c(&quot;lightgreen&quot;,&quot;tomato1&quot;)) + labs(y=&quot;Response Time&quot;, color=&quot;Problem\\nType&quot;) 4.2 Dealing with issue of non-convergence and singular fits Singular fits You may have noticed that a lot of our models over the last few weeks have been giving a warning: boundary (singular) fit: see ?isSingular. Up to now, we’ve been largely ignoring these warnings. However, in this exercise we’re going to look at how to deal with this issue. The warning is telling us that our model has resulted in a ‘singular fit’. Singular fits often indicate that the model is ‘overfitted’ - that is, the random effects structure which we have specified is too complex to be supported by the data. Perhaps the most intuitive advice would be remove the most complex part of the random effects structure (i.e. random slopes). This leads to a simpler model that is not over-fitted. Additionally, when variance estimates are very low for a specific random effect term, this indicates that the model is not estimating this parameter to differ much between the levels of your grouping variable. It might, in some experimental designs, be perfectly acceptable to remove this. A key point here is that when fitting a mixed model, we should think about how the data are generated. Asking yourself questions such as “do we have good reason to assume subjects might vary over time, or to assume that they will have different starting points (i.e., different intercepts)?” can help you in specifying your random effect structure You can read in depth about what this means by reading the help documentation for ?isSingular. For our purposes, a relevant section is copied below: …intercept-only models, or 2-dimensional random effects such as intercept+slope models, singularity is relatively easy to detect because it leads to random-effect variance estimates of (nearly) zero, or estimates of correlations that are (almost) exactly -1 or 1. Convergence warnings Issues of non-convergence can be caused by many things. If you’re model doesn’t converge, it does not necessarily mean the fit is incorrect, however it is is cause for concern, and should be addressed, else you may end up reporting inferences which do not hold. There are lots of different things which you could do which might help your model to converge. A select few are detailed below: double-check the model specification and the data adjust stopping (convergence) tolerances for the nonlinear optimizer, using the optCtrl argument to [g]lmerControl. (see ?convergence for convergence controls). center and scale continuous predictor variables (e.g. with scale) change the optimiser, or use allFit() to try the fit with all available optimizers. This will of course be slow, but is considered ‘the gold standard’; “if all optimizers converge to values that are practically equivalent, then we would consider the convergence warnings to be false positives.” 4.2.1 Exercise 2 The data An experiment was run to replicate “test-enhanced learning” (Roediger &amp; Karpicke, 2006): two groups of 25 participants were presented with material to learn. One group studied the material twice (StudyStudy), the other group studied the material once then did a test (StudyTest). Recall was tested immediately (one minute) after the learning session and one week later. The recall tests were composed of 175 items identified by a keyword (Test_word). The critical (replication) prediction is that the StudyStudy group should perform somewhat better on the immediate recall test, but the StudyTest group will retain the material better and thus perform better on the 1-week follow-up test. load(url(&quot;https://edin.ac/2RWbl6g&quot;)) str(tel) ## &#39;data.frame&#39;: 17498 obs. of 5 variables: ## $ Subject_ID: chr &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; &quot;StudyTest_L&quot; ... ## $ Group : chr &quot;StudyTest&quot; &quot;StudyTest&quot; &quot;StudyTest&quot; &quot;StudyTest&quot; ... ## $ Delay : chr &quot;min&quot; &quot;week&quot; &quot;min&quot; &quot;min&quot; ... ## $ Test_word : chr &quot;van&quot; &quot;dinosaur&quot; &quot;typewriter&quot; &quot;chimney&quot; ... ## $ Correct : num 1 0 0 0 1 1 1 1 0 0 ... ► Question 5 Plot the data. Does it look like the effect was replicated? ► Solution You can make use of stat_summary() again! ggplot(tel, aes(Delay, Correct, col=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;)+ theme_light() It’s more work, but some people might rather calculate the numbers and then plot them directly. It does just the same thing: tel %&gt;% group_by(Delay, Group) %&gt;% summarise( mean = mean(Correct), se = sd(Correct)/sqrt(n()) ) %&gt;% ggplot(., aes(x=Delay, col = Group)) + geom_pointrange(aes(y=mean, ymin=mean-se, ymax=mean+se))+ theme_light() + labs(y = &quot;Correct&quot;) That looks like test-enhanced learning to me! ► Question 6 Test the critical hypothesis using a mixed-effects model. Fit the maximal random effect structure supported by the experimental design. Some questions to consider: Item accuracy is a binary variable. What kind of model will you use? We can expect variability across subjects (some people are better at learning than others) and across items (some of the recall items are harder than others). How should this be represented in the random effects? ► Solution m &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 + Delay + Group | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) summary(m) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: Correct ~ Delay * Group + (1 + Delay | Subject_ID) + (1 + Delay + Group | Test_word) ## Data: tel ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 16289.8 16390.8 -8131.9 16263.8 17485 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -7.4352 -0.4778 0.2763 0.5182 7.5360 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Test_word (Intercept) 1.160006 1.07704 ## Delayweek 0.005543 0.07445 -0.80 ## GroupStudyTest 0.011905 0.10911 -0.93 0.97 ## Subject_ID (Intercept) 2.527104 1.58969 ## Delayweek 0.044399 0.21071 -0.60 ## Number of obs: 17498, groups: Test_word, 175; Subject_ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.28215 0.33082 3.876 0.000106 *** ## Delayweek -1.07133 0.07068 -15.157 &lt; 2e-16 *** ## GroupStudyTest -0.42866 0.45344 -0.945 0.344480 ## Delayweek:GroupStudyTest 0.79433 0.10189 7.796 6.41e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Delywk GrpStT ## Delayweek -0.441 ## GropStdyTst -0.688 0.309 ## Dlywk:GrpST 0.292 -0.679 -0.430 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question 7 Your model with maximal random effects will probably not converge, or will obtain a singular fit. Simplify the model until you achieve convergence. What we’re aiming to do here is to follow Barr et al.’s advice of defining our maximal model and then removing only the terms to allow a non-singular fit. Note: This strategy - starting with the maximal random effects structure and removing terms until obtaining model convergence, is just one approach, and there are drawbacks (see Matuschek et al., 2017). There is no consensus on what approach is best (see ?isSingular). Tip: you can look at the variance estimates and correlations easily by using the VarCorr() function. What jumps out? Hint: Generalization over subjects could be considered more important than over items - if the estimated variance of slopes for Delay and Group by-items are comparatively small, it might be easier to remove them? ► Solution VarCorr(m) ## Groups Name Std.Dev. Corr ## Test_word (Intercept) 1.077036 ## Delayweek 0.074453 -0.803 ## GroupStudyTest 0.109112 -0.930 0.966 ## Subject_ID (Intercept) 1.589687 ## Delayweek 0.210710 -0.600 The by-item slope of Group seems to be quite highly correlated with other by-item terms. For now, we will just simply remove the term (however, we could - if we had theoretical justification - constrain our model so that there was 0 correlation) m2 &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 + Delay | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) VarCorr(m2) ## Groups Name Std.Dev. Corr ## Test_word (Intercept) 1.027503 ## Delayweek 0.055413 -1.000 ## Subject_ID (Intercept) 1.598988 ## Delayweek 0.208731 -0.599 It’s still a singular fit, and the Delay random slope by Test_word variance is extremely low and perfectly correlated with the intercept, let’s try removing that: m3 &lt;- glmer(Correct ~ Delay*Group + (1 + Delay | Subject_ID) + (1 | Test_word), data=tel, family=&quot;binomial&quot;, glmerControl(optimizer = &quot;bobyqa&quot;)) Hooray, the model converged! summary(m3) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: Correct ~ Delay * Group + (1 + Delay | Subject_ID) + (1 | Test_word) ## Data: tel ## Control: glmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 16285.3 16347.4 -8134.6 16269.3 17490 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -7.5190 -0.4795 0.2758 0.5199 8.0698 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Test_word (Intercept) 0.9961 0.9980 ## Subject_ID (Intercept) 2.5170 1.5865 ## Delayweek 0.0387 0.1967 -0.52 ## Number of obs: 17498, groups: Test_word, 175; Subject_ID, 50 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.25720 0.32837 3.829 0.000129 *** ## Delayweek -1.04684 0.06750 -15.510 &lt; 2e-16 *** ## GroupStudyTest -0.39822 0.45243 -0.880 0.378765 ## Delayweek:GroupStudyTest 0.77634 0.09916 7.829 4.91e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Delywk GrpStT ## Delayweek -0.370 ## GropStdyTst -0.687 0.267 ## Dlywk:GrpST 0.252 -0.679 -0.371 ► Question 8 Plot the model-estimated condition means and variability. Hint: This is very similar to question 4. ► Solution ef &lt;- as.data.frame(effect(&quot;Delay:Group&quot;, m3)) ggplot(ef, aes(Delay, fit, color=Group)) + geom_pointrange(aes(ymax=upper, ymin=lower), position=position_dodge(width = 0.2))+ theme_classic() # just for a change :) ► Question 9 What should we do with this information? How can we apply test-enhanced learning to learning R and statistics? ► Solution You’ll get the benefits of test-enhanced learning if you try yourself before looking at the solutions! If you don’t test yourself, you’re more likely to forget it in the long run. 4.2.2 Exercise 3 The data Made-up data from a RCT treatment study: 5 therapists randomly assigned participants to control or treatment group and monitored the participants’ performance over time. There was a baseline test, then 6 weeks of treatment, with test sessions every week (7 total sessions). load(url(&quot;https://edin.ac/2GPGgev&quot;)) summary(tx) ## group session therapist Score PID ## treatment:490 Min. :1 Length:945 Min. :0.1091 Length:945 ## control :455 1st Qu.:2 Class :character 1st Qu.:0.5149 Class :character ## Median :4 Mode :character Median :0.6164 Mode :character ## Mean :4 Mean :0.6315 ## 3rd Qu.:6 3rd Qu.:0.7427 ## Max. :7 Max. :1.2244 ► Question 10 Plot the data. Does it look like the treatment had an effect on the performance score? ► Solution ggplot(tx, aes(session, Score, color=group)) + stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;) + stat_smooth() + theme_classic() Just for fun, let’s add on the individual participant scores, and also make a plot for each therapist. ggplot(tx, aes(session, Score, color=group)) + stat_summary(fun.data = mean_se, geom=&quot;pointrange&quot;) + stat_smooth() + theme_classic() + geom_line(aes(group=PID), alpha=.2)+facet_wrap(~therapist) ► Question 11 Consider these questions when you’re designing your model(s) and use your answers to motivate your model design and interpretation of results: What are the levels of nesting? How should that be reflected in the random effect structure? What is the shape of change over time? Do you need polynomials to model this shape? If yes, what order polynomials? ► Solution There are repeated measures of participants (session). There are also repeated measures of therapists (each one treated many participants). Looks like linear change, don’t need polynomials. Good to know that there is no difference at baseline, so no need for orthogonal time. ► Question 12 Test whether the treatment had an effect using mixed-effects modeling. Specify the maximal model. ► Solution # start with maximal model m1 &lt;- lmer(Score ~ session * group + (1 + session | PID) + (1 + session | therapist), data=tx, REML=FALSE) summary(m1) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ session * group + (1 + session | PID) + (1 + session | therapist) ## Data: tx ## ## AIC BIC logLik deviance df.resid ## -1643.8 -1590.4 832.9 -1665.8 934 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.63292 -0.58800 0.01438 0.55505 2.87871 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## PID (Intercept) 1.373e-02 1.172e-01 ## session 9.848e-04 3.138e-02 -0.60 ## therapist (Intercept) 1.225e-09 3.500e-05 ## session 1.267e-11 3.560e-06 -1.00 ## Residual 5.368e-03 7.327e-02 ## Number of obs: 945, groups: PID, 135; therapist, 5 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.526849 0.015841 134.976208 33.258 &lt; 2e-16 *** ## session 0.033688 0.004100 134.987110 8.217 1.51e-13 *** ## groupcontrol 0.018136 0.022830 134.980597 0.794 0.428360 ## session:groupcontrol -0.020138 0.005908 134.988836 -3.408 0.000862 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sessin grpcnt ## session -0.655 ## groupcontrl -0.694 0.454 ## sssn:grpcnt 0.454 -0.694 -0.655 ## convergence code: 0 ## boundary (singular) fit: see ?isSingular ► Question 13 Try adjusting your model by removing random effects or correlations, examine the model again, and so on.. ► Solution VarCorr(m1) ## Groups Name Std.Dev. Corr ## PID (Intercept) 1.1718e-01 ## session 3.1382e-02 -0.601 ## therapist (Intercept) 3.5001e-05 ## session 3.5595e-06 -1.000 ## Residual 7.3269e-02 There’s a correlation of exactly -1 between the random intercepts and slopes for therapists, and the standard deviation estimate for session|therapist is pretty small. Let’s remove it. m2 &lt;- lmer(Score ~ session * group + (1 + session | PID) + (1 | therapist), data=tx, REML=FALSE) VarCorr(m2) ## Groups Name Std.Dev. Corr ## PID (Intercept) 0.11717 ## session 0.03138 -0.601 ## therapist (Intercept) 0.00000 ## Residual 0.07327 It now looks like estimates for random intercepts for therapists is now 0. If we remove this, our model finally is non-singular: m3 &lt;- lmer(Score ~ session * group + (1 + session | PID), data=tx, REML=FALSE) summary(m3) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ session * group + (1 + session | PID) ## Data: tx ## ## AIC BIC logLik deviance df.resid ## -1649.8 -1611.0 832.9 -1665.8 937 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.63284 -0.58803 0.01439 0.55505 2.87860 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## PID (Intercept) 0.0137278 0.11717 ## session 0.0009847 0.03138 -0.60 ## Residual 0.0053686 0.07327 ## Number of obs: 945, groups: PID, 135 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.526849 0.015840 135.024518 33.262 &lt; 2e-16 *** ## session 0.033688 0.004100 135.003868 8.217 1.51e-13 *** ## groupcontrol 0.018136 0.022827 135.024519 0.794 0.428305 ## session:groupcontrol -0.020138 0.005908 135.003868 -3.409 0.000861 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) sessin grpcnt ## session -0.655 ## groupcontrl -0.694 0.454 ## sssn:grpcnt 0.454 -0.694 -0.655 Lastly, it’s then a good idea to check that the parameter estimates and SE are not radically different across these models (they are virtually identical) summary(m1)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015841487 134.9762 33.2575521 6.821583e-67 ## session 0.03368821 0.004099740 134.9871 8.2171564 1.511024e-13 ## groupcontrol 0.01813605 0.022830001 134.9806 0.7943954 4.283598e-01 ## session:groupcontrol -0.02013829 0.005908354 134.9888 -3.4084434 8.616018e-04 summary(m2)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015840424 135.0051 33.2597829 6.637414e-67 ## session 0.03368821 0.004099570 135.0026 8.2174987 1.507284e-13 ## groupcontrol 0.01813605 0.022828481 135.0051 0.7944483 4.283288e-01 ## session:groupcontrol -0.02013829 0.005908109 135.0026 -3.4085852 8.611671e-04 summary(m3)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 0.52684907 0.015839602 135.0245 33.2615092 6.511331e-67 ## session 0.03368821 0.004099593 135.0039 8.2174516 1.507605e-13 ## groupcontrol 0.01813605 0.022827296 135.0245 0.7944895 4.283047e-01 ## session:groupcontrol -0.02013829 0.005908142 135.0039 -3.4085656 8.612216e-04 ► EXTRA: Question 14 Try the code below to use the allFit() function to fit your final model with all the available optimizers. You might need to install the dfoptim package to get one of the optimizers If you have an older version of lme4, then allFit() might not be directly available, and you will need to run the following: source(system.file(\"utils\", \"allFit.R\", package=\"lme4\")). fits &lt;- allFit(yourmodel) summary(fits) "],
["recap-individual-differences.html", "Chapter 5 Recap &amp; Individual differences A broad overview 5.1 Simple regression 5.2 Clustered (multi-level) data structures 5.3 Random intercept models 5.4 Random slopes 5.5 Fixef, Ranef, Coef 5.6 More levels: Nested and Crossed random-effects 5.7 THE LAB (finally!) 5.8 Exercise 2", " Chapter 5 Recap &amp; Individual differences Packages lme4 require(tidyverse) require(lme4) library(lmerTest) library(effects) Lecture Slides Coming soon A broad overview In a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals. Multi-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them. We can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes). The first part of this lab will guide you through a walkthrough of these concepts, before you tackle some exercises. 5.1 Simple regression Formula: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) R command: lm(outcome ~ predictor, data = dataframe) Note: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +). 5.2 Clustered (multi-level) data structures When our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this. If we separate out our data to show an individual plot for each subject, we can see how the fitted regression line from lm() is assumed to be the same for each subject. 5.3 Random intercept models By including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters. Formula: Level 1: \\(Y_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + \\epsilon_{ij}\\) Level 2: \\(\\beta_{0j} = \\gamma_{00} + u_{0j}\\) Where the expected values of \\(u_0\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{u_0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed. We can now see that the \\(\\beta_0\\) estimate for a particular group \\(j\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(u_{0j}\\)). R command: lmer(outcome ~ predictor + (1 | grouping), data = dataframe) Notice how the fitted line of the random intercept model has an adjustment for each subject. Each subject’s line has been moved up or down accordingly. Shrinkage If you think about it, we might have done a similar thing with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept. However, the estimate of these models will be slightly different: Why? One of the benefits of multi-level models is that our cluster-level estimates are shrunk towards the average depending on a) the level of across-cluster variation and b) the number of datapoints in clusters. 5.4 Random slopes Formula: Level 1: \\(Y_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + \\epsilon_{ij}\\) Level 2: \\(\\beta_{0j} = \\gamma_{00} + u_{0j}\\) \\(\\beta_{1j} = \\gamma_{10} + u_{1j}\\) Where the expected values of \\(u_0\\), \\(u_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{u_0}^2)\\), \\(\\sigma_{u_1}^2)\\), \\(\\sigma_\\epsilon^2)\\) respectively. We will further assume that these are normally distributed. As with the intercept \\(\\beta_0\\), the slope of the predictor \\(\\beta_1\\) is now modelled by a mean and a random effect for each group (\\(u_{1j}\\)). R command: lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe) Note: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part. 5.5 Fixef, Ranef, Coef The plots below show the fitted values from each model for each subject: In the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_u\\). The standard deviation (and variance, which is \\(\\sigma_u^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function). In the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. ► Fixed effects We can extract the fixed effects using the fixef() function: fixef(random_intercept_model) ## (Intercept) x1 ## 413.3736136 -0.9135829 ► Random effects We can extract the deviations for each group from these fixed effect estimates using the ranef() function. ranef(random_intercept_model) ## $subject ## (Intercept) ## sub_308 -29.278163 ## sub_309 -6.830994 ## sub_310 8.492231 ## sub_330 78.673944 ## sub_331 84.713164 ## sub_332 82.639416 ## sub_333 91.245686 ## sub_334 -74.788101 ## sub_335 27.022779 ## sub_337 3.361008 ## sub_349 -93.706202 ## sub_350 -57.004012 ## sub_351 65.920694 ## sub_352 -33.857447 ## sub_369 -64.358442 ## sub_370 -78.286762 ## sub_371 70.669067 ## sub_372 -52.841464 ## sub_373 -119.756827 ## sub_374 97.970426 ## ## with conditional variances for &quot;subject&quot; ► Group-level coefficients We can also see the estimate for each subject directly, using the coef() function. These sometimes get referred to as “Best Linear Unbiased Estimates (BLUPS)”. coef(random_intercept_model) ## $subject ## (Intercept) x1 ## sub_308 384.0955 -0.9135829 ## sub_309 406.5426 -0.9135829 ## sub_310 421.8658 -0.9135829 ## sub_330 492.0476 -0.9135829 ## sub_331 498.0868 -0.9135829 ## sub_332 496.0130 -0.9135829 ## sub_333 504.6193 -0.9135829 ## sub_334 338.5855 -0.9135829 ## sub_335 440.3964 -0.9135829 ## sub_337 416.7346 -0.9135829 ## sub_349 319.6674 -0.9135829 ## sub_350 356.3696 -0.9135829 ## sub_351 479.2943 -0.9135829 ## sub_352 379.5162 -0.9135829 ## sub_369 349.0152 -0.9135829 ## sub_370 335.0869 -0.9135829 ## sub_371 484.0427 -0.9135829 ## sub_372 360.5321 -0.9135829 ## sub_373 293.6168 -0.9135829 ## sub_374 511.3440 -0.9135829 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; ► Plotting random effects The quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4. randoms &lt;- ranef(random_intercept_model, condVar=TRUE) dotplot.ranef.mer(randoms) ## $subject 5.6 More levels: Nested and Crossed random-effects The same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups). ► Question Consider the example where we have observations for each student in every class within a number of schools: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”? ► Solution No. The classes in one school are distinct from the classes in another even though they are named the same. The classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable. In R, we can specify this using: (1 | school) + (1 | class:school) or, more succinctly: (1 | school/class) ► Question Consider another example, where we administer the same set of tasks at multiple time-points for every participant. Are tasks nested within participants? ► Solution No - tasks are seen by multiple participants (and participants see multiple tasks). We could visualise this as the below: In the sense that these are not nested, they are crossed random effects. In R, we can specify this using: (1 | subject) + (1 | task) Definitions Nested: Each group belongs uniquely to a higher-level group. Crossed: Not-nested. Note that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | scool) + (1 | class) and (1 | school/class) would give the same results. 5.7 THE LAB (finally!) The data 44 participants across 4 groups (between-subjects) were tested 5 times (waves) in 11 domains. In each wave of testing, each domain received a score on a 20-point scale and a set of several questions, which could be answered correctly or incorrectly. load(url(&quot;https://edin.ac/2Hd6V4Q&quot;)) summary(dat5) ## Anonymous_Subject_ID IndivDiff Wave Domain Correct Error Group ## Length:2011 Min. :39.30 Min. :1.000 Length:2011 Min. : 0.000 Min. :0.00000 Length:2011 ## Class :character 1st Qu.:69.20 1st Qu.:2.000 Class :character 1st Qu.: 4.000 1st Qu.:0.00000 Class :character ## Mode :character Median :79.70 Median :3.000 Mode :character Median : 8.000 Median :0.00000 Mode :character ## Mean :77.73 Mean :2.712 Mean : 9.904 Mean :0.06216 ## 3rd Qu.:88.10 3rd Qu.:4.000 3rd Qu.:12.000 3rd Qu.:0.00000 ## Max. :95.20 Max. :5.000 Max. :45.000 Max. :1.00000 ## NA&#39;s :1474 ## Score ## Min. : 0.0 ## 1st Qu.: 8.0 ## Median :14.0 ## Mean :12.2 ## 3rd Qu.:17.0 ## Max. :20.0 ## 5.7.1 Exercise 1 Research question Did the groups differ in overall performance? There are different ways to test this: use the 20-point score or the accuracy? Keep the domains separate or calculate an aggregate across all domains? Which way makes the most sense to you? ► Question 1 Make a plot that corresponds to the reseach question. Does it look like there’s a difference? ► Solution Lots of options for this one, here is one that shows Group and Domain differences: ggplot(dat5, aes(Domain, Score, color=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + coord_flip() Looks like there are group differences and domain differences, but not much in the way of group-by-domain differences. ► Question 2 Use a mixed-effects model to test the difference. Will you use a linear or logistic model? What should the fixed(s) effect be? What should the random effect(s) be? Tip: For now, forget about the longitudinal aspect to the data. ► Solution We’re interested in the amount to which Groups vary in their overall performance, so we want a fixed effect of Group. We also have observations clustered by subject and domain. # maximal model doesn&#39;t converge, removed random Group slopes for Domain mod_grp &lt;- lmer(Score ~ Group + (1 | Anonymous_Subject_ID) + (1 | Domain), data=dat5, REML=FALSE) summary(mod_grp) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ Group + (1 | Anonymous_Subject_ID) + (1 | Domain) ## Data: dat5 ## ## AIC BIC logLik deviance df.resid ## 10398.2 10437.5 -5192.1 10384.2 2004 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.0251 -0.4981 0.0639 0.6338 3.2779 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Anonymous_Subject_ID (Intercept) 19.486 4.414 ## Domain (Intercept) 1.064 1.031 ## Residual 9.122 3.020 ## Number of obs: 2011, groups: Anonymous_Subject_ID, 44; Domain, 11 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 15.832 1.121 50.067 14.122 &lt; 2e-16 *** ## GroupB -4.159 2.262 44.369 -1.839 0.0727 . ## GroupC -3.621 1.768 43.968 -2.048 0.0465 * ## GroupW -7.270 1.673 44.042 -4.345 8.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) GroupB GroupC ## GroupB -0.457 ## GroupC -0.585 0.290 ## GroupW -0.618 0.306 0.392 Yes, substantial Group differences: overall, group A does the best, group B is slightly behind, group C next, and group W does the worst. 5.8 Exercise 2 Research question Did performance change over time (across waves)? Did the groups differ in pattern of change? ► Question 3 Make a plot that corresponds to the reseach question. Does it look like there was a change? A group difference? ► Solution ggplot(dat5, aes(Wave, Score, color=Group, fill=Group)) + stat_summary(fun.data=mean_se, geom=&quot;ribbon&quot;, alpha=0.3, color=NA) + stat_summary(fun.y=mean, geom=&quot;line&quot;) Yes, looks like groups A, C, and W are improving, but group B is getting worse. ► Question 4 Use mixed-effects model(s) to test this. Hint: Fit a baseline model in which scores change over time (wave), then assess improvement in model fit due to inclusion of overall group effect and finally the interaction of group with time. Note that you may want to remove correlations in random terms which are close to -1 or 1 ► Solution # maximal model doesn&#39;t converge, removed random intercept-slope corrections for Domain mod_wv &lt;- lmer(Score ~ Wave + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave || Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) mod_wv_grp &lt;- lmer(Score ~ Wave+Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave || Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) mod_wv_x_grp &lt;- lmer(Score ~ Wave*Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave || Domain), data=dat5, REML=FALSE, lmerControl(optimizer = &quot;bobyqa&quot;)) anova(mod_wv, mod_wv_grp, mod_wv_x_grp) ## Data: dat5 ## Models: ## mod_wv: Score ~ Wave + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave || ## mod_wv: Domain) ## mod_wv_grp: Score ~ Wave + Group + (1 + Wave | Anonymous_Subject_ID) + (1 + ## mod_wv_grp: Wave || Domain) ## mod_wv_x_grp: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 + ## mod_wv_x_grp: Wave || Domain) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## mod_wv 8 9728.5 9773.3 -4856.2 9712.5 ## mod_wv_grp 11 9719.1 9780.8 -4848.6 9697.1 15.3184 3 0.001564 ** ## mod_wv_x_grp 14 9719.3 9797.8 -4845.6 9691.3 5.8366 3 0.119837 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(mod_wv_x_grp) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: Score ~ Wave * Group + (1 + Wave | Anonymous_Subject_ID) + (1 + Wave || Domain) ## Data: dat5 ## Control: lmerControl(optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 9719.3 9797.8 -4845.6 9691.3 1997 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.1272 -0.5626 0.0087 0.6212 3.6875 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Anonymous_Subject_ID (Intercept) 22.32429 4.72486 ## Wave 0.74623 0.86384 -0.33 ## Domain (Intercept) 1.34097 1.15800 ## Domain.1 Wave 0.00907 0.09524 ## Residual 6.08446 2.46667 ## Number of obs: 2011, groups: Anonymous_Subject_ID, 44; Domain, 11 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.77242 1.22214 50.62646 10.451 3.07e-14 *** ## Wave 1.25493 0.23625 43.33324 5.312 3.56e-06 *** ## GroupB -1.35725 2.47242 45.88114 -0.549 0.58570 ## GroupC -4.14635 1.91496 43.90062 -2.165 0.03585 * ## GroupW -6.31834 1.81509 44.22255 -3.481 0.00114 ** ## Wave:GroupB -1.14516 0.52894 50.14483 -2.165 0.03517 * ## Wave:GroupC -0.03706 0.36833 36.71923 -0.101 0.92041 ## Wave:GroupW -0.50898 0.35441 38.77308 -1.436 0.15898 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) Wave GroupB GroupC GroupW Wv:GrB Wv:GrC ## Wave -0.356 ## GroupB -0.454 0.176 ## GroupC -0.586 0.227 0.290 ## GroupW -0.618 0.240 0.306 0.395 ## Wave:GroupB 0.159 -0.440 -0.387 -0.102 -0.107 ## Wave:GroupC 0.229 -0.632 -0.113 -0.363 -0.154 0.282 ## Wave:GroupW 0.238 -0.657 -0.117 -0.152 -0.368 0.293 0.421 ► Question 5 Plot the group-level data (see Question 3) and model fitted values from your final model from Question 4. Hint: using fortify(model) or broom::augment(model) as your starting point will help. ► Solution ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=.fitted), fun.y=mean, geom=&quot;line&quot;) We fit a linear model, but the model fit lines are not straight lines. Why is that? ► Question 6 Create individual subject plots for the data and the model’s fitted values. Will these show straight lines? Hint: make use of facet_wrap() to create a different panel for each level of a grouping variable. ► Solution ggplot(fortify(mod_wv_x_grp), aes(Wave, Score, color=Group)) + facet_wrap(~ Anonymous_Subject_ID) + stat_summary(fun.data=mean_se, geom=&quot;pointrange&quot;) + stat_summary(aes(y=.fitted), fun.y=mean, geom=&quot;line&quot;) The individual subject plots show linear fits, which is a better match to the model. But now we see the missing data – some participants only completed the first few waves. ► Question 7 Make a plot of the actual (linear) model prediction. Hint: Use the effect() function from the effects package. ► Solution ef &lt;- as.data.frame(effect(&quot;Wave:Group&quot;, mod_wv_x_grp)) ggplot(ef, aes(Wave, fit, color=Group, fill=Group)) + geom_ribbon(aes(ymax=fit+se, ymin=fit-se), color=NA, alpha=0.1) + geom_line() ► Question 8 What important things are different between the plot from question 7 and that from question 5? ► Solution Group B was not actually getting worse. The appearance that it was getting worse is an artifact of selective drop-out: there’s only a few people in this group and the better-performing ones only did the first few waves so they are not represented in the later waves, but the worse-performing ones are contributing to the later waves. The model estimates how the better-performing ones would have done in later waves based on their early-wave performance and the pattern of performance of other participants in the study. summary(mod_wv_x_grp)$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.7724233 1.2221370 50.62646 10.4508929 3.068589e-14 ## Wave 1.2549312 0.2362546 43.33324 5.3117751 3.555503e-06 ## GroupB -1.3572482 2.4724182 45.88114 -0.5489558 5.856976e-01 ## GroupC -4.1463512 1.9149594 43.90062 -2.1652424 3.584565e-02 ## GroupW -6.3183356 1.8150933 44.22255 -3.4809977 1.136187e-03 ## Wave:GroupB -1.1451562 0.5289407 50.14483 -2.1649991 3.517224e-02 ## Wave:GroupC -0.0370582 0.3683291 36.71923 -0.1006117 9.204061e-01 ## Wave:GroupW -0.5089799 0.3544077 38.77308 -1.4361424 1.589816e-01 Note that the Group A slope (coefficient for Wave) is 1.255 and, relative to that slope, the Group B slope is -1.142 (coefficient for Wave:GroupB). This means that the model-estimated slope for Group B is 0.112, which is very slightly positive, not strongly negative as appeared in the initial plots. One of the valuable things about mixed-effects (aka multilevel) modeling is that individual-level and group-level trajectories are estimated. This helps the model overcome missing data in a sensible way. In fact, MLM/MLR models are sometimes used for imputing missing data. However, one has to think carefully about why data are missing. Group B is small and it might just be a coincidence that the better-performing participants dropped out after the first few waves, which would make it easier to generalize the patterns to them. On the other hand, it might be the case that there is something about the study that makes better-performing members of Group B drop out, which should make us suspicious of generalizing to them. ► Question 9 Create a plot of the subject and domain random effects. ► Solution randoms &lt;- ranef(mod_wv_x_grp, condVar=TRUE) dotplot.ranef.mer(randoms) ## $Anonymous_Subject_ID ## ## $Domain "],
["break-week.html", "Chapter 6 Break Week", " Chapter 6 Break Week "],
["efa-and-pca.html", "Chapter 7 EFA and PCA 7.1 Today’s Exercises", " Chapter 7 EFA and PCA Packages psych GPArotation car GGally (optional) Lecture Slides Coming soon Background &amp; Reading Coming soon 7.1 Today’s Exercises Background A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours: Stealing Lying Skipping school Vandalism Breaking curfew Threatening others Bullying Spreading malicious rumours Using a weapon Fighting Your task is to use the dimension reduction techniques you learned about in the lecture to help inform how to organise the items she has developed into subscales ► Question 1 Load the psych package and read in the dataset ‘Conduct_problems.csv’. The first column is clearly an ID column, and it is easiest just to discard this for when we are doing factor analysis. Create a correlation matrix for the items. Inspect the items to check their suitability for exploratory factor analysis. You can use a function such as corr.test(df) from the psych package to create the correlation matrix. You can check the factorability of the correlation matrix using KMO(df). You can check linearity of relations using scatterplotMatrix(df) (from the car package). If you add the argument diagonal=histogram You can view the histograms on the diagonals, allowing you to check univariate normality (which is usually a good enough proxy for multivariate normality). You can do the same using the ggpairs function from the GGally package. NOTE. df=dataframe ► Solution library(psych) df &lt;- read.csv(&quot;../labs/Conduct_problems.csv&quot;) # discard the first column df &lt;- df[,-1] corr.test(df) ## Call:corr.test(x = df) ## Correlation matrix ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 1.00 0.59 0.49 0.48 0.60 0.17 0.30 0.32 0.26 0.20 ## item2 0.59 1.00 0.53 0.51 0.66 0.20 0.33 0.30 0.29 0.19 ## item3 0.49 0.53 1.00 0.49 0.55 0.15 0.25 0.24 0.25 0.15 ## item4 0.48 0.51 0.49 1.00 0.65 0.23 0.29 0.32 0.28 0.25 ## item5 0.60 0.66 0.55 0.65 1.00 0.21 0.30 0.29 0.27 0.21 ## item6 0.17 0.20 0.15 0.23 0.21 1.00 0.54 0.57 0.41 0.44 ## item7 0.30 0.33 0.25 0.29 0.30 0.54 1.00 0.83 0.61 0.58 ## item8 0.32 0.30 0.24 0.32 0.29 0.57 0.83 1.00 0.61 0.59 ## item9 0.26 0.29 0.25 0.28 0.27 0.41 0.61 0.61 1.00 0.44 ## item10 0.20 0.19 0.15 0.25 0.21 0.44 0.58 0.59 0.44 1.00 ## Sample Size ## [1] 450 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## item1 0 0 0 0 0 0 0 0 0 0 ## item2 0 0 0 0 0 0 0 0 0 0 ## item3 0 0 0 0 0 0 0 0 0 0 ## item4 0 0 0 0 0 0 0 0 0 0 ## item5 0 0 0 0 0 0 0 0 0 0 ## item6 0 0 0 0 0 0 0 0 0 0 ## item7 0 0 0 0 0 0 0 0 0 0 ## item8 0 0 0 0 0 0 0 0 0 0 ## item9 0 0 0 0 0 0 0 0 0 0 ## item10 0 0 0 0 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option KMO(df) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = df) ## Overall MSA = 0.87 ## MSA for each item = ## item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 ## 0.90 0.88 0.92 0.88 0.84 0.94 0.82 0.81 0.95 0.94 car::scatterplotMatrix(df) or alternatively. library(GGally) ggpairs(data=df, diag=list(continuous=&quot;density&quot;), axisLabels=&quot;show&quot;) ► Question 2 How many dimensions should be retained? Use a scree plot, parallel analysis, and MAP test to guide you. ► Solution You can use fa.parallel(df) to conduct both parallel analysis and view a scree plot. fa.parallel(df) ## Parallel analysis suggests that the number of factors = 2 and the number of components = 2 In this case the scree plot has a kink at the third factor, so we probably want to retain 2 factors. We can conduct the MAP test using vss(df). vss(df) ## ## Very Simple Structure ## Call: vss(x = df) ## Although the VSS complexity 1 shows 7 factors, it is probably more reasonable to think about 2 factors ## VSS complexity 2 achieves a maximimum of 0.92 with 2 factors ## ## The Velicer MAP achieves a minimum of 0.03 with 2 factors ## BIC achieves a minimum of NA with 2 factors ## Sample Size adjusted BIC achieves a minimum of NA with 3 factors ## ## Statistics by number of factors ## vss1 vss2 map dof chisq prob sqresid fit RMSEA BIC SABIC complex eChisq SRMR eCRMS eBIC ## 1 0.77 0.00 0.106 35 8.8e+02 3.6e-161 6.0 0.77 0.231 662 773 1.0 1.0e+03 1.6e-01 0.179 797 ## 2 0.78 0.92 0.034 26 4.0e+01 3.9e-02 2.1 0.92 0.035 -119 -36 1.1 1.4e+01 1.8e-02 0.024 -145 ## 3 0.78 0.91 0.058 18 1.2e+01 8.6e-01 1.9 0.93 0.000 -98 -41 1.2 3.4e+00 9.1e-03 0.014 -107 ## 4 0.63 0.90 0.104 11 6.1e+00 8.7e-01 1.4 0.95 0.000 -61 -26 1.3 1.6e+00 6.3e-03 0.013 -66 ## 5 0.69 0.87 0.149 5 2.4e+00 7.9e-01 1.6 0.94 0.000 -28 -12 1.3 7.5e-01 4.3e-03 0.013 -30 ## 6 0.71 0.89 0.252 0 1.2e-01 NA 1.4 0.95 NA NA NA 1.4 2.9e-02 8.4e-04 NA NA ## 7 0.79 0.89 0.397 -4 1.1e-07 NA 1.5 0.94 NA NA NA 1.3 2.2e-08 7.4e-07 NA NA ## 8 0.79 0.89 0.455 -7 1.6e-08 NA 1.5 0.94 NA NA NA 1.4 2.8e-09 2.6e-07 NA NA The MAP test suggests retaining 2 factors. ► Question 3 Having decided how many dimensions to retain in the previous question, conduct an EFA to extract this many factors, using a suitable rotation and extraction method. ► Solution You can use the fa() function from the psych package, for example, you could choose an oblimin rotation to allow factors to correlate and use minres as the extraction method. conduct_efa &lt;- fa(df, nfactors=2, rotate=&#39;oblimin&#39;, fm=&#39;minres&#39;) ► Question 4 Inspect the loadings and give the factors you extracted labels based on the patterns of loadings. Look back to the description of the items, and suggest a name for you factors ► Solution You can inspect the loadings using: conduct_efa$loadings ## ## Loadings: ## MR1 MR2 ## item1 0.613 0.375 ## item2 0.654 0.420 ## item3 0.554 0.384 ## item4 0.619 0.339 ## item5 0.693 0.502 ## item6 0.527 -0.337 ## item7 0.766 -0.458 ## item8 0.782 -0.484 ## item9 0.613 -0.276 ## item10 0.552 -0.360 ## ## MR1 MR2 ## SS loadings 4.128 1.593 ## Proportion Var 0.413 0.159 ## Cumulative Var 0.413 0.572 We can see that the first five items have high loadings for one factor and the second five items have high loadings for the other. The first five items all have in common that they are non-aggressive forms of conduct problems, while the last five items are all aggressive behaviours. We could, therefore, label our factors: ‘non-aggressive’ and ‘aggressive’ conduct problems. ► Question 5 How correlated are your factors? ► Solution We can inspect the factor correlations (if we used an oblique rotation) using: conduct_efa$Phi ## NULL We can see here that there is a moderate correlation between the two factors. An oblique rotation would be appropriate here. ► Question 6 Using the same data, conduct a PCA using the principal() function. What differences do you notice compared to your EFA? Do you think a PCA or an EFA is more appropriate in this particular case? ► Solution We can use: principal(df, nfactors=2) ## Principal Components Analysis ## Call: principal(r = df, nfactors = 2) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC1 RC2 h2 u2 com ## item1 0.17 0.77 0.62 0.38 1.1 ## item2 0.17 0.81 0.68 0.32 1.1 ## item3 0.11 0.75 0.58 0.42 1.0 ## item4 0.21 0.74 0.60 0.40 1.2 ## item5 0.16 0.85 0.75 0.25 1.1 ## item6 0.73 0.08 0.53 0.47 1.0 ## item7 0.87 0.20 0.80 0.20 1.1 ## item8 0.88 0.19 0.82 0.18 1.1 ## item9 0.72 0.21 0.56 0.44 1.2 ## item10 0.75 0.09 0.57 0.43 1.0 ## ## RC1 RC2 ## SS loadings 3.29 3.22 ## Proportion Var 0.33 0.32 ## Cumulative Var 0.33 0.65 ## Proportion Explained 0.51 0.49 ## Cumulative Proportion 0.51 1.00 ## ## Mean item complexity = 1.1 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 166.43 with prob &lt; 1.9e-22 ## ## Fit based upon off diagonal values = 0.98 We can see that while the loadings differ somewhat between the EFA and the PCA, the overall pattern is quite similar. This is not always the case, especially when the item communalities are low. In terms of which method is more appropriate, arguably EFA would be more appropriate in this case because our researcher wishes to measure a theoretical construct (conduct problems), rather than simply reduce the dimensions of her data. "]
]
