<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Simple Linear Model and Mixed Methods | Data Analysis for Psychology Using R</title>
  <meta name="description" content="This is the main page of the course and contains a course overview, schedule and learning outcomes." />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Simple Linear Model and Mixed Methods | Data Analysis for Psychology Using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the main page of the course and contains a course overview, schedule and learning outcomes." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Simple Linear Model and Mixed Methods | Data Analysis for Psychology Using R" />
  
  <meta name="twitter:description" content="This is the main page of the course and contains a course overview, schedule and learning outcomes." />
  

<meta name="author" content="Department of Psychology, University of Edinburgh" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tests-and-modelling-in-r.html">
<link rel="next" href="testing-the-assumptions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PPLS Summer Training (R and Stats)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview of the Course</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programme"><i class="fa fa-check"></i>Programme</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-outcomes"><i class="fa fa-check"></i>Learning Outcomes</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preparation"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#expectations"><i class="fa fa-check"></i>Expectations</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-team"><i class="fa fa-check"></i>The team</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#list-of-extra-resources"><i class="fa fa-check"></i>List of extra resources</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authorship"><i class="fa fa-check"></i>Authorship</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#last-but-not-least"><i class="fa fa-check"></i>Last, but not least</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="extra-resources.html"><a href="extra-resources.html"><i class="fa fa-check"></i><b>1</b> Extra Resources</a><ul>
<li class="chapter" data-level="1.1" data-path="extra-resources.html"><a href="extra-resources.html#more-r-practice"><i class="fa fa-check"></i><b>1.1</b> More R practice</a></li>
<li class="chapter" data-level="1.2" data-path="extra-resources.html"><a href="extra-resources.html#data-cleaning"><i class="fa fa-check"></i><b>1.2</b> Data Cleaning</a></li>
<li class="chapter" data-level="1.3" data-path="extra-resources.html"><a href="extra-resources.html#visualisations"><i class="fa fa-check"></i><b>1.3</b> Visualisations</a></li>
<li class="chapter" data-level="1.4" data-path="extra-resources.html"><a href="extra-resources.html#other-common-methods-in-r"><i class="fa fa-check"></i><b>1.4</b> Other Common Methods in R</a></li>
<li class="chapter" data-level="1.5" data-path="extra-resources.html"><a href="extra-resources.html#big-data"><i class="fa fa-check"></i><b>1.5</b> Big Data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html"><i class="fa fa-check"></i><b>2</b> Introduction to R and RStudio</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#first-step"><i class="fa fa-check"></i><b>2.1</b> First Step</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#second-step"><i class="fa fa-check"></i><b>2.2</b> Second Step</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#r-as-an-interactive-envrionment"><i class="fa fa-check"></i><b>2.3</b> R as an interactive envrionment</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#setting-up-your-working-directory"><i class="fa fa-check"></i><b>2.4</b> Setting Up Your Working Directory</a></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#spacing"><i class="fa fa-check"></i><b>2.5</b> Spacing</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#typos"><i class="fa fa-check"></i><b>2.6</b> Typos</a></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#unfinishe.-d"><i class="fa fa-check"></i><b>2.7</b> Unfinishe…. d</a></li>
<li class="chapter" data-level="2.8" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#basic-arithmetic"><i class="fa fa-check"></i><b>2.8</b> Basic Arithmetic</a></li>
<li class="chapter" data-level="2.9" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#using-functions-for-calculations"><i class="fa fa-check"></i><b>2.9</b> Using Functions for Calculations</a></li>
<li class="chapter" data-level="2.10" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#short-example"><i class="fa fa-check"></i><b>2.10</b> Short Example</a></li>
<li class="chapter" data-level="2.11" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#exercise-1"><i class="fa fa-check"></i><b>2.11</b> Exercise 1</a></li>
<li class="chapter" data-level="2.12" data-path="introduction-to-r-and-rstudio.html"><a href="introduction-to-r-and-rstudio.html#exercise-2"><i class="fa fa-check"></i><b>2.12</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html"><i class="fa fa-check"></i><b>3</b> Tests and modelling in R</a><ul>
<li class="chapter" data-level="3.1" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.2" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#t-test"><i class="fa fa-check"></i><b>3.2</b> T test</a></li>
<li class="chapter" data-level="3.3" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#chi-squared-distribution-and-test"><i class="fa fa-check"></i><b>3.3</b> Chi squared distribution and test</a><ul>
<li class="chapter" data-level="3.3.1" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#contingency-tables"><i class="fa fa-check"></i><b>3.3.1</b> Contingency tables</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#chi-squared-distribution"><i class="fa fa-check"></i><b>3.4</b> Chi squared distribution</a></li>
<li class="chapter" data-level="3.5" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#one-way-anova"><i class="fa fa-check"></i><b>3.5</b> One way Anova</a></li>
<li class="chapter" data-level="3.6" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#sharks-and-ice-cream-example"><i class="fa fa-check"></i><b>3.6</b> Sharks and ice cream example</a></li>
<li class="chapter" data-level="3.7" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#simple-linear-regression-in-r"><i class="fa fa-check"></i><b>3.7</b> Simple Linear Regression in R</a></li>
<li class="chapter" data-level="3.8" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#regression-diagnostics---assess-the-validity-of-a-model"><i class="fa fa-check"></i><b>3.8</b> Regression Diagnostics - assess the validity of a model</a><ul>
<li class="chapter" data-level="3.8.1" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#violations-of-the-assumptions-available-treatments"><i class="fa fa-check"></i><b>3.8.1</b> Violations of the assumptions: available treatments</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#standardisation"><i class="fa fa-check"></i><b>3.9</b> Standardisation</a></li>
<li class="chapter" data-level="3.10" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#interaction-simple-slope-and-multiple-explanatory-factors"><i class="fa fa-check"></i><b>3.10</b> Interaction (simple slope) and multiple explanatory factors</a></li>
<li class="chapter" data-level="3.11" data-path="tests-and-modelling-in-r.html"><a href="tests-and-modelling-in-r.html#aic-bic"><i class="fa fa-check"></i><b>3.11</b> AIC &amp; BIC</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Model and Mixed Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#data-sets"><i class="fa fa-check"></i><b>4.1</b> Data sets</a></li>
<li class="chapter" data-level="4.2" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#longitudinal-data"><i class="fa fa-check"></i><b>4.2</b> Longitudinal Data</a></li>
<li class="chapter" data-level="4.3" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#why-a-new-model"><i class="fa fa-check"></i><b>4.3</b> Why a new model?</a></li>
<li class="chapter" data-level="4.4" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#ecological-fallacy-quick-illustration---no-need-to-run"><i class="fa fa-check"></i><b>4.4</b> Ecological Fallacy (quick illustration) - no need to run</a></li>
<li class="chapter" data-level="4.5" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#simple-example"><i class="fa fa-check"></i><b>4.5</b> Simple Example</a></li>
<li class="chapter" data-level="4.6" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#now-for-advanced-model-set-up"><i class="fa fa-check"></i><b>4.6</b> Now for Advanced: Model set up</a><ul>
<li class="chapter" data-level="4.6.1" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#pooling"><i class="fa fa-check"></i><b>4.6.1</b> Pooling</a></li>
<li class="chapter" data-level="4.6.2" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#no-pooling"><i class="fa fa-check"></i><b>4.6.2</b> No pooling</a></li>
<li class="chapter" data-level="4.6.3" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#partial-pooling-varying-intercepts"><i class="fa fa-check"></i><b>4.6.3</b> Partial Pooling (varying intercepts)</a></li>
<li class="chapter" data-level="4.6.4" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#partial-pooling-extended---varying-intercepts-andor-slopes"><i class="fa fa-check"></i><b>4.6.4</b> Partial Pooling Extended - (varying intercepts and/or slopes)</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#multilevel-modelling-with-random-intercepts-and-slopes"><i class="fa fa-check"></i><b>4.7</b> Multilevel modelling with random intercepts and slopes</a><ul>
<li class="chapter" data-level="4.7.1" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#overview-of-the-data-set"><i class="fa fa-check"></i><b>4.7.1</b> Overview of the data set</a></li>
<li class="chapter" data-level="4.7.2" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#prepare"><i class="fa fa-check"></i><b>4.7.2</b> Prepare</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="simple-linear-model-and-mixed-methods.html"><a href="simple-linear-model-and-mixed-methods.html#random-slopes-intercepts-and-cross-level-interactions-optional"><i class="fa fa-check"></i><b>4.8</b> Random slopes, intercepts and cross level interactions (optional)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="testing-the-assumptions.html"><a href="testing-the-assumptions.html"><i class="fa fa-check"></i><b>5</b> Testing the assumptions</a></li>
<li class="chapter" data-level="6" data-path="coffee-break.html"><a href="coffee-break.html"><i class="fa fa-check"></i><b>6</b> Coffee break</a></li>
<li class="chapter" data-level="7" data-path="logistic-setting.html"><a href="logistic-setting.html"><i class="fa fa-check"></i><b>7</b> Logistic setting</a><ul>
<li class="chapter" data-level="7.1" data-path="logistic-setting.html"><a href="logistic-setting.html#simple-example-1"><i class="fa fa-check"></i><b>7.1</b> Simple Example</a><ul>
<li class="chapter" data-level="7.1.1" data-path="logistic-setting.html"><a href="logistic-setting.html#optional-odds-refresher"><i class="fa fa-check"></i><b>7.1.1</b> Optional (Odds Refresher)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="now-for-advanced-logistic-mixed-effects.html"><a href="now-for-advanced-logistic-mixed-effects.html"><i class="fa fa-check"></i><b>8</b> Now for Advanced: logistic mixed effects</a></li>
<li class="chapter" data-level="9" data-path="now-over-to-you.html"><a href="now-over-to-you.html"><i class="fa fa-check"></i><b>9</b> Now, over to you!</a><ul>
<li class="chapter" data-level="9.1" data-path="now-over-to-you.html"><a href="now-over-to-you.html#data-description"><i class="fa fa-check"></i><b>9.1</b> Data Description</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="week-4.html"><a href="week-4.html"><i class="fa fa-check"></i><b>10</b> week 4</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis for Psychology Using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-model-and-mixed-methods" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Simple Linear Model and Mixed Methods</h1>
<p>Note: this section is partially adapted from <a href="http://www.stat.rutgers.edu/home/yhung/Stat586/Mixed%20model/appendix-mixed-models.pdf">Fox’s Linear Mixed Models</a> and <a href="https://www.jstatsoft.org/article/view/v067i01">Bates et al (2015)</a> . We will first focus on simple linear model, we extend it to fixed effect model, finally we discuss random effects modelling. We will also use the data from study of <a href="https://onlinelibrary.wiley.com/doi/full/10.1046/j.1365-2869.2003.00337.x">Belenky et al (2003)</a>. This is pretty famous dataset that is easy to find online and you may find some help on it easily as many people are using it either to teach or to learn about mixed effects.</p>
<div id="data-sets" class="section level2">
<h2><span class="header-section-number">4.1</span> Data sets</h2>
<p>Some of the data that we will use today is available via respective packages. All the additional data sets can be found <a href="https://anaushakova.files.wordpress.com/2019/06/data_part3-2.zip">here</a>.</p>
</div>
<div id="longitudinal-data" class="section level2">
<h2><span class="header-section-number">4.2</span> Longitudinal Data</h2>
<p>We have seen quite a few examples of linear models last week, let us now finally get to the ones you probably came here to learn more about. The world can be slightly more complex of course than traditional set up of linear models. Things change in time and they change differently for different units of analysis (i.e. participants). We want to control for the variation in this change when making claims about casual relationship we find using data. In psychology and linguistics research, especially, you often find that you will have some repeated measures of your variable of interest for participants (i.e. memory after performing certain tests, reaction scores, attitudes, etc). These are related either to multiple treatments given to participants or/and arise from measuring the same relationships at different time points.</p>
<p>Remember old model set up:
<span class="math display">\[\begin{equation}
Y_[i] = \beta_0 + \beta_1 * X_i + \epsilon_i

\end{equation}\]</span></p>
<p>And remember the assumptions that are required for the linear model. Can you recall those for the residual term?
Once in repeated measures setting you will see that most of these will remain. As in the simple linear model, we do care most about our residual term. We want it to be random and normally distributed with zero mean. The main reason for that of course is the fact that we want our model to explain all the meaningful variations without leaving anything in the residual term.</p>
<p>The only difference we have now is the we want to control for each individual unobserved characteristics so we introduce extra parameter which will account for those.</p>
<p><span class="math display">\[\begin{equation}
Y_[i] = \beta_0 + \beta_1 * X_i + \alpha + \epsilon_i

\end{equation}\]</span></p>
<p>This is the fixed effect model.</p>
<p>We can also introduce further parameters that will allow us to include effects at individual levels (i.e. pupils) and at the entity levels (i.e.schools)</p>
<p><span class="math display">\[\begin{equation}
Y_[i] = \beta_0 + \beta_1 * X_i + \theta_i + \gamma_ij + \epsilon_i

\end{equation}\]</span></p>
</div>
<div id="why-a-new-model" class="section level2">
<h2><span class="header-section-number">4.3</span> Why a new model?</h2>
<p>The reason why we are introducing a new model for repeated measures data or data that has few levels is the consideration of Ecological fallacy. What do we mean by that is that there may be differences across the effects on the levels of units compared to that on individual levels. Lets look at the example based on some data. Below, I have a generic plot of data points that illustrates the corresponding values of Y for each unit of some variable X.</p>
<p>Do you think we have positive relationship or negative? Is it easy to tell what is happening?</p>
<p><img src="eco_fal.jpeg" width="60%" style="display: block; margin: auto;" /></p>
<p>I think its hard to tell! Let us look at something a bit more specific.</p>
</div>
<div id="ecological-fallacy-quick-illustration---no-need-to-run" class="section level2">
<h2><span class="header-section-number">4.4</span> Ecological Fallacy (quick illustration) - no need to run</h2>
<p>Lets get some data in and load <code>lmer</code> package:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lme4)
disease&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;disease.csv&#39;</span>)</code></pre>
</div>
<div id="simple-example" class="section level2">
<h2><span class="header-section-number">4.5</span> Simple Example</h2>
<p>We would like to study the relationship between prevalence of deadly diseases and average income per cities. We will run three models but first let’s provide a simple visualisation of the data:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Make sure you have ggplot2</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">qplot</span>(disease<span class="op">$</span>Income, disease<span class="op">$</span>HDisease, <span class="dt">xlab=</span><span class="st">&#39;Income&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Disease&#39;</span>)</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Looks like we are dealing with quite positive relationship. Lets get to the models:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Not controlling for city</span>
model_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(HDisease<span class="op">~</span>Income, <span class="dt">data =</span> disease)
<span class="co">#Controlling for city effects</span>
model_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(HDisease<span class="op">~</span>Income<span class="op">+</span><span class="kw">factor</span>(City), <span class="dt">data =</span> disease)
<span class="co">#Controlling for city and across cities effects</span>
model_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(HDisease<span class="op">~</span>Income <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>City), <span class="dt">data =</span> disease)</code></pre>
<p>We can compare the outputs:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Produce tidy outputs using texreg</span>
<span class="kw">library</span>(texreg)
<span class="kw">screenreg</span>(<span class="kw">list</span>(model_<span class="dv">0</span>, model_<span class="dv">1</span>, model_<span class="dv">2</span>))</code></pre>
<pre><code>## 
## =========================================================
##                        Model 1     Model 2     Model 3   
## ---------------------------------------------------------
## (Intercept)              1.65 ***    3.07 ***   16.41 ***
##                         (0.27)      (0.11)      (2.87)   
## Income                   1.95 ***   -0.99 ***   -0.98 ***
##                         (0.05)      (0.07)      (0.07)   
## factor(City)2                        2.88 ***            
##                                     (0.16)               
## factor(City)3                        5.96 ***            
##                                     (0.21)               
## factor(City)4                        8.91 ***            
##                                     (0.26)               
## factor(City)5                       12.06 ***            
##                                     (0.33)               
## factor(City)6                       14.89 ***            
##                                     (0.40)               
## factor(City)7                       17.78 ***            
##                                     (0.47)               
## factor(City)8                       20.90 ***            
##                                     (0.54)               
## factor(City)9                       23.86 ***            
##                                     (0.59)               
## factor(City)10                      26.93 ***            
##                                     (0.68)               
## ---------------------------------------------------------
## R^2                      0.95        1.00                
## Adj. R^2                 0.95        1.00                
## Num. obs.              100         100         100       
## RMSE                     1.34        0.32                
## AIC                                            148.61    
## BIC                                            159.03    
## Log Likelihood                                 -70.31    
## Num. groups: City                               10       
## Var: City (Intercept)                           81.14    
## Var: Residual                                    0.10    
## =========================================================
## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05</code></pre>
</div>
<div id="now-for-advanced-model-set-up" class="section level2">
<h2><span class="header-section-number">4.6</span> Now for Advanced: Model set up</h2>
<p>The previous example was something known as <code>toy</code> example. We can use some real data examples now.
Lets load all the packages we will need.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Load neccesary packages</span>
<span class="kw">library</span>(lattice)
<span class="kw">library</span>(arm)</code></pre>
<p>There will be two main illustration we want to focus on. First one will consider clustering of temporal variation by unit of analysis (i.e. participant) and the second one will consider clustering of units by group (i.e. school).</p>
<p>For the first example, we will use the data on sleep deprivation. The example is based on Belenky et al(2003) study that looks at patterns of sleeping across individuals that are going through the stages of sleep deprivation. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time on a series of tests given each day to each subject.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Read data in</span>
sleep&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;sleep.csv&#39;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Check whats in</span>
<span class="kw">str</span>(sleep)</code></pre>
<pre><code>## &#39;data.frame&#39;:    180 obs. of  4 variables:
##  $ X       : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Reaction: num  250 259 251 321 357 ...
##  $ Days    : int  0 1 2 3 4 5 6 7 8 9 ...
##  $ Subject : int  308 308 308 308 308 308 308 308 308 308 ...</code></pre>
<p>Let us just plot the data as it is. We will use Days as our X, and Reaction as our Y. You will recognize something we just have seen.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Plot reaction dynamics across days</span>
<span class="kw">qplot</span>(sleep<span class="op">$</span>Days, sleep<span class="op">$</span>Reaction, <span class="dt">xlab=</span><span class="st">&#39;Days&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Reaction&#39;</span>)</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>It is hard to tell whether there is a positive effect or whether there may be different levels of effect. It is clear that there is some tendency for increasing reaction as time passes but we have various individuals we may find that on individual levels we may observe a more detailed picture.</p>
<p>We can use <code>xy plot()</code> to plot the variation of reaction time for the test which were given to participants across ten days of the recovery.</p>
<p>To check arguments that can be modified have a look <a href="https://www.rdocumentation.org/packages/lattice/versions/0.10-10/topics/xyplot">here</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">xyplot</span>(Reaction <span class="op">~</span><span class="st"> </span>Days<span class="op">|</span>Subject, sleep, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;g&quot;</span>,<span class="st">&quot;p&quot;</span>,<span class="st">&quot;r&quot;</span>), <span class="co">#we are mixing the types of plots here, try removing the option</span>
       <span class="dt">index =</span> <span class="cf">function</span>(x,y) <span class="kw">coef</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x))[<span class="dv">1</span>], <span class="co">#we specify that we are plotting x and y using lm() fit </span>
       <span class="dt">xlab =</span> <span class="st">&quot;Days of sleep deprivation&quot;</span>,
       <span class="dt">ylab =</span> <span class="st">&quot;Average reaction time (ms)&quot;</span>, <span class="dt">aspect =</span> <span class="st">&quot;xy&quot;</span>)</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-10-1.png" width="95%" style="display: block; margin: auto;" /></p>
<p>What do you see? It looks like for some individuals there were pretty fast changes in the reactions times across periods with some having a steady growth in reaction level. Mostly we observe upwards movement but the speed seems to vary so as the starting points.</p>
<p>Note how in the plots I included the variables <strong>Days|Subject, sleep</strong>. This helps to identify that we are looking at the effect of time for each subject’s sleep variable.</p>
<p>Let us now see what we will observe numerically if we were to run a model to fit this data.</p>
<div id="pooling" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Pooling</h3>
<p>We can start with simple model, this is also know as <em>complete polling</em>, we assume that effects of variable <em>Day</em> were the same for everyone. Just like in simple linear model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Lets regress reactions on days</span>
model_simple&lt;-<span class="kw">lm</span>(Reaction<span class="op">~</span>Days, sleep)
<span class="kw">summary</span>(model_simple)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Reaction ~ Days, data = sleep)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -110.848  -27.483    1.546   26.142  139.953 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***
## Days          10.467      1.238   8.454 9.89e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 47.71 on 178 degrees of freedom
## Multiple R-squared:  0.2865, Adjusted R-squared:  0.2825 
## F-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15</code></pre>
</div>
<div id="no-pooling" class="section level3">
<h3><span class="header-section-number">4.6.2</span> No pooling</h3>
<p>Some may say that it will be quite naive to put all together as we have some variability within each individual records that we do not account. Alternatively, we can attempt to control for each subject, this is known as fixed effect and is equivalent to having a dummy variable for each of the individuals. That way we can ensure that we separate the variation that is due to individuals unique characteristics. Here is an example:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Lets regress reactions on days but now control for indviduals via including &#39;Subject&#39;. </span>
model_subject&lt;-<span class="kw">lm</span>(Reaction<span class="op">~</span>Days<span class="op">+</span><span class="kw">as.factor</span>(Subject), sleep)
<span class="kw">summary</span>(model_subject)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Reaction ~ Days + as.factor(Subject), data = sleep)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -100.540  -16.389   -0.341   15.215  131.159 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            295.0310    10.4471  28.240  &lt; 2e-16 ***
## Days                    10.4673     0.8042  13.015  &lt; 2e-16 ***
## as.factor(Subject)309 -126.9008    13.8597  -9.156 2.35e-16 ***
## as.factor(Subject)310 -111.1326    13.8597  -8.018 2.07e-13 ***
## as.factor(Subject)330  -38.9124    13.8597  -2.808 0.005609 ** 
## as.factor(Subject)331  -32.6978    13.8597  -2.359 0.019514 *  
## as.factor(Subject)332  -34.8318    13.8597  -2.513 0.012949 *  
## as.factor(Subject)333  -25.9755    13.8597  -1.874 0.062718 .  
## as.factor(Subject)334  -46.8318    13.8597  -3.379 0.000913 ***
## as.factor(Subject)335  -92.0638    13.8597  -6.643 4.51e-10 ***
## as.factor(Subject)337   33.5872    13.8597   2.423 0.016486 *  
## as.factor(Subject)349  -66.2994    13.8597  -4.784 3.87e-06 ***
## as.factor(Subject)350  -28.5311    13.8597  -2.059 0.041147 *  
## as.factor(Subject)351  -52.0361    13.8597  -3.754 0.000242 ***
## as.factor(Subject)352   -4.7123    13.8597  -0.340 0.734300    
## as.factor(Subject)369  -36.0992    13.8597  -2.605 0.010059 *  
## as.factor(Subject)370  -50.4321    13.8597  -3.639 0.000369 ***
## as.factor(Subject)371  -47.1498    13.8597  -3.402 0.000844 ***
## as.factor(Subject)372  -24.2477    13.8597  -1.750 0.082108 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 30.99 on 161 degrees of freedom
## Multiple R-squared:  0.7277, Adjusted R-squared:  0.6973 
## F-statistic: 23.91 on 18 and 161 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We have got some improvement in terms of explain variance (check Adjusted R-squared).</p>
<p>We can test whether controlling for each individual variation improves on a simple model as well. An F test is here to help since we have nested models:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#F test </span>
<span class="kw">anova</span>(model_simple, model_subject)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Reaction ~ Days
## Model 2: Reaction ~ Days + as.factor(Subject)
##   Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1    178 405252                                  
## 2    161 154634 17    250618 15.349 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Not bad! Guess we can stop here? We can think for a moment…</p>
<p><strong>Quick note: in the structure above we do not consider anything with regards to between units variation, we are looking at what happening within individuals reaction scores but do not account how they vary between each other</strong></p>
</div>
<div id="partial-pooling-varying-intercepts" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Partial Pooling (varying intercepts)</h3>
<p>An attempt do a bit of both (partial pooling) can be using random effect modelling structure. Such model will first look at the variation between individuals and then measure their distance from average individual effect, so we are partially pooling everyone together. Please note that we are assuming the slopes are the same, it is the intercepts that will vary for each person.</p>
<p>There are quite a few variations of mixed methods specifications. A useful table was found in <a href="https://www.jstatsoft.org/article/view/v067i01">Bates et al (2015)</a>:</p>
<p><img src="mixed.png" width="80%" style="display: block; margin: auto;" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Run baseline model (will use it later for comparison, we have  no controls)</span>
model_null &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st">  </span>(<span class="dv">1</span><span class="op">|</span>Subject), <span class="co"># note how we use 1 to suggest that that we keep the slope constant and vary intercept</span>
                  <span class="dt">data=</span>sleep)
<span class="co">#Run the model (note thay we also control for the effect of time by subject)</span>
model_mix &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>Subject) , sleep)
<span class="co">#Summarise</span>
<span class="kw">summary</span>(model_mix)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 | Subject)
##    Data: sleep
## 
## REML criterion at convergence: 1786.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.2257 -0.5529  0.0109  0.5188  4.2506 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  Subject  (Intercept) 1378.2   37.12   
##  Residual              960.5   30.99   
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 251.4051     9.7467   25.79
## Days         10.4673     0.8042   13.02
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.371</code></pre>
<p>Double check that the intercept for fixed effect is identical to the one we found earlier when we controlled for the subject. The coefficient for ‘Days’ reports the average effect of an extra day of sleep deprivation on reaction score.</p>
<p>Lets get to a bit more compressed output:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Use display()</span>
<span class="kw">display</span>(model_mix)</code></pre>
<pre><code>## lmer(formula = Reaction ~ Days + (1 | Subject), data = sleep)
##             coef.est coef.se
## (Intercept) 251.41     9.75 
## Days         10.47     0.80 
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  Subject  (Intercept) 37.12   
##  Residual             30.99   
## ---
## number of obs: 180, groups: Subject, 18
## AIC = 1794.5, DIC = 1801.7
## deviance = 1794.1</code></pre>
<p>We can extract fixed effect estimation:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#fixef () wil take fixed effect out for us</span>
<span class="kw">fixef</span>(model_mix) </code></pre>
<pre><code>## (Intercept)        Days 
##   251.40510    10.46729</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#se.fixef() wil take he standard error of the fixed effect </span>
<span class="kw">se.fixef</span>(model_mix)</code></pre>
<pre><code>## (Intercept)        Days 
##   9.7467163   0.8042214</code></pre>
<p>Or random effect:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ranef () wil take random effect out for us</span>
<span class="kw">ranef</span>(model_mix)</code></pre>
<pre><code>## $Subject
##     (Intercept)
## 308   40.783710
## 309  -77.849554
## 310  -63.108567
## 330    4.406442
## 331   10.216189
## 332    8.221238
## 333   16.500494
## 334   -2.996981
## 335  -45.282127
## 337   72.182686
## 349  -21.196249
## 350   14.111363
## 351   -7.862221
## 352   36.378425
## 369    7.036381
## 370   -6.362703
## 371   -3.294273
## 372   18.115747
## 
## with conditional variances for &quot;Subject&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#se.ranef() wil take he standard error of the fixed effect </span>
<span class="kw">se.ranef</span>(model_mix)</code></pre>
<pre><code>## $Subject
##     (Intercept)
## 308    9.475668
## 309    9.475668
## 310    9.475668
## 330    9.475668
## 331    9.475668
## 332    9.475668
## 333    9.475668
## 334    9.475668
## 335    9.475668
## 337    9.475668
## 349    9.475668
## 350    9.475668
## 351    9.475668
## 352    9.475668
## 369    9.475668
## 370    9.475668
## 371    9.475668
## 372    9.475668</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#We can also just get the predicted intercept + slope for each ( fixef() plus the value of ranef())</span>
<span class="kw">coef</span>(model_mix)</code></pre>
<pre><code>## $Subject
##     (Intercept)     Days
## 308    292.1888 10.46729
## 309    173.5556 10.46729
## 310    188.2965 10.46729
## 330    255.8115 10.46729
## 331    261.6213 10.46729
## 332    259.6263 10.46729
## 333    267.9056 10.46729
## 334    248.4081 10.46729
## 335    206.1230 10.46729
## 337    323.5878 10.46729
## 349    230.2089 10.46729
## 350    265.5165 10.46729
## 351    243.5429 10.46729
## 352    287.7835 10.46729
## 369    258.4415 10.46729
## 370    245.0424 10.46729
## 371    248.1108 10.46729
## 372    269.5209 10.46729
## 
## attr(,&quot;class&quot;)
## [1] &quot;coef.mer&quot;</code></pre>
</div>
<div id="partial-pooling-extended---varying-intercepts-andor-slopes" class="section level3">
<h3><span class="header-section-number">4.6.4</span> Partial Pooling Extended - (varying intercepts and/or slopes)</h3>
<p>We can also vary slopes if we wanted to, we saw earlier that we may have very different line fit for the reaction time for different individuals.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Run the model (note thay we also control for the effect of time by subject)</span>
model_mix_slope &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(Days<span class="op">|</span>Subject), <span class="co"># note that now 1 was replaced with Days to suggest varying slopes</span>
                        sleep)
<span class="co">#Summarise</span>
<span class="kw">summary</span>(model_mix_slope)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (Days | Subject)
##    Data: sleep
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
<p>Each subject now will have their own slope estimation, which is as the intercept is partially pooled towards the center of <em>Days</em> effects’ distribution. Note carefully the results for <em>Days</em> coefficients in the main output.</p>
<p>Whatever decision you make, it will always be driven by your data. You may want to do the model comparison exercises and analyse the residuals in model output to see how much varying intercepts/slopes improved your understanding of underlying relationships in your data.</p>
<p>In mixed model setting, we can use log-likelihood ratio test to compare the models we just built:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Make sure lmtest is loaded</span>
<span class="kw">library</span>(lmtest)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Compare the null model with random intercept model using likelihood ratio test</span>
<span class="kw">lrtest</span>(model_null,model_mix)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: Reaction ~ (1 | Subject)
## Model 2: Reaction ~ Days + (1 | Subject)
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   3 -952.16                         
## 2   4 -893.23  1 117.86  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note how we add incrementally each model to the test:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Now also compare varying intercept with both varying interecept + slopes</span>
<span class="kw">lrtest</span>(model_mix, model_mix_slope)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: Reaction ~ Days + (1 | Subject)
## Model 2: Reaction ~ Days + (Days | Subject)
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   4 -893.23                         
## 2   6 -871.81  2 42.837   4.99e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The model with random intercept improves on over null model. Having a random slope and intercept improved the model even more. Anything else to add for your results write up?</p>
<p>We can also use something which is known <em>pseudo R-squared</em> to see how much variance was explained due to our explanatory factor. We will need to calculate it by hand. We will use our model without controls where were have random intercepts and we will add <em>Days</em> to compare the explained variation that is due to addition of explanatory factors. Just a reminder of the models we need:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Null-baseline</span>
model_null &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st">  </span>(<span class="dv">1</span><span class="op">|</span>Subject) , sleep)
<span class="co">#Extended-Days</span>
model_mix &lt;-<span class="st"> </span><span class="kw">lmer</span>(Reaction <span class="op">~</span><span class="st"> </span>Days <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>Subject) , sleep)</code></pre>
<p>Let us extract the variance:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## We can try to extract the partial variance that was explained by Days for mixed model</span>
<span class="co">#Calculate variance for the null</span>
totvar_model_null &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_null)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_null)<span class="op">$</span>varcor)
<span class="co">#Calculate variance for the mixed</span>
totvar_model_mix &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_mix)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_mix)<span class="op">$</span>varcor) 
<span class="co">#Check the ratio of the difference between the too overall total variance in null</span>
Var_expl &lt;-(totvar_model_null<span class="op">-</span>totvar_model_mix)<span class="op">/</span>totvar_model_null
Var_expl</code></pre>
<pre><code>## [1] 0.2775756</code></pre>
<p>We find that about 28% of variation in reaction can be explained by time, when controlling for subjects.</p>
<p>Lets finally put all these models side by side using <code>texreg()</code>. We can check what our main coefficients are, likelihood and also export if for the paper if we want to</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Make sure to have textreg loaded</span>
<span class="kw">library</span>(texreg)</code></pre>
<p>And now put the main models that we were working with for the overall comparison. Note, how tidy the output looks and we can now focus specifically on the effect of <em>Days</em>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">screenreg</span>(<span class="kw">list</span>(model_null, model_mix))</code></pre>
<pre><code>## 
## ==================================================
##                           Model 1      Model 2    
## --------------------------------------------------
## (Intercept)                298.51 ***   251.41 ***
##                             (9.05)       (9.75)   
## Days                                     10.47 ***
##                                          (0.80)   
## --------------------------------------------------
## AIC                       1910.33      1794.47    
## BIC                       1919.91      1807.24    
## Log Likelihood            -952.16      -893.23    
## Num. obs.                  180          180       
## Num. groups: Subject        18           18       
## Var: Subject (Intercept)  1278.34      1378.18    
## Var: Residual             1958.87       960.46    
## ==================================================
## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05</code></pre>
</div>
</div>
<div id="multilevel-modelling-with-random-intercepts-and-slopes" class="section level2">
<h2><span class="header-section-number">4.7</span> Multilevel modelling with random intercepts and slopes</h2>
<p>Let’s consider another example, we can now try to fit similar models to a structure with levels that are represented by groups. Previously we had participant level variability. But what if we had participant levels that we were further grouped by other variable (i.e. country of birth, cohort, study trial)?</p>
<p>We can use mixed models as well. A very common way to illustrate the example would be using students grades across schools that vary within the pupils at the school level but also vary across schools. This is relevant in cases where grades were measured at different times as well. Let us work through something which is a bit more meaningful.</p>
<p>For example studies that look at this closely have a look at this <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2566146/">review paper</a>. The data set that we will work with has information on <em>IQ scores</em> for school students and is taken from Snijders and Bosker (2012) textbook. We will have different levels here, variability of grades within the schools and across the schools. What we want to understand is whether the average score on language test has anything to do with scores that student get in their SES and IQ and the scores at <em>the average school level</em>.</p>
<div id="overview-of-the-data-set" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Overview of the data set</h3>
<table>
<thead>
<tr class="header">
<th>variable</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>school</td>
<td>School id</td>
</tr>
<tr class="even">
<td>pupil</td>
<td>Student id</td>
</tr>
<tr class="odd">
<td>lang_score</td>
<td>language score</td>
</tr>
<tr class="even">
<td>ses</td>
<td>Socioeconomic status of the student (mean centered)</td>
</tr>
<tr class="odd">
<td>IQ_verb</td>
<td>Verbal IQ of the student (mean centered)</td>
</tr>
<tr class="even">
<td>sex</td>
<td>Gender of the student</td>
</tr>
<tr class="odd">
<td>Minority</td>
<td>Dummy indicator of whether student is of minority background</td>
</tr>
<tr class="even">
<td>sch_ses</td>
<td>Average SES in a school (mean centered)</td>
</tr>
<tr class="odd">
<td>sch_iqv</td>
<td>Average verbal IQ in a school (mean centered)</td>
</tr>
<tr class="even">
<td>sch_min</td>
<td>Proportion of minority students in a school</td>
</tr>
</tbody>
</table>
</div>
<div id="prepare" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Prepare</h3>
<p>Before we start let us load all the packages we will need later.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nlme)
<span class="kw">library</span>(lattice)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Read data in:</span>
schools&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;schools.csv&#39;</span>)
<span class="kw">head</span>(schools)</code></pre>
<pre><code>##   school pupil lang_score    ses IQ_verb sex Minority sch_ses sch_iqv
## 1      1     3         46  -4.73    3.13   0        0 -14.035 -1.4039
## 2      1     4         45 -17.73    2.63   0        1 -14.035 -1.4039
## 3      1     5         33 -12.73   -2.37   0        0 -14.035 -1.4039
## 4      1     6         46  -4.73   -0.87   0        0 -14.035 -1.4039
## 5      1     7         20 -17.73   -3.87   0        0 -14.035 -1.4039
## 6      1     8         30 -17.73   -2.37   0        1 -14.035 -1.4039
##   sch_min
## 1    0.63
## 2    0.63
## 3    0.63
## 4    0.63
## 5    0.63
## 6    0.63</code></pre>
<p>Let us first build a simple mixed effect model, we will try to predict <code>language scores</code> and will control for <code>the schoo</code> level only:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Simple model (account for school level)</span>
model_schools_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(lang_score <span class="op">~</span><span class="st">  </span>(<span class="dv">1</span><span class="op">|</span>school), <span class="dt">data =</span> schools)
<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: lang_score ~ (1 | school)
##    Data: schools
## 
## REML criterion at convergence: 26595.7
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.1850 -0.6417  0.0905  0.7226  2.5281 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  school   (Intercept) 18.24    4.271   
##  Residual             62.85    7.928   
## Number of obs: 3758, groups:  school, 211
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  41.0038     0.3257   125.9</code></pre>
<p>We can also set up a <strong>NULL</strong> model, explained variance of which we will use to calculate pseudo <strong>R squared</strong> later:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#NUll model - constant intercept</span>
model_schools_null&lt;-<span class="st"> </span><span class="kw">lm</span>(lang_score <span class="op">~</span><span class="dv">1</span>, <span class="dt">data =</span> schools)
<span class="kw">summary</span>(model_schools_null)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lang_score ~ 1, data = schools)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.413  -5.413   0.587   6.587  16.587 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  41.4130     0.1451   285.5   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.893 on 3757 degrees of freedom</code></pre>
<p>We can quickly check whether having varying intercepts improves over having a constant one for each student:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Log-likelihood ratio test ( null versus control)</span>
<span class="kw">lrtest</span>(model_schools_null,model_schools_<span class="dv">0</span>)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: lang_score ~ 1
## Model 2: lang_score ~ (1 | school)
##   #Df LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   2 -13544                         
## 2   3 -13298  1 492.54  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Lets start adding a few relevant covariates, we can start with IQ and then see how much additional variance can be explained by this variable:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Add IQ scores to the model</span>
model_schools_IQ &lt;-<span class="st"> </span><span class="kw">lmer</span>(lang_score <span class="op">~</span><span class="st"> </span>IQ_verb <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>school), <span class="dt">data =</span> schools) 
<span class="kw">summary</span>(model_schools_IQ)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: lang_score ~ IQ_verb + (1 | school)
##    Data: schools
## 
## REML criterion at convergence: 24917.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.1952 -0.6378  0.0659  0.7098  3.2132 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  school   (Intercept)  9.909   3.148   
##  Residual             40.479   6.362   
## Number of obs: 3758, groups:  school, 211
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 41.05442    0.24402  168.24
## IQ_verb      2.50722    0.05439   46.09
## 
## Correlation of Fixed Effects:
##         (Intr)
## IQ_verb 0.003</code></pre>
<p>Lets calculate variance of empty model, then with IQ so we can see the added contribution from a new covariate:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Extract the variances:</span>
totvar_model_<span class="dv">0</span> &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)<span class="op">$</span>varcor)

totvar_model_IQ &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_schools_IQ)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_IQ)<span class="op">$</span>varcor)

<span class="co"># Proportion of variance explained by IQ:</span>
Rsq_IQ &lt;-<span class="st"> </span>(totvar_model_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span>totvar_model_IQ)<span class="op">/</span>totvar_model_<span class="dv">0</span>
Rsq_IQ</code></pre>
<pre><code>## [1] 0.3786057</code></pre>
<p>There are other ways to access model fit in more standard way. Remember that the mixed effect model is fit via MLE. We can stil however built an equivalent of R sqaured measure that is based on the ratio of two log likelihoods, There is a package for that too:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MuMIn)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;MuMIn&#39;:
##   method         from
##   predict.merMod lme4</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">r.squaredGLMM</span>(model_schools_IQ)</code></pre>
<pre><code>## Warning: &#39;r.squaredGLMM&#39; now calculates a revised statistic. See the help
## page.</code></pre>
<pre><code>##            R2m       R2c
## [1,] 0.3419141 0.4713251</code></pre>
<p>We will get two outputs: R squared (marginal): 0.3423 and R sqaured (condiftional): 0.471, the former being the one that comes from accounting for fixed effect only and latter comes via combination of both random and fixed.</p>
<p>On top of that, we can also make assessment of intraclass correlation (ICC), this will allow us to investigate whether there are systematic similarities in grades within the schools. ICC is quite useful measure and can be also used to diagnose your models. If there is a very strong correlation between the units within the group or a really weak one - both may require attention.</p>
<p>Lets get one for our first model:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ICC for our first model (by hand)</span>
ICC_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)<span class="op">$</span>varcor)<span class="op">/</span>((<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_<span class="dv">0</span>)<span class="op">$</span>varcor))
ICC_<span class="dv">0</span></code></pre>
<pre><code>## [1] 0.2249341</code></pre>
<p>This tell us that about <em>22 percent</em> of variation in language scores is at the school level. This can also be stated as the following: if we picked randomly two students that belong to the same school, we would expect about 0.22 correlation in their language scores.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## ICC from the model with IQ  (by hand)</span>
ICC_IQ&lt;-<span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_IQ)<span class="op">$</span>varcor)<span class="op">/</span>((<span class="kw">summary</span>(model_schools_IQ)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_schools_IQ)<span class="op">$</span>varcor))
ICC_IQ</code></pre>
<pre><code>## [1] 0.1966476</code></pre>
<p>Or via direct function from the package <code>sjstats</code></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sjstats)
<span class="kw">icc</span>(model_schools_<span class="dv">0</span>)</code></pre>
<pre><code>## # Intraclass Correlation Coefficient
## 
##      Adjusted ICC: 0.225
##   Conditional ICC: 0.225</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">icc</span>(model_schools_IQ)</code></pre>
<pre><code>## # Intraclass Correlation Coefficient
## 
##      Adjusted ICC: 0.197
##   Conditional ICC: 0.129</code></pre>
<p>We dropped to around 0.2 now (see Adjusted), this tell us that once we look at the similarity of IQ scores at the school level, the correlation between the scores obtained on lang_score can be explained by similarities in IQ scores within the school.</p>
<p><em>Quikc note</em>:there is a slight differnce between an adjusted and conditional ICC . Former takes take all sources of uncertainty ( all random effects) into account, including the conditional ICC (fixed effects)</p>
<p>Finally, lets now add the schools average scores dynamic to our model:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Add schools averages in</span>
model_schools_IQ_sc &lt;-<span class="st"> </span><span class="kw">lmer</span>(lang_score <span class="op">~</span><span class="st"> </span>IQ_verb <span class="op">+</span><span class="st"> </span>sch_iqv <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>school), <span class="dt">data =</span> schools)
<span class="kw">summary</span>(model_schools_IQ_sc)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: lang_score ~ IQ_verb + sch_iqv + (1 | school)
##    Data: schools
## 
## REML criterion at convergence: 24893.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.2201 -0.6399  0.0631  0.7054  3.2173 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  school   (Intercept)  8.785   2.964   
##  Residual             40.442   6.359   
## Number of obs: 3758, groups:  school, 211
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  41.1132     0.2329 176.526
## IQ_verb       2.4536     0.0555  44.212
## sch_iqv       1.3127     0.2627   4.997
## 
## Correlation of Fixed Effects:
##         (Intr) IQ_vrb
## IQ_verb -0.007       
## sch_iqv  0.043 -0.209</code></pre>
<p>Try to get ICC and R squared by yourself here. You will find that controlling for average IQ in the school will improve model as well. We can thus incorporate the school levels on top of individual level of the final model and perform a likelihood ratio test.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Finally: we would like to control for effect of IQ at school level</span>
model_school_final &lt;-<span class="st"> </span><span class="kw">lmer</span>(lang_score <span class="op">~</span><span class="st"> </span>IQ_verb <span class="op">+</span><span class="st"> </span>sch_iqv <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">+</span>IQ_verb<span class="op">|</span>school), <span class="co">#note how we control for IQ here to specify that we have random intercept, individual and group level predictor)</span>
           <span class="dt">data =</span> schools)
<span class="kw">summary</span>(model_school_final)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: lang_score ~ IQ_verb + sch_iqv + (1 + IQ_verb | school)
##    Data: schools
## 
## REML criterion at convergence: 24870.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.2604 -0.6337  0.0676  0.7035  2.7622 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  school   (Intercept)  8.9787  2.9964        
##           IQ_verb      0.1995  0.4466   -0.63
##  Residual             39.6858  6.2997        
## Number of obs: 3758, groups:  school, 211
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  41.1275     0.2347 175.248
## IQ_verb       2.4802     0.0645  38.453
## sch_iqv       1.0305     0.2633   3.913
## 
## Correlation of Fixed Effects:
##         (Intr) IQ_vrb
## IQ_verb -0.279       
## sch_iqv -0.002 -0.187
## convergence code: 0
## Model failed to converge with max|grad| = 0.00224192 (tol = 0.002, component 1)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lrtest</span>(model_schools_IQ_sc, model_school_final)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: lang_score ~ IQ_verb + sch_iqv + (1 | school)
## Model 2: lang_score ~ IQ_verb + sch_iqv + (1 + IQ_verb | school)
##   #Df LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   5 -12447                         
## 2   7 -12435  2 23.394   8.32e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Finally, as usual by now, lets put everything side by side:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Lest all the models together for comparison</span>
<span class="kw">screenreg</span>(<span class="kw">list</span>(model_schools_<span class="dv">0</span>, model_schools_IQ, model_schools_IQ_sc, model_school_final))</code></pre>
<pre><code>## 
## ===========================================================================================
##                                  Model 1        Model 2        Model 3        Model 4      
## -------------------------------------------------------------------------------------------
## (Intercept)                          41.00 ***      41.05 ***      41.11 ***      41.13 ***
##                                      (0.33)         (0.24)         (0.23)         (0.23)   
## IQ_verb                                              2.51 ***       2.45 ***       2.48 ***
##                                                     (0.05)         (0.06)         (0.06)   
## sch_iqv                                                             1.31 ***       1.03 ***
##                                                                    (0.26)         (0.26)   
## -------------------------------------------------------------------------------------------
## AIC                               26601.69       24925.14       24903.93       24884.54    
## BIC                               26620.38       24950.07       24935.09       24928.16    
## Log Likelihood                   -13297.84      -12458.57      -12446.97      -12435.27    
## Num. obs.                          3758           3758           3758           3758       
## Num. groups: school                 211            211            211            211       
## Var: school (Intercept)              18.24           9.91           8.78           8.98    
## Var: Residual                        62.85          40.48          40.44          39.69    
## Var: school IQ_verb                                                                0.20    
## Cov: school (Intercept) IQ_verb                                                   -0.84    
## ===========================================================================================
## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05</code></pre>
<p>Looking now at various combinations, which one you think makes most sense both statistically and substantively?</p>
</div>
</div>
<div id="random-slopes-intercepts-and-cross-level-interactions-optional" class="section level2">
<h2><span class="header-section-number">4.8</span> Random slopes, intercepts and cross level interactions (optional)</h2>
<p>For this part we will use the dataset taken from Hox’s (2010) textbook on multilevel modelling. The dataset includes the following variables:</p>
<table>
<thead>
<tr class="header">
<th>variable</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PUPIL</td>
<td>pupil id within class</td>
</tr>
<tr class="even">
<td>CLASS</td>
<td>class id (1,…,100)</td>
</tr>
<tr class="odd">
<td>POPULAR</td>
<td>popularity score of the student</td>
</tr>
<tr class="even">
<td>SEX</td>
<td>sex of the student (0=male, 1=female)</td>
</tr>
<tr class="odd">
<td>T.EXP</td>
<td>years of experience of the teacher</td>
</tr>
<tr class="even">
<td>FEMALE</td>
<td>student is FEMALE</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Lets read the data in</span>
popularity&lt;-<span class="kw">read.csv</span>(<span class="st">&#39;Popularity.csv&#39;</span>)</code></pre>
<p>We will go on straight to building our models, starting with null once again for later comparison.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Usual set up of the null and simple model controlling for the group</span>
model_null &lt;-<span class="st"> </span><span class="kw">lm</span>(POPULAR <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> popularity)
model_pop_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">lmer</span>(POPULAR <span class="op">~</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>CLASS), <span class="dt">data =</span> popularity)
<span class="kw">summary</span>(model_pop_<span class="dv">0</span>)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: POPULAR ~ (1 | CLASS)
##    Data: popularity
## 
## REML criterion at convergence: 5115.6
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.88825 -0.63376 -0.05155  0.71091  3.00393 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  CLASS    (Intercept) 0.8798   0.9380  
##  Residual             0.6387   0.7992  
## Number of obs: 2000, groups:  CLASS, 100
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)   5.3076     0.0955   55.58</code></pre>
<p>Test whether random intercepts is an improvement:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Quick model comprison</span>
<span class="kw">lrtest</span>(model_pop_<span class="dv">0</span>,model_null)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: POPULAR ~ (1 | CLASS)
## Model 2: POPULAR ~ 1
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   3 -2557.8                         
## 2   2 -3244.8 -1 1373.9  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Lets see whether gender has anything to do with popularity:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Adding an extra covariate</span>
model_pop_sex &lt;-<span class="st"> </span><span class="kw">lmer</span>(POPULAR <span class="op">~</span><span class="st"> </span>FEMALE<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>CLASS), <span class="dt">data =</span> popularity)
<span class="kw">summary</span>(model_pop_sex)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: POPULAR ~ FEMALE + (1 | CLASS)
##    Data: popularity
## 
## REML criterion at convergence: 4492.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.3184 -0.6892  0.0018  0.5961  3.8239 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  CLASS    (Intercept) 0.8622   0.9286  
##  Residual             0.4599   0.6782  
## Number of obs: 2000, groups:  CLASS, 100
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  4.89722    0.09530   51.39
## FEMALETRUE   0.84370    0.03096   27.25
## 
## Correlation of Fixed Effects:
##            (Intr)
## FEMALETRUE -0.158</code></pre>
<p>And how the variable affects our values of R-squared:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## Pseudo R squared:</span>
totvar_mod_pop_<span class="dv">0</span> &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_pop_<span class="dv">0</span>)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_<span class="dv">0</span>)<span class="op">$</span>varcor)
totvar_mod_pop_sex &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_pop_sex)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_sex)<span class="op">$</span>varcor)
R_sq_pop &lt;-<span class="st"> </span>(totvar_mod_pop_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span>totvar_mod_pop_sex)<span class="op">/</span>totvar_mod_pop_<span class="dv">0</span>
R_sq_pop</code></pre>
<pre><code>## [1] 0.1293066</code></pre>
<p>What about ICC?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Again either by hand</span>
<span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_sex)<span class="op">$</span>varcor)<span class="op">/</span>((<span class="kw">summary</span>(model_pop_sex)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_sex)<span class="op">$</span>varcor))</code></pre>
<pre><code>## [1] 0.652142</code></pre>
<p>Anything else which can be important here? Lets add teaching experience:</p>
<pre class="sourceCode r"><code class="sourceCode r">model_pop_teach &lt;-<span class="st"> </span><span class="kw">lmer</span>(POPULAR <span class="op">~</span><span class="st"> </span>FEMALE<span class="op">+</span>T.EXP<span class="op">+</span>(<span class="dv">1</span><span class="op">|</span>CLASS), <span class="dt">data =</span> popularity)
<span class="kw">summary</span>(model_pop_teach)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: POPULAR ~ FEMALE + T.EXP + (1 | CLASS)
##    Data: popularity
## 
## REML criterion at convergence: 4444.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.3585 -0.6797  0.0244  0.5933  3.7851 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  CLASS    (Intercept) 0.4860   0.6971  
##  Residual             0.4599   0.6782  
## Number of obs: 2000, groups:  CLASS, 100
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  3.56068    0.17148  20.765
## FEMALETRUE   0.84467    0.03095  27.291
## T.EXP        0.09345    0.01085   8.609
## 
## Correlation of Fixed Effects:
##            (Intr) FEMALE
## FEMALETRUE -0.088       
## T.EXP      -0.905  0.000</code></pre>
<p>Lets check whether there is an improvement:</p>
<pre class="sourceCode r"><code class="sourceCode r">totvar_mod_pop_<span class="dv">0</span> &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_pop_<span class="dv">0</span>)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_<span class="dv">0</span>)<span class="op">$</span>varcor)
totvar_mod_teach &lt;-<span class="st"> </span>(<span class="kw">summary</span>(model_pop_teach)<span class="op">$</span>sigma)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">summary</span>(model_pop_teach)<span class="op">$</span>varcor)
Rsq_teach &lt;-<span class="st"> </span>(totvar_mod_pop_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span>totvar_mod_teach<span class="op">/</span>totvar_mod_pop_<span class="dv">0</span> )
Rsq_teach</code></pre>
<pre><code>## [1] 0.8955763</code></pre>
<p>Teacher experience plus sex of the student explain an extra chunk of variability in popularity. Does this effect varies across the schools, explore the following model yourself:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Final model (cross-level interaction)</span>
model_teach_sc &lt;-<span class="st"> </span><span class="kw">lmer</span>(POPULAR <span class="op">~</span><span class="st"> </span>FEMALE<span class="op">+</span>T.EXP<span class="op">+</span>(<span class="dv">1</span><span class="op">+</span>FEMALE<span class="op">|</span>CLASS), <span class="dt">data =</span> popularity)</code></pre>
<p>Run test to see if there are any differences:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Test</span>
<span class="kw">lrtest</span>(model_pop_teach,model_teach_sc)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: POPULAR ~ FEMALE + T.EXP + (1 | CLASS)
## Model 2: POPULAR ~ FEMALE + T.EXP + (1 + FEMALE | CLASS)
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
## 1   5 -2222.2                         
## 2   7 -2137.9  2 168.46  &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>You may note a warning.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#I ll have to change the optimizer here to ensure convergence (our model is getting a bit complex)</span>
<span class="kw">library</span>(optimx)
model_pop_interaction &lt;-<span class="st"> </span><span class="kw">lmer</span>(POPULAR <span class="op">~</span><span class="st"> </span>FEMALE<span class="op">*</span>T.EXP<span class="op">+</span>(<span class="dv">1</span><span class="op">+</span>FEMALE<span class="op">|</span>CLASS), <span class="dt">data =</span> popularity, 
                              <span class="dt">control=</span><span class="kw">lmerControl</span>(<span class="dt">optimizer=</span><span class="st">&quot;optimx&quot;</span>, <span class="co">#make a note of the optimizer here, you can try to vary those</span>
                                 <span class="dt">optCtrl=</span><span class="kw">list</span>(<span class="dt">method=</span><span class="st">&#39;nlminb&#39;</span>)))
<span class="kw">summary</span>(model_pop_interaction)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: POPULAR ~ FEMALE * T.EXP + (1 + FEMALE | CLASS)
##    Data: popularity
## Control: 
## lmerControl(optimizer = &quot;optimx&quot;, optCtrl = list(method = &quot;nlminb&quot;))
## 
## REML criterion at convergence: 4268.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.9337 -0.6519  0.0216  0.5307  3.4883 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  CLASS    (Intercept) 0.4120   0.6419       
##           FEMALETRUE  0.2264   0.4758   0.08
##  Residual             0.3924   0.6264       
## Number of obs: 2000, groups:  CLASS, 100
## 
## Fixed effects:
##                   Estimate Std. Error t value
## (Intercept)       3.313521   0.161015  20.579
## FEMALETRUE        1.329594   0.133049   9.993
## T.EXP             0.110235   0.010232  10.774
## FEMALETRUE:T.EXP -0.034035   0.008457  -4.024
## 
## Correlation of Fixed Effects:
##             (Intr) FEMALETRUE T.EXP 
## FEMALETRUE  -0.046                  
## T.EXP       -0.909  0.042           
## FEMALETRUE:  0.042 -0.908     -0.046</code></pre>
<p>Find the change in explained variance by yourself here and for the grand finale - put all side by side:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Models summaries</span>
<span class="kw">screenreg</span>(<span class="kw">list</span>( model_pop_sex, model_pop_teach, model_teach_sc, model_pop_interaction))</code></pre>
<pre><code>## 
## =========================================================================================
##                                    Model 1       Model 2       Model 3       Model 4     
## -----------------------------------------------------------------------------------------
## (Intercept)                            4.90 ***      3.56 ***      3.34 ***      3.31 ***
##                                       (0.10)        (0.17)        (0.16)        (0.16)   
## FEMALETRUE                             0.84 ***      0.84 ***      0.84 ***      1.33 ***
##                                       (0.03)        (0.03)        (0.06)        (0.13)   
## T.EXP                                                0.09 ***      0.11 ***      0.11 ***
##                                                     (0.01)        (0.01)        (0.01)   
## FEMALETRUE:T.EXP                                                                -0.03 ***
##                                                                                 (0.01)   
## -----------------------------------------------------------------------------------------
## AIC                                 4500.89       4454.36       4289.89       4284.43    
## BIC                                 4523.30       4482.36       4329.10       4329.24    
## Log Likelihood                     -2246.45      -2222.18      -2137.95      -2134.22    
## Num. obs.                           2000          2000          2000          2000       
## Num. groups: CLASS                   100           100           100           100       
## Var: CLASS (Intercept)                 0.86          0.49          0.41          0.41    
## Var: Residual                          0.46          0.46          0.39          0.39    
## Var: CLASS FEMALETRUE                                              0.27          0.23    
## Cov: CLASS (Intercept) FEMALETRUE                                  0.02          0.02    
## =========================================================================================
## *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05</code></pre>
<p>What do you conclude? Which model should we go with?</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tests-and-modelling-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="testing-the-assumptions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/Week_3.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
