```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
set.seed(1)

ggplot2::theme_set(ggplot2::theme_gray(base_size=15))
```



# Testing hypotheses {#chap-hyp-test}


<div class="lo">
#### Instructions {-}
  
- In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour.
- The Rmarkdown file for this week is [here](https://uoe-psychology.github.io/uoe_psystats/dapr1/labsheets/week_13_practice.Rmd).


#### Learning outcomes {-}

**LO1.** Understand null and alternative hypotheses, and how to specify them for a given research question.

**LO2.** Understand how to obtain a null distribution via simulation.

**LO3.** Understand statistical significance and how to calculate p-values from null distributions.

</div>



## Recap
In [Week 11](#chap-sampling-distributions) we saw that a population is described by parameters, and samples are described by statistics. We typically can not measure the entire population, so a population parameter needs to be estimated by a sample statistic. We quantified the uncertainty in our estimate by the standard error (SE) of the sample statistic. This is defined as the standard deviation of the sampling distribution. The latter shows the values of the sample statistic when computed on many samples of the same size from the same population.

In practice, however, we are not able to quantify the SE by taking repeated samples from the population, as due to money or time constraints we only have one sample of size $n$. In [Week 12](#bootstrapping-and-confidence-intervals) we learned to approximate the sampling distribution by bootstrapping.
This approach repeatedly samples with replacement from the observed sample, using the same sample size.
The standard deviation of the bootstrap distribution is an estimate of the SE of the statistic.
Using the statistic from the observed sample, and the estimated SE from the bootstrap distribution, we learned how to give a range of plausible values for the population parameter.

This week we will learn how to use the sample data to answer a wide range of questions about a population.



## Walkthrough


<div class='red'>
#### Research question of the walkthrough {-}
Is there such a thing as extrasensory perception (ESP), also known as the "sixth sense"?
</div>


### Extrasensory perception (ESP) {-}

According to [Wikipedia](https://en.wikipedia.org/wiki/Extrasensory_perception), "__Extrasensory perception__ or __ESP__, also called the sixth sense, includes claimed reception of information not gained through the recognized physical senses, but sensed with the mind."

A famous test for ESP involves a set of cards, shown in Figure \@ref(fig:zener), known as Zener cards:
```{r zener, echo = FALSE, out.width = '80%', fig.align = 'center', fig.cap="Zener cards (Source: https://en.wikipedia.org/wiki/Extrasensory_perception)"}
knitr::include_graphics('images/zener.png')
```

There are five different cards, one for each of the following symbols:

1. circle
2. cross
3. wavy lines
4. square
5. star

Subjects randomly draw a card from these five, and telepathically communicate the chosen card to another subject who then tries to guess the symbol. No visual or auditory clues are allowed.


### Example 1: ESP experimental results {-#example-esp-results}
In an experiment conducted on a class of $n = 98$ students, 25 of them correctly guessed their partner's card. 

This is equivalent to saying that the observed sample proportion of correct guesses is $\hat{p} = 25/98 = 0.26$.

`r msmbstyle::question_begin(header = "&#x25BA; Question")`
Is the sample proportion of correct guesses high enough to provide evidence of ESP?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
We are not sure yet.

We still have to introduce the statistical framework that, assuming no ESP, allows us to determine either:

- if the observed sample statistic is highly unlikely to happen due to sampling variability,
- or if it would be very likely to see such a value by sampling variation.

As we will see, this framework is called __hypothesis testing__ and involves using a __statistical test__.
`r msmbstyle::solution_end()`



### Example 2: In-class activity {-#example-in-class}

- Everyone in the class randomly picks one of the five symbols in Figure \@ref(fig:zener).
- Draw the symbol on a bit of paper __without showing it to anyone!__
- Pair up with someone.
- Telepathically communicate your symbol to your partner, who has to guess it without any visual or auditory clues.
- Switch roles.
- Did your partner guess your card correctly?


Keep track of the sample size $n$ of the in-class experiment, and the sample statistic $\hat{p}$.
We will use these two values later on to see if we have evidence of ESP in this class.



### The random chance model {-}

`r msmbstyle::question_begin()`
If there was no such thing as ESP, what proportion of guesses do you expect to be correct?
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin()`
If there is no ESP, because there are five cards, each person has a chance of correctly guessing the symbol equal to $1/5 = 0.2$.

This is similar to asking: what's the chance of correctly guessing the face of a die? If we had to randomly guess, since there are 6 faces, we have a probability of $1/6$. 
The only difference is that instead of six faces, here we only have five, hence the $1/5$ chance.
`r msmbstyle::solution_end()`


We use the chance-alone model to help us decide whether:

- the observed proportion could easily happen by sampling variation if we are randomly guessing the symbol, or
- the underlying process is something else, such as having extrasensory perception.

The chance model for the ESP example is specified by the probability of correctly guessing the symbol if we were to guess at random: $p = 1/5$.

We call this probability the **null hypothesis**, and it represents what we would expect if there was no ESP ("no effect").

We denote the null hypothesis by:
<center>
$$
H_0 : \  p = \frac{1}{5}
$$
</center>



### The alternative explanation {-}

The research question of interest (i.e. if ESP exists) is captured by the __alternative hypothesis__, which contradicts the null hypothesis.
We always put the research claim of interest in the alternative hypothesis, rather than the null one.

The null and the alternative hypotheses are competing claims about the population parameter and we can not have the same value of the parameter included in both the null and the alternative. 

In the ESP example, we would have evidence of ESP if the chance of correctly guessing the symbol was greater than when randomly guessing it:

<center>
$$
H_1 : p > \frac{1}{5}
$$
</center>




### Statistical test {-}

As we saw in [Week 11](#chap-sampling-distributions), sample statistics vary from sample to sample.
Even if the population proportion was really equal to 1/5, not every random sample would have a sample proportion exactly equal to 1/5.

The key question is then: __how do we decide in a principled way if a sample proportion is sufficiently above 1/5 to suggest presence of ESP?__

In [Example 1](#example-esp-results) we obtained a sample proportion of 25/98 = 0.26. 
Is 0.26 sufficiently higher than 0.20 to suggest presence of ESP?


The set of principles that allows us to make an informed claim under uncertainty about the population is called **statistical hypothesis testing**.

<div class="def">
#### Statistical test {-}

A ___statistical test___ is a method that uses the data collected on a sample to assess a claim about the population.
</div>



### Understanding statistical evidence {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question")`
If $\hat{p}$ denotes the proportion of correctly guessed symbols in an ESP experiment, which of the following sample proportions provide the strongest evidence **in favour** of ESP?

a. $\hat{p} = 0$
b. $\hat{p} = 1/5$
c. $\hat{p} = 1/2$
d. $\hat{p} = 3/4$
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
The correct answer is (d) as $\hat{p} = 3/4 = 0.75$ is the highest proportion of correctly guessed symbols.
`r msmbstyle::solution_end()`

This example shows us that we assess a claim about a population parameter by quantifying the statistical evidence that the observed statistic gives against the null hypothesis and in favour of the alternative hypothesis.



### Statistical hypotheses {-}

We perform statistical tests on two competing hypotheses about a population parameter:

- __Null hypothesis__ $H_0$: typically a claim about "no effect" or "no difference between groups";
- __Alternative hypothesis__ $H_1$: the claim we seek evidence for.

The null hypothesis is typically a very specific claim about the population parameter, while the alternative hypothesis is a broader statement.

To visually help you identify the null vs the alternative, typically:

- $H_0$ includes an $=$ sign
- $H_1$ includes $>$, $<$ or $\neq$

If $H_1$ contains either $>$ or $<$, we say that the alternative hypothesis is **one-sided**.
If $H_1$ contains $\neq$, we say that the alternative hypothesis is **two-sided**.

Putting this together, in the ESP investigation we have two competing hypotheses:

<center>
$$
H_0 : \  p = \frac{1}{5} \\
H_1 : \  p > \frac{1}{5}
$$
</center>


### Statistical significance {-}

Now that we have defined a framework to assess two competing hypotheses about a population parameter, how do we quantify the evidence that the sample data bring in favour of the alternative hypothesis?

We do this by quantifying how unusual it is to obtain a statistic as extreme or more extreme than the observed statistic, if the null hypothesis is true.
If it is very unusual, we have significant evidence against the null hypothesis.

<div class="def">
#### Statistical significance {-#def-statistical-significance}

Assuming the null hypothesis to be true, we say that the sample results are ___statistically significant___ if it is unlikely that by random chance alone we obtain a statistic that is as extreme as the observed sample statistic, or more extreme (in the direction specified by the alternative hypothesis).
</div>


Hypothesis testing basically boils down to quantifying how likely or unlikely it is to observe the sample statistic if the null hypothesis is true. Two decisions are possible:

- If the sample data are statistically significant, we __reject__ $H_0$ as we have enough evidence against $H_0$ and in favour of $H_1$.
- If the sample data are not statistically significant, we __do not reject__ $H_0$ as we do not have convincing evidence against $H_0$.

This has an analogy in law. One of the fundamental legal principles is the **presumption of innocence**, which says that a person is considered innocent until proven guilty, and that the evidence must be beyond reasonable doubt.

The null hypothesis ($H_0$), corresponding to no effect, is that the defendant is innocent. The alternative hypothesis ($H_1$) is that the defendant is guilty.

The court presumes $H_0$ to be true (the defendant is innocent) unless the prosecutor can provide strong evidence that the defendant is guilty beyond a reasonable doubt. The burden of proof is on the prosecutor (in our case, the data) to convince the court that the defendant is guilty.

Similarly, we retain $H_0$ unless there is strong evidence to reject $H_0$.

Recall that:

- $p$ = proportion of correct guesses of the symbol
- $H_0 : \  p = 1/5$ vs $H_1 : \  p > 1/5$



#### Scenario 1: statistically significant results {-}

If results are __statistically significant__:

- The sample proportion $\hat{p}$ of correct guesses is unlikely to occur by random chance alone. Recall that by random chance we mean if ESP does not exist and thus $p = 1/5$.
- The sample data provide evidence that the population proportion of correct guesses is higher than 1/5, meaning that we have evidence of ESP.


#### Scenario 2: not statistically significant results {-}

If results are __not statistically significant__:

- The sample proportion $\hat{p}$ of correct guesses could easily happen by random chance alone. 
Recall that by random chance we mean if ESP does not exist and thus $p = 1/5$.
- The sample data do not provide enough evidence to conclude that $p > 1/5$ or that ESP exists.


### Null distribution {-}

In the previous section, we said that if the observed statistic is very unusual, we have significant evidence against the null hypothesis. But __how do we quantify if a value is unusual?__

The key idea is to look at the sampling distribution of the statistic if $H_0$ were true. We do this by generating many samples, assuming the null hypothesis to be true, and computing the statistic on each of the generated samples. The distribution of these statistics is called the **null distribution**.

<div class="def">
#### Null distribution {-}

The ___null distribution___ shows the values of the statistic that we expect to obtain by sampling variation, if the null hypothesis is true.

**Centre**: The null distribution is centred at the value of the population parameter specified in the null hypothesis.
</div>

We will now compute the null distribution for [Example 1](#example-esp-results) on ESP. One experiment is a sequence of $n = 98$ guesses (resulting in a sequence of S's and F's, where S = success and F = failure) and this represents **one sample**. The required steps are:

1. Obtain many random samples of size $n = 98$, assuming a probability of success equal to 1/5
2. Compute the proportion of successes for each sample
3. Plot the distribution of proportions

Load the required packages:
```{r, message=FALSE}
library(tidyverse)
library(moderndive)
```

Create a tibble with the possible outcome of one play. Either:

- your partner correctly guesses your card (S = success), or 
- your partner does not correctly guess your card (F = failure).

_**Note:** Make sure the column name in the tibble is `vals` and that it is a factor._
```{r, message=FALSE}
outcomes <- tibble(vals = factor(c('S', 'F')))
outcomes
```

Assign the probabilities of each possible outcome, remembering that the probabilities must sum to one:

- Probability of success (S) = probability of correctly guessing the symbol: $p = 1/5$ 
- Probability of failure (F) is the complement to one: $1 - p = 1 - 1/5 = 4/5$.

```{r}
prob <- c(1/5, 4/5)
```

Generate 1,000 samples of size $n = 98$ with the probabilities specified by the null hypothesis:
```{r, message=FALSE, warning=FALSE}
samples <- rep_sample_n(outcomes, size = 98, replace = TRUE, reps = 1000, prob = prob)
samples
```

Note that the tibble has $98,000$ rows. Each sample of size $n = 98$ represents a sequence of 98 plays where each play can either be S (your partner correctly guessed your card) or F (your partner didn't guess correctly). Hence, each sample is a sequence of S's and F's. Because we have 1,000 of such samples, the total number of rows will be $98 \times 1,000 = 98,000$.

We can now compute the proportion of successes in each sample:

```{r}
null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'S') / n())

null_distribution
```

Let us now plot the 1,000 sample proportions obtained assuming the null hypothesis to be true. This plot shows the **null distribution**, and each dot represents one of the 1000 proportions from the above tibble:
```{r, fig.height=7.3, fig.align='center', out.width='70%'}
ggplot(null_distribution, aes(x = prop)) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = 'white', stackratio = 0.5) +
  labs(x = expr(hat(p)))
```

Where does the sample statistic from [Example 1](#example-esp-results) lie in the null distribution? Let's add a red vertical line showing the value of the observed statistic, $\hat{p} = 0.26$:

```{r, fig.height=7.3, fig.align='center', out.width='70%'}
ggplot(null_distribution, aes(x = prop)) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = 'white', stackratio = 0.5) +
  geom_vline(xintercept = 0.26, color = 'tomato1', size = 1) +
  labs(x = expr(hat(p)))
```


Do we have a high chance of observing the statistic $\hat{p} = 0.26$ under the null hypothesis of random guessing?

Different researchers might reach different conclusions by looking at this plot. 
Rather than taking a decision by visually inspecting the plot, we want to find a generically applicable tool that would make different researchers all reach to the same conclusion.



### P-value {-}

Recall now the definition of [statistical significance](#def-statistical-significance). We have evidence against the null hypothesis if it would be very unusual to obtain statistics as extreme or more extreme than the observed statistic in the null distribution.

To summarise: in order to measure how unusual the observed statistic is under the null hypothesis, we need to generate many statistics under the null hypothesis (null distribution) and see what's the proportion of the generated statistics that are as extreme as, or more extreme than, the observed statistic.

The proportion of statistics in the null distribution as extreme or more extreme than the observed statistic is known as the **p-value**.

**The smaller the p-value, the higher the statistical evidence against the null hypothesis.**


<div class="def">
#### P-value {-}

The ___p-value___ represents the chance of obtaining a statistic as extreme or more extreme than the observed one, if the null hypothesis were true.
</div>


In [Example 1](#example-esp-results) on ESP, we can identify the statistics in the null distribution that are as extreme or more extreme than the observed one ($\hat{p} = 0.26$) by colour-coding them:

```{r, fig.height=7.3, fig.align='center', out.width='70%'}
ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.26))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= 0.26)) +
  theme(legend.position="top")
```


We now calculate the proportion of statistics in the null distribution which are greater than or equal 0.26. You can either do that by eye, counting the red dots and dividing them by the total number of dots (1,000) or, more quickly:

```{r}
pvalue <- null_distribution %>%
  summarise(pvalue = sum(prop >= 0.26) / n())

pvalue
```


_**Interpretation of the p-value:** If we were to randomly guess the card symbol in [Example 1](#example-esp-results), the chance of getting a proportion as high as 0.26 is `r pvalue$pvalue %>% round(2)`._


The p-value needs to be calculated in the direction specified by the alternative hypothesis:

- The p-value of a one-sided hypothesis is the proportion in tail specified by $H_1$.
- The p-value of a two-sided hypothesis is twice the proportion in the smallest tail, i.e. the tail with the smallest count.

In the example below, the smallest tail (the tail with the lowest count) is the right tail. So if we were to perform an hypothesis test for the two-sided alternative $p \neq 1/5$, we would compute it as twice the proportion of statistics greater than or equal to the observed statistic (twice the proportion in the right tail):
```{r echo=FALSE, fig.height=4.5, fig.align='center', out.width='100%', message=FALSE, warning=FALSE}
library(patchwork)

plt1 <- ggplot(null_distribution, aes(x = prop, fill = (prop <= 0.26))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5, stroke = 0.2) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  labs(x = expr(hat(p)), fill = expr(hat(p) <= 0.26), title = 'Left tail') +
  theme_gray(base_size = 10) +
  theme(legend.position="top", plot.title = element_text(hjust = 0.5, face = 'bold'))

plt2 <- ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.26))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5, stroke = 0.2) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= 0.26), title = 'Right tail') +
  theme_gray(base_size = 10) +
  theme(legend.position="top", plot.title = element_text(hjust = 0.5, face = 'bold'))

plt1 | plt2
```

_**Note:** If you were to calculate the p-value as twice the proportion in the bigger tail, you would obtain a p-value greater than 1. But remember, a probability must always be between 0 and 1!_

You can calculate a p-value via randomization for any statistic:

1. Simulate many samples assuming $H_0$ to be true.
2. Compute the statistic on each of the simulated samples.
3. Compute the proportion of simulated statistics as extreme or more extreme than the observed statistic, in the direction specified by the alternative hypothesis.



### Making a formal decision {-}

The smaller the p-value, the greater the evidence that the data provide against the null hypothesis $H_0$.

`r msmbstyle::question_begin()`
Which of the following p-values gives the strongest evidence against $H_0$?

a. 0.005
b. 0.1
c. 0.35
d. 0.92
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
The correct answer is (a) as it is the smallest p-value.
`r msmbstyle::solution_end()`


To summarise, the outcome of a statistical test is either:

1. The p-value is small:
    + we reject the null hypothesis;
    + the observed sample statistic would be extreme in the null distribution;
    + the results are statistically significant;
    + the test concludes that we have enough evidence in favour of $H_1$.
2. The p-value is not small:
    + we do not reject the null hypothesis;
    + the observed sample statistic would not be extreme in the null distribution;
    + the results are not statistically significant;
    + we do not have sufficient evidence to reject $H_0$.


__How small should the p-value be?__

This is set by the researcher before seeing any data.
We decide if a p-value is small or not by specifying a threshold below which a p-value is deemed to be small.

<div class="def">
#### Significance level {-}

The ___significance level___ $\alpha$ is the threshold below which we deem a p-value small enough to reject the null hypothesis.
</div>

For a given significance level $\alpha$, specified before collecting any data:

- if the p-value $\leq \alpha$ we reject $H_0$,
- if the p-value $> \alpha$ we do not reject $H_0$.

Typically, "default" choices for $\alpha$ are $0.05$ or $0.01$.



_**Caution:** Never accept $H_0$. "Do not reject $H_0$" is not the same as "accept $H_0$". Not having sufficient evidence against $H_0$ does not mean having evidence for $H_0$._


The following table summarizes in words the strength of evidence that the sample results bring in favour of the alternative hypothesis for different p-values:

|          p-value             | strength of evidence                             |
|:----------------------------:|:-------------------------------------------------|
|      0.1 $<$ p-value         | not much evidence against null hypothesis        |
|  0.05 $<$ p-value $\leq$ 0.1 | moderate evidence against the null hypothesis    |
| 0.01 $<$ p-value $\leq$ 0.05 | strong evidence against the null hypothesis      |
|     p-value $\leq$ 0.01      | very strong evidence against the null hypothesis |



We can now wrap up [Example 1](#example-esp-results) on ESP, and conclude that at a significance level of 0.05, we do not reject the null hypothesis as the p-value 0.07 is greater than $\alpha = 0.05$. Hence, the sample data do not show significant evidence for ESP.


An alternative to the p-value as a measure of the strength of evidence is given by the **standardized statistic**. This measures how many standard deviations away from the mean of the null distribution the observed statistic is. A standardized statistic is typically denoted by $z$, and can be calculated as:

$$
\textrm{standardized statistic} = z = \frac{\textrm{statistic} - \textrm{mean of null distribution}}{\textrm{standard deviation of null distribution}}
$$


We interpret a standardized statistic according to the following table:

| standardized statistic  | strength of evidence                              |
|:-----------------------:|:--------------------------------------------------|
|between −1.5 and 1.5     | little or no evidence against the null hypothesis |
|below −1.5 or above 1.5  | moderate evidence against the null hypothesis     |
|below −2 or above 2      | strong evidence against the null hypothesis       |
|below −3 or above 3      | very strong evidence against the null hypothesis  |


To compute the standardized statistic for [Example 1](#example-esp-results) on ESP, we first need to find the standard deviation of the null distribution:
```{r}
null_distribution %>%
  summarise(sd = sd(prop))
```

The standardized statistic is then:
<center>
$$
\textrm{standardized statistic} = z = \frac{0.26 - 0.20}{0.04} = 1.5
$$
</center>


If we interpreted the standardized statistic, rather than the p-value, for [Example 1](#example-esp-results) on ESP, we would have reached the same conclusion. As the observed statistic (0.26) is only 1.5 standard deviations above the mean of the null distribution (0.20), the sample data bring little or no evidence against the null hypothesis, thus we do not reject $H_0$.


This idealized version of a histogram shows how far from the mean of the null distribution each standardized statistic value is:
```{r standardized-statistic, fig.height=3, echo=FALSE, fig.align='center'}
xg = seq(-4, 4, by = 0.001)
yg = dnorm(xg)
dfg = tibble(xg, yg)

ggplot(dfg, aes(x = xg, y = yg, ymin = 0, ymax = yg)) +
  geom_line() +
  geom_ribbon(fill = 'lightyellow') + 
  geom_ribbon(data=subset(dfg, xg<0.01 & xg>-0.01), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg<3.01 & xg>2.99), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg > -3.01 & xg < -2.99), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg<2.01 & xg>1.99), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg > -2.01 & xg < -1.99), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg<1.51 & xg>1.49), aes(ymax=yg), ymin=0, fill="darkblue") +
  geom_ribbon(data=subset(dfg, xg > -1.51 & xg < -1.49), aes(ymax=yg), ymin=0, fill="darkblue") +
  theme_minimal(base_size = 15) +
  scale_x_continuous(breaks = c(-3, -2, -1.5, 0, 1.5, 2, 3)) +
  scale_y_continuous(breaks = NULL) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  labs(x = NULL, y = NULL)
```

Recall from [Week 12](#calculating-confidence-intervals-using-a-bootstrap-standard-error) that within 2 standard deviations around the mean lie approximately 95% of all values.
Similarly, within 3 standard deviations around the mean lie 99.7% of all values.


_**Note:** The p-value is a probability, so it's always between 0 and 1. A standardized statistic, instead, can be positive or negative._


## Summary

Today we have learned to assess how much evidence the sample data bring against the null hypothesis and in favour of the alternative hypothesis.

The null hypothesis, denoted $H_0$, is a claim about a population parameter that is initially assumed to be true. It typically represents "no effect" or "no difference between groups".

The alternative hypothesis, denoted $H_1$, is the claim we seek evidence for.

We assessed the strength of evidence against $H_0$ following these steps:

1. Generate many samples assuming the null hypothesis to be true;
2. Obtain the null distribution by computing the statistic on each of the generated samples;
3. Compute the p-value as the proportion of statistics in the null distribution as extreme or more extreme than the observed statistic, in the direction specified by the alternative hypothesis.

The sample size and the observed statistic in the ESP experiment are:
```{r, eval=FALSE}
size <- 98
observed_prop <- 0.26
```

We obtained the null distribution with the following code:
```{r, message=FALSE, warning=FALSE, eval=FALSE}
outcomes <- tibble(vals = factor(c('S', 'F')))
prob <- c(1/5, 4/5)
reps <- 1000

null_distribution <- outcomes %>%
  rep_sample_n(size = size, replace = TRUE, reps = reps, prob = prob) %>%
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'S') / n())
```

The following code chunk plots the null distribution and shows in red the statistics that are as extreme or more extreme than the observed statistic:
```{r, eval=FALSE, fig.height=7.3, fig.align='center', out.width='70%'}
ggplot(null_distribution, aes(x = prop, fill = (prop >= observed_prop))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= !!observed_prop)) +
  theme(legend.position="top")
```

We compute the p-value as the proportion of statistics in the null distribution which are as extreme or more extreme than the observed statistic:
```{r, eval=FALSE}
pvalue <- null_distribution %>%
  summarise(pvalue = sum(prop >= observed_prop) / n())

pvalue
```



## Lab

<div class="red">
#### Research question of the lab {-#w13-question-lab}

Has the average body temperature for healthy humans changed from the long-thought 37 °C?
</div>

In today's lab we will investigate the average body temperature for healthy humans.
You might probably be thinking that the average is about 37 °C, and this is what most people would answer as this has been taken as granted for many years.

However, could it be possible that the average body temperature for healthy humans has changed over time? Perhaps this could be due to the climate change?

We will use data^[Shoemaker, A. L. (1996). _What’s Normal: Temperature, Gender and Heartrate. Journal of Statistics Education, 4_(2), 4.] comprising measurements on body temperature and pulse rate for a sample of $n = 50$ healthy subjects.

Using the concepts learned today, the question boils down to:
_do the sample data provide significant evidence (at a 5% level) that the average body temperature is really different from the long-thought 37 °C?_



### Required packages {-}

Before attempting to answer the following questions, make sure to run the following code chunk, which assumes that you have already installed the packages `tidyverse` and `moderndive`.
If you have not installed them yet, type `install.packages("tidyverse")` and `install.packages("moderndive")` in the R console.

Load the `tidyverse` and `moderndive` packages:
```{r message = FALSE}
library(tidyverse)
library(moderndive)
```


### Importing the data {-}

Let's start by loading the data using the function `read_tsv` to read tab separated values:
```{r message=FALSE}
bodytemp <- read_tsv('https://edin.ac/2vkjSaw')
bodytemp
```


### Data inspection {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 1")`
1. What are the names of the variables in the dataset?
2. What are the dimensions of the tibble?
3. Are there any missing values? _**Hint:** Use the function `anyNA()` to check if there are any Not Available (NA) entries._
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`

```{r}
names(bodytemp)
```
1. The variable names are `BodyTemp` and `Pulse`.

```{r}
dim(bodytemp)
```
2. The tibble has 50 rows and two columns.

```{r}
anyNA(bodytemp)
```
3. The tibble does not appear to have missing values.

`r msmbstyle::solution_end()`



### Sample average and standard deviation {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 2")`
What is the average temperature in the sample and the standard deviation?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r}
sample_stats <- bodytemp %>%
  summarise(avg_temp = mean(BodyTemp), 
            sd_temp = sd(BodyTemp))

sample_stats
```

The average body temperature in the sample is $\bar{x} =$ `r sample_stats %>% pull(avg_temp) %>% round(2)` °C and the standard deviation is $s =$ `r sample_stats %>% pull(sd_temp) %>% round(2)` °C.

`r msmbstyle::solution_end()`


### Null and alternative hypothesis {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 3")`
State the null and alternative hypothesis for the research question of this lab.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
<center>
$$
H_0: \ \mu = 37\ {}^\circ\mathrm{C} \\
H_1: \ \mu \neq 37\ {}^\circ\mathrm{C}
$$
</center>
`r msmbstyle::solution_end()`



### Bootstrap distribution and null distribution {-}

In order to calculate a p-value, we need to obtain the sampling distribution of the mean assuming that the null hypothesis is true (__null distribution__).

However, we only have one sample, so we must approximate the sampling distribution with a bootstrap distribution showing how the sample mean varies from bootstrap sample to bootstrap sample.

The required steps are:

1. Obtain a bootstrap distribution (this is centred at the sample mean $\bar{x} =$ `r sample_stats %>% pull(avg_temp) %>% round(2)`)
2. Shift the bootstrap distribution to be centred at the parameter value specified in the null hypothesis ($\mu =$ 37). This is the null distribution.

_**Note:** Why did we shift the bootstrap distribution to obtain the null distribution? Because the null distribution must be centred at the value of the population parameter specified in the null hypothesis._

`r msmbstyle::question_begin(header = "&#x25BA; Question 4")`
Compute the bootstrap distribution of the mean using 10,000 repeated samples.

Plot the bootstrap distribution using a histogram.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
The following code chunk computes the bootstrap distribution using 10,000 repeated samples:
```{r}
bootstrap_distribution <- bodytemp %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 10000) %>%
  group_by(replicate) %>%
  summarise(avg_temp = mean(BodyTemp))

bootstrap_distribution
```

Let's plot the bootstrap distribution as a histogram, rather than a dotplot:
```{r, message=FALSE}
ggplot(bootstrap_distribution, aes(x = avg_temp)) +
  geom_histogram(color = 'white') +
  labs(x = expr(bar(x)))
```

The bootstrap distribution is centred around the sample mean:
```{r}
bootstrap_distribution %>% 
  summarise(avg = mean(avg_temp))
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin(header = "&#x25BA; Question 5")`
Compute the null distribution, which needs to be centred at the value of the parameter specified by the null hypothesis.

Plot the null distribution using a histogram.

_**Hint**: Since you already have the bootstrap distribution, which is centred at the sample mean, you can shift the original sample by adding to each body temperature the difference between the parameter value in the null hypothesis and the sample mean._
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
The null distribution needs to be centred at the value of the parameter specified by the null hypothesis.
The bootstrap distribution we computed earlier is centred at the sample mean.

Recall that:
<center>
$$
H_0: \ \mu = 37\ {}^\circ\mathrm{C} \\
H_1: \ \mu \neq 37\ {}^\circ\mathrm{C}
$$
</center>

We could shift the bootstrap distribution to have mean 37 °C by adding 37 - `r bodytemp %>% summarise(avg_temp = mean(BodyTemp)) %>% pull(avg_temp) %>% round(2)` °C = `r 37 - bodytemp %>% summarise(avg_temp = mean(BodyTemp)) %>% pull(avg_temp) %>% round(2)` °C to each body temperature in the sample.

Let us calculate the shift:
```{r}
null_hypothesis <- 37

observed_mean <- bodytemp %>%
  summarise(avg_temp = mean(BodyTemp)) %>%
  pull(avg_temp)
observed_mean

shift <- null_hypothesis - observed_mean
shift
```

Shift the original sample data values:
```{r}
bodytemp_shifted <- bodytemp %>%
  mutate(BodyTemp = BodyTemp + shift)

null_distribution <- bodytemp_shifted %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 10000) %>%
  group_by(replicate) %>%
  summarise(avg_temp = mean(BodyTemp))
null_distribution
```

The null distribution is centred at the parameter value specified in the null:
```{r}
null_distribution %>%
  summarise(avg = mean(avg_temp))
```

We can visualise the null distribution using a histogram:
```{r, message=FALSE}
ggplot(null_distribution, aes(x = avg_temp)) +
  geom_histogram(color = 'white') +
  labs(x = expr(bar(x)))
```

`r msmbstyle::solution_end()`


### Visualise the observed statistic on the null distribution {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 6")`
Show a histogram of the null distribution, superimposing a vertical red line displaying the observed statistic.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
```{r, message=FALSE}
ggplot(null_distribution, aes(x = avg_temp)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = observed_mean, color = 'tomato1', size = 1) +
  labs(x = expr(bar(x)))
```
`r msmbstyle::solution_end()`


### P-value {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 7")`
Calculate the proportion of means as extreme as the observed statistic in the null distribution.

_**Hint:** As this is a double-sided alternative hypothesis, remember to use 2 * p-value of the smallest tail._
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
This is a two-sided alternative, hence the p-value is two times the proportion in the smallest tail.

Let's plot the smallest tail:
```{r, message=FALSE}
ggplot(null_distribution, aes(x = avg_temp, fill = (avg_temp <= observed_mean))) +
  geom_histogram(color = 'black', binwidth = 0.01, size = 0.1) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  labs(x = expr(bar(x)), fill = expr(bar(x) <= !!round(observed_mean, 2)))
```


```{r}
null_distribution %>%
  summarise(count_smaller = sum(avg_temp <= observed_mean))
```
Out of the 10,000 means in the null distribution, `r null_distribution %>%
  summarise(sum(avg_temp < pull(sample_stats, avg_temp)))` of them are lower than the observed sample statistic (`r sample_stats$avg_temp %>% round(2)`).
  
The proportion of means as low as the observed mean (`r sample_stats$avg_temp %>% round(2)`) in the null distribution is then:
```{r}
null_distribution %>%
  summarise(prop_smaller = sum(avg_temp <= observed_mean) / n())
```

Since this test has a double-sided alternative, we must double this proportion in order to obtain the p-value. This represents the proportion of means in the null distribution as extreme as the observed one:
```{r}
pvalue <- null_distribution %>%
  summarise(pvalue = 2 * sum(avg_temp <= observed_mean) / n())

pvalue
```

The p-value is equal to `r pvalue$pvalue`.
`r msmbstyle::solution_end()`


### Interpreting the p-value {-}

`r msmbstyle::question_begin(header = "&#x25BA; Question 8")`
Using the p-value found in the previous question, how would you answer the [research question](#w13-question-lab) of the lab?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin()`
At a significance level of 5%, the very small p-value we have found, `r pvalue$pvalue` < 0.05, gives very strong evidence against the null hypothesis that the average body temperature for healthy humans is 37 °C.

However, it is worth noting the difference between _statistical significance_ and _practical significance_. 
Even if the sample results lead to convincingly rejecting the null hypothesis, assuming the average body temperature for healthy humans to be closer to 36.8 °C rather than 37 °C has a very minimal impact in practice.
More generally, with large sample sizes a small difference might turn out to be statistically significant. 
However, this does not mean that the difference will be of practical importance to decision-makers.
`r msmbstyle::solution_end()`


## Glossary

- *Alternative hypothesis.* A claim about the population parameter for which we seek evidence for. It contradicts the null hypothesis.
- *Statistical test.* A set of principles for measuring the strength of evidence against a null hypothesis about the parameter of interest.
- *Null hypothesis.* A claim about the population parameter typically involving "no effect" or "no difference between groups".
- *Null distribution.* The values of the statistic that we expect to obtain by sampling variation if the null hypothesis is true. In other words, the sampling distribution of the statistic assuming the null hypothesis to be true.
- *P-value.* The probability of obtaining a value of the statistic at least as extreme as the observed statistic when the null hypothesis is true.
- *Significance level.* A threshold used as a criterion for deciding when a p-value is small enough to provide convincing evidence against the null hypothesis. It is typically denoted by $\alpha$ and common values are 0.01 or 0.05.
- *Statistically significant.* Unlikely to occur just by random chance.


## References

- Lock, R. H., Lock, P. F., Morgan, K. L., Lock, E. F., & Lock, D. F. (2013). *Statistics: Unlocking the power of data.* John Wiley & Sons.
- Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., & VanderStoep, J. (2015). *Introduction to statistical investigations.* New York: Wiley.
