[
["sampling-distributions.html", "Chapter 11 Sampling distributions 11.1 Recap 11.2 Population and samples 11.3 Population parameter and sample statistic 11.4 Sampling distribution 11.5 The typical error of a statistic 11.6 The effect of sample size on the sampling distribution 11.7 Summary", " Chapter 11 Sampling distributions Learning outcomes LO1. Understand the difference between a population parameter and a sample statistic. LO2. Understand that a sampling distribution shows how sample statistics vary from sample to sample. LO3. Understand the effect of sample size on the sampling distribution, and how to quantify the variability of a statistic. Reading This week’s reading is Chapter 7. 11.1 Recap Figure 11.1: The data analysis pipeline In Semester 1 (Weeks 1-10), we started walking through the required steps of a statistical investigation, conveniently summarized in Figure 11.1. Suppose that you are interested in a research question that can be answered by collecting data on some statistical units. Once collected, you (a) import/load the data into R; (b) tidy them so that each column corresponds to a single variable of interest; (c) transform variables if needed; (d) visualise your data and inspect for unusual values; (e) fit statistical models to the data; and (f) communicate your results and conclusions to the wider community. Note. The inner cycle (c-d-e) might need to be re-iterated a few times. In the first semester we saw how to import data into R; how to tidy datasets (for example by making sure some variables are factors); how to transform variables (e.g. standardization via scale(), log-transforming data, …). how to visualise the data using the ggplot2 package. In this semester we will focus on modelling, and communicating our findings to the wider community. 11.2 Population and samples Typically, it is either infeasible in terms of time or cost to perform an exhaustive data collection on the entire population of interest (also known as census). To save time and money, we typically record the variables of interest on a smaller subset of the entire population, also known as a sample. In order to make sure that any conclusions we draw from the sample are generalizable to the wider population, this sample needs to be taken at random. This avoids representation bias, where some units are less represented than others, which would lead to wrong conclusions for the entire population. ► Example Suppose you are interested in the average salary of people working in Sweden. Unfortunately, you neither have the time nor the money to go to Sweden and ask each single person his/her own salary. Hence, you decide to ask some people at random. What are the problems of the following sample selection criteria? Asking 500 random people from Facebook that live in Sweden. Asking 1000 random people from the web that live in Sweden. Calling 200 phone numbers from the telephone directory. Asking 500 people working near the central bank of Sweden. ► Solution All four criteria lead to samples that are not representative of the entire population. This is because some people will have higher chance of being included in the sample than others, leading to representation bias. Because of this, the statistic computed from the sample (sample mean) is not generalizable from the whole population. People that do not have Facebook are not considered in the sample. People that do not use internet are not represented. People without a landline phone are not included. The people working around this area will very likely have higher salaries than the rest of the country. This sample does not represent people working in farming or other sectors. In Semester 1, we discussed how to obtain sample data from a bigger population of interest; this is known as data collection. We now take the opposite direction as we try to use the information from the sample data to draw conclusions about the entire population. Furthermore, we will spend some time assessing how accurate our conclusions about the entire population are. Statistical inference Statistical inference is the process of using the sample data to draw conclusions about the entire population. Figure 11.2: Data collection vs statistical inference ► Caution The inferential process is built on top of the assumption that the sample is randomly drawn from the population of interest. Any sample selection method that is biased, i.e. leading to samples which are not representative of the entire population, will mean that the results we obtain from the data in the sample can not be generalized to the entire population. 11.3 Population parameter and sample statistic To make it easier to understand whether we are referring to the entire population or a sample, we use the term parameter when referring to a numerical summary of the entire population, and the term statistic for a numerical summary of the sample. Parameter vs statistic A parameter is a number describing some aspect of the population. A statistic is a number that is computed from the data in a sample. For example, suppose we want to gain more insight into the salaries of Acme Corporation.1 We are interested in multiple aspects of the population, in particular: what is the average salary and how variable are the values around the mean salary? After selecting at random, say, 500 employees from the entire company, we record for each employee the corresponding salary. The population parameters are the mean salary and the standard deviation for the entire employees of Acme Corporation and are not known to us as we did not perform an exhaustive census. The statistics are the mean salary and standard deviation computed from the salaries of the 500 people in the sample. From this example, you can see that the population parameter and the sample statistic generally have the same name. However, these are often written with different symbols to convey with just one letter: what feature they represent; if it is a population quantity or a quantity computed on a sample. The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding “best guesses” computed on a sample. Common parameters and statistics. Population parameter Sample statistic Mean \\(\\mu\\) \\(\\bar{x}\\) or \\(\\hat{\\mu}\\) Standard deviation \\(\\sigma\\) \\(s\\) or \\(\\hat{\\sigma}\\) Proportion \\(p\\) \\(\\hat{p}\\) The greek letter \\(\\mu\\) (mu) is used as a parameter to denote the population mean/average, while \\(\\bar{x}\\) or \\(\\hat{\\mu}\\) (mu-hat) as a statistic for the mean computed on a sample. The greek letter \\(\\sigma\\) (sigma) is used as a parameter to denote the population standard deviation, while \\(s\\) or \\(\\hat{\\sigma}\\) (sigma-hat) as a statistic for the standard deviation of the collected sample. The letter \\(p\\) is used as a parameter to denote the population proportion, while \\(\\hat{p}\\) (p-hat) as a statistic for the sample proportion. ► Example Proportion of UK people aged between 25 and 34 with a Bachelor’s degree or higher The last UK Census, done in 2011, reports that 40% of people aged 25 to 34 years had a degree-level or above qualification. Suppose that in a random sample of \\(n = 200\\) UK residents who are between 25 and 34 years old, 58 of them have a Bachelor’s degree or higher. Using the appropriate notation, state what is the population parameter and what is the sample statistic. ► Solution The population parameter is the proportion of all UK people aged between 25 to 34 years old with a Bachelor’s degree or higher: \\(p = 0.4\\). The sample statistic is the proportion with a Bachelor’s degree or higher for those in the sample: \\(\\hat{p} = 58/200 = 0.29\\). As discussed, it is generally infeasible to be able to know the value of the population parameter exactly. This would require collecting data for the entire population and then computing the required quantity. Instead, we typically select a random sample from the population, and then compute the quantity of interest for the sample data. We then use this sample statistic as a (point) estimate or best guess of the population parameter. 11.4 Sampling distribution A parameter is typically considered to be a fixed value, while a statistic varies from sample to sample, depending on which units are selected to enter the sample. The fact that a sample statistic, i.e. a quantity computed on a sample, varies from sample to sample can be seen as a downside to sampling. However, we must remember that a population parameter (while being fixed) is generally unknown. On the contrary, we can compute the statistic for a sample. A fundamental question that arises when we estimate an unknown population parameter by a sample statistic is: “how accurate do we believe our best guess to be?”. Rememebering that the parameter is fixed, while the statistics varies from sample to sample, we might proceed in answering this question by looking at how the computed statistic varies depending on the sample. ► Example Average yearly salary of the American National Football League (NFL) players We will now read a file containing the yearly salaries (in millions of dollars) for all players being paid, at the start of 2015, by a National Football League (NFL) team. This entire dataset represents the population of all National Football League players in 2015.2 We are interested in the following research question: what is the average salary of a NFL player in 2015? Read in the data and state, with appropriate notation, what is the population parameter. Select a random sample of 50 players and compute the average yearly salary for the players in the sample. How does your statistic compare to the parameter? Take another sample of size \\(n = 50\\) players and compute the average salary for this new sample. How does it compare with the mean for the previous sample? ► Solution Question 1. library(tidyverse) nfl &lt;- read_tsv(&#39;https://edin.ac/2TexAFA&#39;) head(nfl) ## # A tibble: 6 x 5 ## Player Position Team TotalMoney YearlySalary ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Aaron Rodgers QB Packers 110 22 ## 2 Russell Wilson QB Seahawks 87.6 21.9 ## 3 Ben Roethlisberger QB Steelers 87.4 21.8 ## 4 Philip Rivers QB Chargers 83.2 20.8 ## 5 Cam Newton QB Panthers 104. 20.8 ## 6 Matt Ryan QB Falcons 104. 20.8 dim(nfl) ## [1] 2099 5 nfl_avg &lt;- nfl %&gt;% summarise(avg = mean(YearlySalary)) nfl_avg ## # A tibble: 1 x 1 ## avg ## &lt;dbl&gt; ## 1 2.24 The population parameter is the average yearly salary in 2015, which is \\(\\mu =\\) 2.24. Question 2. In order to randomly sample players, we will use a package called moderndive. If you do not have it installed, run the following command: install.packages(&quot;moderndive&quot;) We then load the package and create a sample of size \\(n = 50\\) players: library(moderndive) nfl_sample_1 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_1 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Craig Dahl S Giants 0.87 0.87 ## 2 1 Jarrett Bush CB Packers 5.25 1.75 ## 3 1 Shayne Graham K Saints 0.98 0.98 ## 4 1 Christian Covington 34DT Texans 2.37 0.593 ## 5 1 Pat McAfee P Colts 14.5 2.9 ## 6 1 Ron Parker S Chiefs 25 5 ## 7 1 Bryan Walters WR Jaguars 1.44 0.723 ## 8 1 John Greco RG Browns 8.37 2.09 ## 9 1 Trenton Robinson S Redskins 0.685 0.68 ## 10 1 Paul Worrilow ILB Falcons 1.49 0.496 ## # ... with 40 more rows The command head shows the first six rows. We can see the names of the players which were included in the sample. The extra column at the beginning is equal to 1 for all cases in the sample. This indicates that these 50 rows are all part of our first sample. Let’s now compute the average salary in this sample: nfl_sample_1_avg &lt;- nfl_sample_1 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_1_avg ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.10 We can see that the average salary in our sample is \\(\\bar{x} =\\) 2.1 million dollars. The sample mean, 2.1, is close to the population mean, 2.24, however not exactly the same. We are not surprised of this result: we do not expect the mean of every sample to be exactly equal to the population mean, but we do hope that they are somewhat close. Question 3. Let’s take another random sample of size 50 and compute the sample mean salaries. nfl_sample_2 &lt;- nfl %&gt;% rep_sample_n(size = 50) nfl_sample_2 ## # A tibble: 50 x 6 ## # Groups: replicate [1] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Trevor Scott 43DE Bears 0.73 0.73 ## 2 1 Scott Crichton 43DE Vikings 3.02 0.755 ## 3 1 NaVorro Bowman ILB 49ers 45.2 9.05 ## 4 1 Brian Dixon CB Saints 1.53 0.51 ## 5 1 Kyle Williams WR Broncos 0.745 0.745 ## 6 1 Tony Jefferson S Cardinals 1.50 0.498 ## 7 1 Anthony Hitchens 43OLB Cowboys 2.66 0.664 ## 8 1 Cedric Ogbuehi RT Bengals 9.33 2.33 ## 9 1 Jordan Poyer CB Browns 2.22 0.555 ## 10 1 Brent Celek TE Eagles 29.2 4.88 ## # ... with 40 more rows nfl_sample_2_avg &lt;- nfl_sample_2 %&gt;% summarise(avg = mean(YearlySalary)) nfl_sample_2_avg ## # A tibble: 1 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.05 The statistic computed on the second sample is \\(\\bar{x} =\\) 2.05. Again, this is similar to the population parameter, \\(\\mu =\\) 2.24. We also note that the mean computed on the second sample is different from the mean computed on the first sample. We could also have immediately obtained two samples, each of size \\(n = 50\\) units. This means that we repeat/replicate the activity of sampling 50 units twice. This is done in the function rep_sample_n(size = 50) by including the extra argument reps = 2: nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 2) nfl_samples ## # A tibble: 100 x 6 ## # Groups: replicate [2] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Andrew Luck QB Colts 38.3 7.65 ## 2 1 Antonio Brown WR Steelers 42.0 8.39 ## 3 1 Patrick Chung S Patriots 7.2 2.4 ## 4 1 Chris Williams LG Bills 13.2 3.29 ## 5 1 Chris Lewis-Harris CB Bengals 0.585 0.585 ## 6 1 Lance Louis RG Colts 2.02 1.01 ## 7 1 Adam Humphries WR Buccaneers 1.58 0.525 ## 8 1 Wade Keliikipi DT Eagles 1.58 0.525 ## 9 1 Dustin Colquitt P Chiefs 18.8 3.75 ## 10 1 Tyronne Green G Panthers 0.745 0.745 ## # ... with 90 more rows If you explore this tibble, it has \\(50 \\times 2 = 100\\) rows. The colum replicate takes value 1 for the first 50 rows, and the value 2 for the next 50 rows. This indicates that the players in rows 1 to 50 are selected to be in the first sample, while the players in rows 51 to 100 are those selected to be in the second sample. We can now compute the mean yearly salary for each of the two samples: nfl_avgs &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_avgs ## # A tibble: 2 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.57 ## 2 2 2.19 We see that both are close to the population parameter which, we remind, is equal to 2.24. Clearly, we can now extend this repeated sampling procedure to more than just two samples of size = 50. Let us try to obtain 1000 repeated samples all of size 50 from the sample population. nfl_samples &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 1000) nfl_samples ## # A tibble: 50,000 x 6 ## # Groups: replicate [1,000] ## replicate Player Position Team TotalMoney YearlySalary ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Matthew Masifilo 43DE Buccaneers 1.11 0.555 ## 2 1 Kyle Juszczyk FB Ravens 2.46 0.615 ## 3 1 Gabe Jackson LG Raiders 2.86 0.714 ## 4 1 Akeem Ayers 43OLB Rams 6 3 ## 5 1 Andy Mulumba LB Packers 1.49 0.497 ## 6 1 Corey Linsley C Packers 2.40 0.601 ## 7 1 Billy Turner T Dolphins 3.15 0.787 ## 8 1 Bennett Jackson CB Giants 0.435 0.435 ## 9 1 Chase Daniel QB Chiefs 10 3.33 ## 10 1 Scott Lutrus LB Colts 1.40 0.465 ## # ... with 49,990 more rows This tibble has \\(50 \\times 2000 = 100,000\\) rows. The first 50 players are part of the 1st sample, the next 50 belong to the 2nd sample, and so on… We can now compute the mean of each of the 2000 samples, obtaining a tibble of 2000 sample means: nfl_avgs &lt;- nfl_samples %&gt;% group_by(replicate) %&gt;% summarise(avg = mean(YearlySalary)) nfl_avgs ## # A tibble: 1,000 x 2 ## replicate avg ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2.74 ## 2 2 1.69 ## 3 3 2.44 ## 4 4 2.29 ## 5 5 2.37 ## 6 6 2.63 ## 7 7 2.46 ## 8 8 1.99 ## 9 9 2.29 ## 10 10 2.28 ## # ... with 990 more rows Let us plot the distribution of the sample mean for 2000 random samples: ggplot(nfl_avgs, aes(x = avg)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_avg, avg), color = &quot;red&quot;, size = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 11.3: Sampling distribution of the mean Figure 11.3 shows the values of the sample mean computed from sample to sample. Hence, it shows the variability of the sample mean induced by sampling variation. This plot is fundamental in statistical inference, and it is called sampling distribution. Each of these dots is one of the values in the column avg of the tibble nfl_avgs. Sampling distribution The sampling distribution shows the distribution of the statistic for different samples of the same size from the same population. Clearly, we can compute sampling distributions for other statistics: the proportion, the standard deviation, … All this requires are the following steps: Obtaining multiple samples, all of the same size, from the same population For each sample, calculate the value of the statistic Plot the distribution of the computed statistics 11.5 The typical error of a statistic The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample. This is key in understanding how accurate our estimate (best guess) of the population parameter, based on just one sample, will be. You already know how to compute the variability, using the standard deviation. So, the variability of the statistic can be quantified by calculating the standard deviation of its sampling distribution. This is not different from an ordinary standard deviation in terms of code and formulas, but the importance of this quantity led to giving it its own name: the standard error of the statistic. In other words, we use: standard deviation to denote the variability among the values in a particular sample standard error to denote the variability of the statistics computed on many samples Standard error The standard error of a statistic is the standard deviation of its sampling distribution. It is denoted \\(SE\\) and measures the typical error when estimating the population parameter with the sample statistic. You can think of the SE as a typical distance from the population parameter. 11.6 The effect of sample size on the sampling distribution It is of interest to see how the sampling distribution of the mean salary, shown in Figure 11.3 for a sample size \\(n = 50\\), changes with the sample size. In the following code chunk we compute the sampling distribution of the mean for samples of size \\(n = 50,\\ n = 200,\\ n = 1000\\). means_n_50 &lt;- nfl %&gt;% rep_sample_n(size = 50, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(sample_mean = mean(YearlySalary)) means_n_100 &lt;- nfl %&gt;% rep_sample_n(size = 100, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(sample_mean = mean(YearlySalary)) means_n_500 &lt;- nfl %&gt;% rep_sample_n(size = 500, reps = 1000) %&gt;% group_by(replicate) %&gt;% summarise(sample_mean = mean(YearlySalary)) We now combine the datasets for different sample sizes into a unique tibble, adding a column with the sample size, and then plot the distributions for different sample sizes: means_vary_n &lt;- bind_rows( means_n_50 %&gt;% mutate(n = 50), means_n_100 %&gt;% mutate(n = 100), means_n_500 %&gt;% mutate(n = 500) ) ggplot(means_vary_n, aes(x = sample_mean)) + geom_histogram(color = &quot;white&quot;) + geom_vline(xintercept = pull(nfl_avg, avg), color = &quot;red&quot;, size = 1) + facet_grid(cols = vars(n), labeller = label_both) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 11.4: The effect of sample size on the sampling distribution Figure 11.4 shows that as the sample size increases, the variability of the sampling distributions decreases, hence the standard error of the statistic decreases as the sample size increases. We can create a tibble that shows, for each sample size, the standard error of the sample mean: means_vary_n %&gt;% group_by(n) %&gt;% summarise(SE = sd(sample_mean)) ## # A tibble: 3 x 2 ## n SE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 50 0.414 ## 2 100 0.298 ## 3 500 0.123 As the sample size (\\(n\\)) increases, the standard error (\\(SE\\)) decreases. The larger the sample size, the lower the typical error of our best guess, and for every sample we will obtain a calculated statistic that is more similar to the population parameter (lower distance between the sample statistic and the parameter). Recall that for the NFL example, the population mean was 2.24. 11.7 Summary We revised LO1 in Section 11.2 You might remember it from the cartoon Wile E. Coyote and the Road Runner.↩ Of course a population might change over time, and so you might wonder why did we say that a population parameter is fixed. Because of the large number of units in the entire population, it is reasonable to assume that the addition of comparatively few units to the entirety leads to a negligible change in the population parameter.↩ "]
]
