```{r, echo=FALSE}
HIDDEN_SOLS=FALSE
set.seed(15732)
ggplot2::theme_set(ggplot2::theme_gray(base_size=13))

library(tidyverse)
library(moderndive)
```

# Type I and Type II Errors, Power! {#chap-typeerror}


<div class="lo">
#### Instructions {-}
  
- In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour.
- The Rmarkdown file for this week is [here](https://uoe-psychology.github.io/uoe_psystats/dapr1/labsheets/week_14_practice.Rmd).


#### Learning outcomes {-}

**LO1.** Recap hypothesis testing 

**LO2.** Understand the different types of errors 

**LO3.** Introduce statistical power  

#### Reading {-}

?????????????????????

</div>


## Recap


1. random chance
1. alternative explanation
1. statistical hypothesis HoHa!
1. from random chance model, we constructed a null distribution.(lab involved bootstrapping the null)
1. compare observed statistic to null distribution
1. retrieved p-value (proportion of null distribution at least as extreme as observed)
1. made a formal decision.


## Walkthrough

### Example 1: Coin toss {-}    

<div style="background-image: url('flipping.jpg')">
#### Research Question & Hypotheses {-}

Is our coin biased? 
<br><br>
*Null hypothesis*: We're just as likely to get heads as tails when we flip the coin.
$$H_0: p_{heads} = 0.5$$
*Alternative hypothesis*: We're more likely to see either heads or tails when we flip the coin.
$$H_1: p_{heads} \neq 0.5$$

#### Data collection {-} 

We toss the coin 90 times, and it lands on heads 54 times. 
</div>

#### Analysis {-}  

<div class="red">
<br>
**Steps**
<br> 

1. Calculate our statistic  
1. Generate the null distribution  
1. Calcualte the probability of seeing our statistic (or one which is more extremely different the null)if the null were true (the p-value)  

</div>  

1. Calculate our statistic, $\hat{p}$  
```{r}
p_hat = 55/90
p_hat
```


2. Generate the null distribution  

Is our 54 of 90 coin tosses that surprising?<br>
Let's compare it against the null distribution.<br>
Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size $n$ would vary if the null is true. In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2.

```{r warning=FALSE, message=FALSE}
# Specify our possible outcomes and their probabilities under the null
outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(1/2, 1/2)

# generate samples under the null
samples <- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob)

# calculate the statistics for each sample to create the null distribution
null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())

# plot the null distribution  
ggplot(null_distribution, aes(x = prop)) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = 'white', stackratio = 0.5) +
  labs(x = expr(hat(p)))
```

add the plot observed

```{r}
ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.61))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  geom_vline(xintercept = 0.61, color = 'tomato1', size = 1) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= 0.61)) +
  theme(legend.position="top")

```

3. Calculate our p-value

`r msmbstyle::question_begin()`
What is our p-value?   

1. The proportion of the null distribution which is $\geq0.61$
1. The proportion of the null distribution which is $\geq0.61$ or $\leq0.39$  
1. 2 times the proportion of the null distribution which is $\geq0.61$ or $\leq0.39$ (whichever is smallest)  

`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
if we perform an hypothesis test for the two-sided alternative $p \neq ...$, we compute it as twice the proportion in the smallest tail, i.e. the tail with the smallest count.
```{r}
pvalue <- null_distribution %>%
  summarise(
    pvalue_lefttail = sum(prop <= 0.39) / n(),
    pvalue_righttail = sum(prop >= 0.61) / n()
  )

pvalue

0.018 * 2
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin()`
What is our formal decision about our coin? 

1. Not enough evidence to reject $H_0$ ($H_0:$ We're just as likely to get heads as tails when we flip the coin.)
1. Evidence in favour of $H_1$ ($H_1:$ We're more likely to see either heads or tails when we flip the coin.)

`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
According to our p-value, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. 
`r msmbstyle::solution_end()`


BUT have we made an mistake here? 

is 55 out of 90 coin tosses *that* surprising? 

convicting an innocent person

option... decrease alpha!
div think - what are we doing here? we making the probability of convicting an innocent person much less likely 
casually - we've got to get some *really strong evidence* that they're guilty. 

### Example 2: ??

on the other hand...
research Q: is global temperature rising? 
H_o, temperature in y1 == temperature y2
H_a temperature in y1 != temperature y2
do a test, 
get a p-value <.01
with our new alpha of .001, we don't reject H_o. 
But maybe we're being to strict! 
Now we are letting a guilty person walk free!


### Two different types of errors

|                          | $H_0$ is True          | $H_0$ is False           |
|:-------------------------|:----------------------:|:------------------------:|
|Test doesn't reject $H_0$ | Correct                | Type II Error            |
|Test rejects $H_0$        | Type I Error           | Correct                  |



|                      | Person is innocent     | Person is guilty         |
|:---------------------|:----------------------:|:------------------------:|
|Verdict = Innocent | Correct                   | Let's criminal walk      |
|Verdit =  Guilty   | Wrongful conviction       | Correct                  |



|                          | $H_0$ is True          | $H_0$ is False             |
|:-------------------------|:----------------------:|:--------------------------:|
|Test doesn't reject $H_0$ | Correct                | Type II Error<br>($\beta$) |
|Test rejects $H_0$        | Type I Error<br>($\alpha$)| Correct                 |

#### Type I error.  

*if* the null hypthosis is true, then the sampling variability of our statistic is our null distribution (plotted above), and we will reject (incorrectly) any sample statistic which results in a p-value which is $\leq$ our $\alpha$ level (e.g., the 0.05 we set earlier). 

So the probability of making this error is equal the $alpha$ level we set. 


<div class="red"> 
multiple tests!

the probability of making a type I error is the probability of getting an extreme sample statistic simply due to chance sampling variation. (i.e., we just happened to get a random sample with an extreme statistic).

*thought experiment*
there are 20 researchers. 
each has a perfectly balanced/fair coin.  
each one conducts a statistical test at $\alpha = 0.05$ to evaluate whether their coin is fair.
how many of the researchers' tests would we expect to result in a type I error?




</div>


Q what are our ideal values for alpha nad beta? 
A 0 and 0. 
BUT, this isn't possible when we sample



## 

### In-class activity





---

## Lab



