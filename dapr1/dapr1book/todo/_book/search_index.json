<<<<<<< Updated upstream
[
["chap-typeerror.html", "Chapter 1 Type I and Type II Errors, Power! 1.1 Overview of last week 1.2 Walkthrough 1.3 Summary 1.4 Lab", " Chapter 1 Type I and Type II Errors, Power! Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Recap hypothesis testing LO2. Understand the different types of errors LO3. Introduce statistical power Reading ????????????????????? 1.1 Overview of last week Last week we learned to frame a research question in terms of null and alternative hypotheses about parameters. We likened the null hypothesis (\\(H_0\\)) to a ‘random chance model’ - i.e., if differences or effects that we observe are actually just due to sampling variation. We generated a distribution for the null hypothesis (the null distribution) which reflected how much sample statistics would vary due to chance if the null hypothesis is true (e.g., if there really is no difference/effect). We did this by simulating lots of samples and computing the statistic on each of the samples. We then compared the observed statistic to the null distribution we had generated. This is equivalent to asking how likely it would be to get our observed statistic if the null hypothesis was actually true. We learned that the p-value is the proportion of simulated sample statistics in our null distribution which were as or more extreme than our observed statistic. Finally, we thought about how we might make a formal decision about whether or not to reject the null hypothesis based on our p-value. This week, we will recap this process of hypothesis testing, before thinking about the ways in which this method might lead to error. 1.2 Walkthrough Example 1: Coin flip Research Question &amp; Hypotheses Is our coin biased? Null hypothesis: We’re just as likely to get heads as tails when we flip the coin. \\[H_0: p = 0.5\\] Alternative hypothesis: We’re more likely to see either heads or tails when we flip the coin. \\[H_1: p \\neq 0.5\\] Data collection We flip the coin 90 times, and it lands on heads 55 times. Analysis Steps Calculate our statistic Generate the null distribution Calculate the probability of seeing our statistic (or one which is farther away from the null) if the null were true (this is the p-value) 1. Calculate our statistic, \\(\\hat{p}\\) p_hat &lt;- 55/90 p_hat ## [1] 0.6111111 2. Generate the null distribution Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size \\(n\\) would vary if the null is true. In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2. # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) and we can now plot our null distribution: ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + labs(x = expr(hat(p))) And plot the observed statistic on top, like we did last week. ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= 0.61))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + geom_vline(xintercept = 0.61, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= 0.61)) 3. Calculate our p-value How surprising is 55 of 90 coin flips? We can compare it against the null distribution. ► Question What is our p-value? The proportion of the null distribution which is \\(\\geq0.61\\) The proportion of the null distribution which is \\(\\geq0.61\\) or \\(\\leq0.39\\) 2 times the proportion of the null distribution which is \\(\\geq0.61\\) ► Solution If we perform an hypothesis test for the two-sided alternative \\(p \\neq 0.5\\), we compute it as twice the proportion in the smallest tail. pvalue &lt;- null_distribution %&gt;% summarise( pvalue_lefttail = sum(prop &lt;= 0.39 | prop&gt;=0.61) / n(), ) ► Question What is our formal decision about our hypotheses? Not enough evidence to reject \\(H_0\\)(\\(H_0:\\) We’re just as likely to get heads as tails when we flip the coin.) Evidence in favour of \\(H_1\\)(\\(H_1:\\) We’re more likely to see either heads or tails when we flip the coin.) ► Solution According to our p-value, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. An analogy Might we have made a mistake here? Is 55 out of 90 coin flips surprising enough for us to reject the hypothesis that the coin is fair? Last week we discussed an analogy in law, in which a person on trial is presumed innocent until proven guilty. Similarly, we presume \\(H_0\\) to be true until there is strong evidence to reject it. How strong must the evidence be? How do we avoid wrongly convicting an innocent person? (i.e., wrongly rejecting an hypothesis which is actually true?) Two different types of errors Person is innocent Person is guilty Verdict = Innocent Correct decision Criminal goes free Verdict = Guilty Wrongful conviction Correct decision \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct decision Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct decision Type I errors If the null hypothesis is true, then the sampling distribution of our statistic follows the null distribution which we constructed above, and we will reject (incorrectly) any sample statistic which results in a p-value which is less than or equal to our \\(\\alpha\\) level (e.g., the 0.05 we set earlier). So the probability of making this error is equal to the \\(alpha\\) level which we set. In other words, when the null hypothesis is true, 0.05 (or 5%) of the random samples we could take would result in us rejecting it. A thought experiment there are 20 researchers. each researcher has a perfectly balanced/fair coin. each researcher conducts a statistical test at \\(\\alpha = 0.05\\) to evaluate whether their coin is fair (lands on heads equally as often as it lands on tails). ► Question How many of the researchers’ tests would we expect to result in a type I error? Remember: The probability of making a Type I error is the probability of getting an unlikely sample statistic simply due to chance sampling variation (i.e., we just happen to get a random sample with an unlikely statistic). ► Solution This is similar to asking “What is the probability of observing at least one significant result due to chance sampling variation alone?”. We can work this out.. For one researcher, if their null hypothesis is true, the probability that they get a significant result is 0.05,and the probability that they get a non-significant result (p-value \\(&gt; 0.05\\)) is 0.95. If there are 20 researchers, the probability of them all getting non-significant results when their null hypotheses are all true is \\(0.95^{20} = 0.358\\) This means that the robability of the opposite - at least one of them gets a significant result even though all their nulls are true - is \\(1 - 0.358 = 0.642\\) Type II errors and Power The other kind of error we might make is a type II error, and is denoted by \\(\\beta\\). This happens when \\(H_0\\) is false, but we do not have enough evidence to reject it. In our table, the columns specify the possible states of the world (\\(H_0\\) is either True or False). In each of the possible states of the world, there are two potential outcomes of conducting a statistical test (Reject \\(H-0\\) or Don’t reject \\(H_0\\)). We have seen that: + if \\(H_0\\) is true, then the probability of incorrectly rejecting \\(H_0\\) is \\(\\alpha\\) (often set at 0.05), and the probability of correctly retaining (not rejecting) \\(H_0\\) is 0.95. + if \\(H_0\\) is false, then the probability of incorrectly failing to reject \\(H_0\\) is \\(\\beta\\), and the probability of correctly rejecting \\(H_0\\) is \\(1-\\beta\\). This is known as the statistical power of our test. \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct1-\\(\\alpha\\) Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct1-\\(\\beta\\)Power Statistical power Statistical power is the probability of a hypothesis test of finding an effect if there is an effect to be found. Example 2: A biased coin I have a trick coin which is weighted so that it lands on heads 60% of the time (rather than the usual 50% for a normal fair coin). Oh no! Tom has noticed that whenever we flip a coin, I always call heads, and I often seem. He accuses me of cheating by using a trick coin which is biased to land on! I make him an offer: he can flip the coin 50 times in order to decide whether or not it really is a trick coin. ► Question In Tom’s 50 coin flips, what proportion of heads would lead him to the correct conclusion that the coin is a trick coin? ► Solution # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.5, 0.5) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) Now that we have generated the null distribution which Tom will use to test his observed statistic, we need to work out at what values he will reject the null. In other words, we need to work out where the top 5% of the null distribution is? Note that we are only looking at the top 5% because Tom thinks the coin is biased towards heads. So his alternative hypothesis (\\(H_A\\)) is \\(p &gt; 0.5\\), and he will reject \\(H_0\\) if his observed statistic falls in the top end of the distribution. null_distribution &lt;- null_distribution %&gt;% arrange(prop) %&gt;% mutate(idx = 1:n()) # plot the null distribution # ggplot(null_distribution, aes(x = prop, fill = (idx &gt;= 975 | idx &lt;= 25))) + # geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + # scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + # labs(x = expr(hat(p))) null_distribution %&gt;% filter(idx == 950) ## # A tibble: 1 x 3 ## replicate prop idx ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 92 0.62 950 If Tom flips the coin 50 times, under the null hypothesis, he would need to get a sample statistic (\\(\\hat{p}\\)) of greater than or equal to 0.62. ► Question Things we know so far: The coin is rigged to land on heads on 60% of flips - the true probability of heads is 0.6. If 62% or more of Tom’s 50 coin flips come up heads, then he will reject his null hypothesis (that the coin is fair) What’s the probability that Tom’s 50 flips will come up with 62% or more heads? In other words, what is the power of his test? ► Solution We can do this by generating the sampling distribution for when the coin is biased towards heads 60% of the time (which we know is actually true). outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.6, 0.4) samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the true distribution true_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) true_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= 0.62) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.463 Generalising it When we conduct NHST, we set \\(\\alpha\\). In setting \\(\\alpha\\), we define a critical region under the null distribution. The critical value is the value of the statistic which defines the start of this region. Any statistic more extreme than this will result in rejecting the null hypothesis. If the null hypothesis is false, the probability that we reject the null hypothesis depends on a) how far away from the null hypothesis the true state of the world is, and b) our sample size. In our example, we knew the true bias of the coin. But Tom didn’t! What Tom could do, is calculate the power of his test assuming a given value for \\(p\\). He might have thought to himself “hmm, that coin seems to land on heads about 3/4 of the time. I want to know the probability of me being able to correctly reject the null hypothesis is, if the coin is actually biased that much”. The important thing is that, given an assumed effect size (i.e., difference from the null hypothesis), we can compute the power of a test based on our given sample size. This corresponds to the probability of getting a statistic more extreme than the critical value, given a theorised effect size. Had Tom used the code above he might have called his distribution theoretical_distribution rather than true_distribution! Overlapping distributions Talk through applet https://istats.shinyapps.io/power/ https://rpsychologist.com/d3/NHST/ 1.3 Summary 1.4 Lab 1.4.1 Exercise 1: Calculating power for a different coin. If my trick coin was actually weighted so that it landed on heads 75% of the time, what would the power of Tom’s test (50 flips) be? ► Question ► Solution 1.4.2 Exercise 2: Calculating sample size (number of flips) for a desired level of power For a given effect size (i.e., assumed difference from the null hypothesis), we can compute the power of a test based on a given sample size. So, in testing a biased coin, we can calculate the power of a test for different possible biases of coin (e.g., assuming the coin to be biased towards heads 55%, 60%, 75% of the time). Importantly, we can also decide on what we want the power of a test to be, and ask what the minimum sample size is that is needed to detect a given effect size. Q2 - Assuming the coin to be biased towards landing on heads 75% of the time, how many coin flips should Tom do in order to increase his power to 80%? ► Question ► Solution "]
]
=======
[
["index.html", "Data Analysis for Psychology in R (dapR1) - Labs Overview of the Course The team R Cheatsheets R Community", " Data Analysis for Psychology in R (dapR1) - Labs Department of Psychology, University of Edinburgh 2019-2020 Overview of the Course Data Analysis for Psychology in R 1 (dapR1) is your first step on the road to being a data, programming and applied statistics guru! This course provides a introduction to data, R and statistics. It is designed to work slowly through conceptual content that form the basis of understanding and working with data to perform statistical testing. At the same time, we will be introducing you to basic programming in R, covering the fundamentals of working with data, visualization and simple statistical tests. The overall aim of the course is to provide you with all the necessary skills to feel confident working with R and data, before we move on to discuss a broader array of statistical methods in year 2. On this page you will find the notes for your weekly labs including practice exercises and solutions. You can also read a gentle introduction to R and installation guide. As you progress through the course, the content will build up. Find the general overview of topics below so you know where we are heading: SEMESTER 1 Week Lecture Lab topic 1 Introduction lecture: research process, planning and design What is R and installation 2 Measurement: types of data RStudio environment, libraries, packages, scrips and notebooks. Introduction to Rmarkdown 3 Organising data: data sets, tables, plots Assignment: vectors, lists, data frames, data types 4 Describing data: central tendency Introduction to plots and geoms 5 Describing data: variability Central tendency &amp; variability Break 6 Functions &amp; data transformation Hand plotting functions. Introduction to data transformation 7 Statistical models, chance and probability R practice: read in data, merge 8 Fundamentals of probability Introduction to probability. Sampling in R 9 Probability &amp; probability distributions Introduction to probability distributions. Revision 10 Probability distributions: Binomial &amp; Normal Lab test 1 SEMESTER 2 Week Topic 11 Sampling 12 Bootstrapping and confidence intervals 13 Hypothesis testing 14 Normal distribution and probability 15 Revision Break 16 Test for one mean 17 Test for two means (independent samples) 18 Test for two means (paired samples) 19 Chi-squared 20 Covariance &amp; correlation The team Dr Tom Booth Tom.Booth@ed.ac.uk Lecturer and Course Organiser Dr Umberto Noe Umberto.Noe@ed.ac.uk Senior Teching Coordinator (Labs) Dr Josiah King Josiah.King@ed.ac.uk Senior Teching Coordinator (Labs) Ms Emma Waterston Emma.Waterston@ed.ac.uk PhD student in Psychology (Labs) (with special thanks to Dr Anastasia Ushakova) R Cheatsheets You can find a collection of cheatshets that summarise some of the most useful commands you will be using for tasks such as data transformation, visualisation, RMarkdown and many others here/ The key ones you will need to get for this year are: RMarkdown Data Visualisation (ggplot2) Data transformation with dplyr Data Import R Community R has great representation in Edinburgh. Check out these pages to find out more: R Ladies Edinburgh EdinburghR And worldwide you have: R Studio Community ## Warning: package &#39;moderndive&#39; was built under R version 3.6.2 "],
["chap-typeerror.html", "Chapter 1 Type I and Type II Errors, Power! 1.1 Recap 1.2 Walkthrough 1.3 Lab", " Chapter 1 Type I and Type II Errors, Power! Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Recap hypothesis testing LO2. Understand the different types of errors LO3. Introduce statistical power Reading ????????????????????? 1.1 Recap random chance alternative explanation statistical hypothesis HoHa! from random chance model, we constructed a null distribution.(lab involved bootstrapping the null) compare observed statistic to null distribution retrieved p-value (proportion of null distribution at least as extreme as observed) made a formal decision. 1.2 Walkthrough Example 1: Coin flip Research Question &amp; Hypotheses Is our coin biased? Null hypothesis: We’re just as likely to get heads as tails when we flip the coin. \\[H_0: p_{heads} = 0.5\\] Alternative hypothesis: We’re more likely to see either heads or tails when we flip the coin. \\[H_1: p_{heads} \\neq 0.5\\] Data collection We flip the coin 90 times, and it lands on heads 54 times. Analysis Steps Calculate our statistic Generate the null distribution Calculate the probability of seeing our statistic (or one which is more extremely different the null) if the null were true (the p-value) Calculate our statistic, \\(\\hat{p}\\) p_hat &lt;- 55/90 p_hat ## [1] 0.6111111 Generate the null distribution Is our 54 of 90 coin flips that surprising? Let’s compare it against the null distribution. Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size \\(n\\) would vary if the null is true. In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2. # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) # plot the null distribution ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + labs(x = expr(hat(p))) And plot the observed statistic on top, like we did last week. ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= 0.61))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + geom_vline(xintercept = 0.61, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= 0.61)) Calculate our p-value ► Question What is our p-value? The proportion of the null distribution which is \\(\\geq0.61\\) The proportion of the null distribution which is \\(\\geq0.61\\) or \\(\\leq0.39\\) 2 times the proportion of the null distribution which is \\(\\geq0.61\\) or \\(\\leq0.39\\) (whichever is smallest) ► Solution if we perform an hypothesis test for the two-sided alternative \\(p \\neq ...\\), we compute it as twice the proportion in the smallest tail, i.e. the tail with the smallest count. pvalue &lt;- null_distribution %&gt;% summarise( pvalue_lefttail = sum(prop &lt;= 0.39) / n(), pvalue_righttail = sum(prop &gt;= 0.61) / n() ) pvalue ## # A tibble: 1 x 2 ## pvalue_lefttail pvalue_righttail ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.018 0.022 0.018 * 2 ## [1] 0.036 ► Question What is our formal decision about our hypotheses? Not enough evidence to reject \\(H_0\\) (\\(H_0:\\) We’re just as likely to get heads as tails when we flip the coin.) Evidence in favour of \\(H_1\\) (\\(H_1:\\) We’re more likely to see either heads or tails when we flip the coin.) ► Solution According to our p-value, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. An analogy Have we made an mistake here? Is 55 out of 90 coin flips surprising enough? Last week we discussed an analogy in law, in which a person on trial is presumed innocent until proven guilty. Similarly, we presume \\(H_0\\) to be true until there is strong evidence to reject it. How strong must the evidence be? Have we just wrongly convicted an innocent person? (i.e., wrongly rejected an hypothesis which is actually true?) Two different types of errors Person is innocent Person is guilty Verdict = Innocent Correct decision Criminal goes free Verdict = Guilty Wrongful conviction Correct decision \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct Type I errors if the null hypothesis is true, then the sampling distribution of our statistic is the null distribution (plotted above), and we will reject (incorrectly) any sample statistic which results in a p-value which is less than or equal to our \\(\\alpha\\) level (e.g., the 0.05 we set earlier). So the probability of making this error is equal to the \\(alpha\\) level which we set. A thought experiment there are 20 researchers. each researcher has a perfectly balanced/fair coin. each researcher conducts a statistical test at \\(\\alpha = 0.05\\) to evaluate whether their coin is fair (lands on heads equally as often as it lands on tails). ► Question How many of the researchers’ tests would we expect to result in a type I error? Remember: The probability of making a Type I error is the probability of getting an unlikely sample statistic simply due to chance sampling variation (i.e., we just happen to get a random sample with an unlikely statistic). ► Solution Type II errors and Power A type II error is denoted by \\(\\beta\\). This happens when \\(H_0\\) is false, but we do not have enough evidence to reject it. In our table, the columns specify the possible states of the world (\\(H_0\\) is either True or False). In each of the possible states of the world, there are two potential outcomes of conducting a statistical test (Reject \\(H-0\\) or Don’t reject \\(H_0\\)). We have seen that if \\(H_0\\) is true, then the probability of incorrectly rejecting \\(H_0\\) is \\(\\alpha\\). We have seen that if \\(H_0\\) is false, then the probability of incorrectly failing to reject \\(H_0\\) is \\(\\beta\\). Conversely, the probability of correctly rejecting \\(H_0\\) is \\(1-\\beta\\). This is also known as the statistical power of our test. \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct1-\\(\\alpha\\) Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct1-\\(\\beta\\)Power Statistical power Statistical power is the probability of a hypothesis test of finding an effect if there is an effect to be found. Example 2: A biased coin I have a trick coin which is weighted so that it lands on heads 60% of the time (rather than the usual 50% for everyday fair coins). Oh no! Umberto has noticed that whenever we flip a coin I call heads, and I often seem to win. He accuses me of cheating by using a trick coin! I make him an offer: he can flip the coin 50 times in order to decide whether or not it really is a trick coin. ► Question In Umberto’s 50 coin flips, what proportion of heads would lead him to the correct conclusion that the coin is a trick coin? ► Solution If Umberto flips the coin 50 times, under the null hypothesis, he would need to get a sample statistic (\\(\\hat{p}\\)) of .64 or greater # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) null_distribution &lt;- null_distribution %&gt;% arrange(prop) %&gt;% mutate(idx = 1:n()) # plot the null distribution ggplot(null_distribution, aes(x = prop, fill = (idx &gt;= 950))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + labs(x = expr(hat(p))) null_distribution %&gt;% filter(idx == 950) ## # A tibble: 1 x 3 ## replicate prop idx ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 92 0.62 950 Things we know so far: The coin is rigged to land on heads on 60% of flips - the true probability of heads is 0.6. If 62% or more of Umberto’s 50 coin flips come up heads, then he will reject his null hypothesis (that the coin is fair) ► Question What’s the probability that Umberto’s 50 flips will come up with 64% or more heads? In other words, what is the power of his test? ► Solution We can actually generate the true sampling distribution because we know the true parameter (i.e., we know that the coin lands on heads 60% of the time). outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.6, 0.4) samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution alternative_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) alternative_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= 0.62) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.463 Generalising it When we conduct NHST, we set \\(\\alpha\\). In setting \\(\\alpha\\), we define a critical region under the null distribution. The critical value is the value of the statistic which defines the start of this region. Any statistic more extreme than this will result in rejecting the null hypothesis. If the null hypothesis is false, the probability that we reject the null hypothesis depends on a) how far away from the null hypothesis the true state of the world is, and b) our sample size. For a given effect size (i.e., difference from the null hypothesis), we can compute the power of a test based on a given sample size. This corresponds to the probability of getting a statistic more extreme than the critical value, given a theorised effect size (i.e., a theorised alternative distribution) Overlapping distributions Talk through applet https://istats.shinyapps.io/power/ https://rpsychologist.com/d3/NHST/ 1.2.1 In-class activity 1.3 Lab Q1 - if my trick coin was actually weighted so that it landed on heads 75% of the time, what would the power of Umberto’s test be? For a given effect size (i.e., difference from the null hypothesis), we can compute the power of a test based on a given sample size. However…. we can also decide on what we want the power of a test to be, and ask what the minimum sample size is that is needed to detect a given effect size. Q2.- how many coin flips should Umberto do in order to increase his power to 80%? A haven’t thought about this yet :) "],
["chap-normal.html", "Chapter 2 Normal distributions &amp; probability 2.1 Recap 2.2 Walkthrough 2.3 Lab", " Chapter 2 Normal distributions &amp; probability Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. LO2. Reading 2.1 Recap 2.2 Walkthrough how many times do you look at your phhone in a day? a lot of distributions we have seen have looked like this. this is not a coincidence. under general circumstancesm distribution of many statistics will follow this pattern. we have talked about the properties of these shapes, and described them as symettric and bellshaped. an example tibble(x=rnorm(1000,0,1)) %&gt;% ggplot(aes(x=x))+ geom_histogram(alpha=.6)+ theme_light() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 2.2.1 density curves theoretical model for a distribution is a density curve tibble(x=rnorm(1000,0,1)) %&gt;% ggplot(aes(x=x))+ geom_histogram(aes(y=..density..),alpha=.6)+ geom_density()+ theme_light()+ scale_y_continuous(NULL, breaks=NULL) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. calculating from a known distribution: the proportion of samples which have a mean of less than 60. hint (we know that there are 1000 means… ) We can count how many samples have a mean of &lt;60. We can sum up the counts in all the bars below 60. 3+4+10+26+50+68 = 161 161/1000 = 0.161, so 16% of the means are &lt;60. estimating about a theoretical distribution proportion of … which are: less than 100 between x and y 2.2.2 normal but different density curves can take any shape, but the normal density curve has a special form which looks bellshaped. take a look back at the previous two curves.. they look pretty similar, but that’s just because of the axes of the graph changing.. ggplot()+ geom_line(data=d, aes(x=dx,y=dy),lwd=1)+ geom_line(data=d1, aes(x=dx,y=dy),lwd=1)+ theme_light() 2.2.3 parameters applet? \\[ N(\\mu, \\sigma) \\] percentiles probabilities of normal curves 2.2.4 standard normal transforming \\(N(\\mu,\\sigma)\\) to \\(N(0,1)\\) steps + minus the mean from each value + divide by the standard deviation the values are now.. 2.2.5 z-scores \\[ z = \\frac{x-\\mu}{\\sigma} \\] we can use z-scores to identify which scores are more unusual than others. people walk 10000 steps a day. SD of 3000. So this means that 95% of people walk between 4100 and 15900 steps per day. people drink 2500ml of water a day, SD of 500. 95% of people drink between 1520ml and 3480ml of water a day is it more unusual for someone to walk only 6000 steps a day, or for someone to drink only 1700ml of water a day? z1 = (6000-10000)/3000 z2 = (1700-2500)/500 z1;z2 ## [1] -1.333333 ## [1] -1.6 2.2.6 In-class activity 2.3 Lab "]
]
>>>>>>> Stashed changes
