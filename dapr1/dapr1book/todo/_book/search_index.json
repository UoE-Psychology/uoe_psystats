[
["chap-typeerror.html", "Chapter 1 Type I and Type II Errors, Power! 1.1 Recap 1.2 Walkthrough 1.3 Lab", " Chapter 1 Type I and Type II Errors, Power! Instructions In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour. The Rmarkdown file for this week is here. Learning outcomes LO1. Recap hypothesis testing LO2. Understand the different types of errors LO3. Introduce statistical power Reading ????????????????????? 1.1 Recap random chance alternative explanation statistical hypothesis HoHa! from random chance model, we constructed a null distribution.(lab involved bootstrapping the null) compare observed statistic to null distribution retrieved p-value (proportion of null distribution at least as extreme as observed) made a formal decision. 1.2 Walkthrough Example 1: Coin flip Research Question &amp; Hypotheses Is our coin biased? Null hypothesis: We’re just as likely to get heads as tails when we flip the coin. \\[H_0: p = 0.5\\] Alternative hypothesis: We’re more likely to see either heads or tails when we flip the coin. \\[H_1: p \\neq 0.5\\] Data collection We flip the coin 90 times, and it lands on heads 55 times. Analysis Steps Calculate our statistic Generate the null distribution Calculate the probability of seeing our statistic (or one which is farther away from the null) if the null were true (this is the p-value) 1. Calculate our statistic, \\(\\hat{p}\\) p_hat &lt;- 55/90 p_hat ## [1] 0.6111111 2. Generate the null distribution Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size \\(n\\) would vary if the null is true. In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2. # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) # plot the null distribution ggplot(null_distribution, aes(x = prop)) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = &#39;white&#39;, stackratio = 0.5) + labs(x = expr(hat(p))) And plot the observed statistic on top, like we did last week. ggplot(null_distribution, aes(x = prop, fill = (prop &gt;= 0.61))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + geom_vline(xintercept = 0.61, color = &#39;tomato1&#39;, size = 1) + labs(x = expr(hat(p)), fill = expr(hat(p) &gt;= 0.61)) 3. Calculate our p-value How surprising is 55 of 90 coin flips? We can compare it against the null distribution. ► Question What is our p-value? The proportion of the null distribution which is \\(\\geq0.61\\) The proportion of the null distribution which is \\(\\geq0.61\\) or \\(\\leq0.39\\) 2 times the proportion of the null distribution which is \\(\\geq0.61\\) or \\(\\leq0.39\\) (whichever is smallest) ► Solution If we perform an hypothesis test for the two-sided alternative \\(p \\neq 0.5\\), we compute it as twice the proportion in the smallest tail, i.e. the tail with the smallest count. pvalue &lt;- null_distribution %&gt;% summarise( pvalue_lefttail = sum(prop &lt;= 0.39) / n(), pvalue_righttail = sum(prop &gt;= 0.61) / n() ) pvalue ## # A tibble: 1 x 2 ## pvalue_lefttail pvalue_righttail ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.018 0.022 0.018 * 2 ## [1] 0.036 ► Question What is our formal decision about our hypotheses? Not enough evidence to reject \\(H_0\\)(\\(H_0:\\) We’re just as likely to get heads as tails when we flip the coin.) Evidence in favour of \\(H_1\\)(\\(H_1:\\) We’re more likely to see either heads or tails when we flip the coin.) ► Solution According to our p-value, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. An analogy Might we have made a mistake here? Is 55 out of 90 coin flips surprising enough for us to reject the hypothesis that the coin is fair? Last week we discussed an analogy in law, in which a person on trial is presumed innocent until proven guilty. Similarly, we presume \\(H_0\\) to be true until there is strong evidence to reject it. How strong must the evidence be? How do we avoid wrongly convicting an innocent person? (i.e., wrongly rejecting an hypothesis which is actually true?) Two different types of errors Person is innocent Person is guilty Verdict = Innocent Correct decision Criminal goes free Verdict = Guilty Wrongful conviction Correct decision \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct Type I errors If the null hypothesis is true, then the sampling distribution of our statistic follows the null distribution which we constructed above, and we will reject (incorrectly) any sample statistic which results in a p-value which is less than or equal to our \\(\\alpha\\) level (e.g., the 0.05 we set earlier). So the probability of making this error is equal to the \\(alpha\\) level which we set. In other words, when the null hypothesis is true, 0.05 (or 5%) of the random samples we could take would result in us rejecting it. A thought experiment there are 20 researchers. each researcher has a perfectly balanced/fair coin. each researcher conducts a statistical test at \\(\\alpha = 0.05\\) to evaluate whether their coin is fair (lands on heads equally as often as it lands on tails). ► Question How many of the researchers’ tests would we expect to result in a type I error? Remember: The probability of making a Type I error is the probability of getting an unlikely sample statistic simply due to chance sampling variation (i.e., we just happen to get a random sample with an unlikely statistic). ► Solution This is similar to asking “What is the probability of observing at least one significant result due to chance sampling variation alone?”. We can work this out.. The probabilty of 1 researcher getting a non-significant result (p-value \\(&gt; 0.05\\)) assuming the null hypothesis is true is 1 minus the probability of them getting a significant result (p-value \\(\\leq 0.05\\)).\\(1 - 0.05 = 0.95\\). If there are 20 researchers, the probability of them all getting non-significant results when their nulls are all true is \\(0.95^{20} = 0.358\\) Conversely, the probability that at least one of them gets a significant result even though all their nulls are true is \\(1 - 0.358 = 0.642\\) Type II errors and Power A type II error is denoted by \\(\\beta\\). This happens when \\(H_0\\) is false, but we do not have enough evidence to reject it. In our table, the columns specify the possible states of the world (\\(H_0\\) is either True or False). In each of these possible worlds, there…. In each of the possible states of the world, there are two potential outcomes of conducting a statistical test (Reject \\(H-0\\) or Don’t reject \\(H_0\\)). We have seen that if \\(H_0\\) is true, then the probability of incorrectly rejecting \\(H_0\\) is \\(\\alpha\\). We have seen that if \\(H_0\\) is false, then the probability of incorrectly failing to reject \\(H_0\\) is \\(\\beta\\). Conversely, the probability of correctly rejecting \\(H_0\\) is \\(1-\\beta\\), and this is known as the statistical power of our test. \\(H_0\\) is True \\(H_0\\) is False Test doesn’t reject \\(H_0\\) Correct1-\\(\\alpha\\) Type II Error(\\(\\beta\\)) Test rejects \\(H_0\\) Type I Error(\\(\\alpha\\)) Correct1-\\(\\beta\\)Power Statistical power Statistical power is the probability of a hypothesis test of finding an effect if there is an effect to be found. Example 2: A biased coin I have a trick coin which is weighted so that it lands on heads 60% of the time (rather than the usual 50% for everyday fair coins). Oh no! Umberto has noticed that whenever we flip a coin I call heads, and I often seem to win. He accuses me of cheating by using a trick coin! I make him an offer: he can flip the coin 50 times in order to decide whether or not it really is a trick coin. ► Question In Umberto’s 50 coin flips, what proportion of heads would lead him to the correct conclusion that the coin is a trick coin? ► Solution If Umberto flips the coin 50 times, under the null hypothesis, he would need to get a sample statistic (\\(\\hat{p}\\)) of .64 or greater # Specify our possible outcomes and their probabilities under the null outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(1/2, 1/2) # generate samples under the null samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution null_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) null_distribution &lt;- null_distribution %&gt;% arrange(prop) %&gt;% mutate(idx = 1:n()) # plot the null distribution ggplot(null_distribution, aes(x = prop, fill = (idx &gt;= 950))) + geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) + scale_fill_manual(values = c(&#39;white&#39;, &#39;tomato1&#39;)) + labs(x = expr(hat(p))) null_distribution %&gt;% filter(idx == 950) ## # A tibble: 1 x 3 ## replicate prop idx ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 92 0.62 950 Things we know so far: The coin is rigged to land on heads on 60% of flips - the true probability of heads is 0.6. If 62% or more of Umberto’s 50 coin flips come up heads, then he will reject his null hypothesis (that the coin is fair) ► Question What’s the probability that Umberto’s 50 flips will come up with 64% or more heads? In other words, what is the power of his test? ► Solution We can actually generate the true sampling distribution because we know the true parameter (i.e., we know that the coin lands on heads 60% of the time). outcomes &lt;- tibble(vals = factor(c(&#39;Heads&#39;, &#39;Tails&#39;))) prob &lt;- c(0.6, 0.4) samples &lt;- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob) # calculate the statistics for each sample to create the null distribution alternative_distribution &lt;- samples %&gt;% group_by(replicate) %&gt;% summarise(prop = sum(vals == &#39;Heads&#39;) / n()) alternative_distribution %&gt;% summarise( prob_crit = sum(prop &gt;= 0.62) / n() ) ## # A tibble: 1 x 1 ## prob_crit ## &lt;dbl&gt; ## 1 0.463 Generalising it When we conduct NHST, we set \\(\\alpha\\). In setting \\(\\alpha\\), we define a critical region under the null distribution. The critical value is the value of the statistic which defines the start of this region. Any statistic more extreme than this will result in rejecting the null hypothesis. If the null hypothesis is false, the probability that we reject the null hypothesis depends on a) how far away from the null hypothesis the true state of the world is, and b) our sample size. For a given effect size (i.e., difference from the null hypothesis), we can compute the power of a test based on a given sample size. This corresponds to the probability of getting a statistic more extreme than the critical value, given a theorised effect size (i.e., a theorised alternative distribution) Overlapping distributions Talk through applet https://istats.shinyapps.io/power/ https://rpsychologist.com/d3/NHST/ 1.2.1 In-class activity 1.3 Lab Q1 - if my trick coin was actually weighted so that it landed on heads 75% of the time, what would the power of Umberto’s test be? For a given effect size (i.e., difference from the null hypothesis), we can compute the power of a test based on a given sample size. However…. we can also decide on what we want the power of a test to be, and ask what the minimum sample size is that is needed to detect a given effect size. Q2.- how many coin flips should Umberto do in order to increase his power to 80%? A haven’t thought about this yet :) "]
]
