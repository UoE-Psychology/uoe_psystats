```{r, echo=FALSE, message=FALSE}
HIDDEN_SOLS=FALSE
set.seed(15732)
ggplot2::theme_set(ggplot2::theme_gray(base_size=13))

library(tidyverse)
library(moderndive)
```

# Type I, Type II errors and Power {#chap-typeerror}


<div class="lo">
#### Instructions {-}

- In this two-hour lab we will go through worked examples in the first hour, and you will attempt to answer some questions in the second hour.
- The Rmarkdown file for this week is [here](https://uoe-psychology.github.io/uoe_psystats/dapr1/labsheets/week_14_practice.Rmd). It contains the code for the worked examples should you wish to follow along, and blank spaces for your answers to the questions in the second hour.


#### Learning outcomes {-}

**LO1.** Revision of hypothesis testing 

**LO2.** Understand Type I and Type II errors

**LO3.** Introduce statistical power

</div>


## Overview of last week

Last week we learned to frame a research question in terms of **null** and **alternative** hypotheses about parameters.  

+ We related the **null hypothesis** ($H_0$) to a 'random chance model' - i.e., if differences or effects that we observe are actually just due to sampling variation. 

+ We generated a distribution for the null hypothesis (the **null distribution**) which reflected how much sample statistics would vary due to chance if the null hypothesis is true (i.e., if there really is no difference/effect). We did this by simulating lots of samples and computing the statistic on each of the samples.  

+ We then compared the observed statistic to the null distribution we had generated. This is equivalent to asking how likely it would be to get our observed statistic if the null hypothesis was actually true. We learned that the **p-value** is the proportion of simulated sample statistics in our null distribution which were as or more extreme than our observed statistic. 

+ Finally, we thought about how we might make a formal decision about *whether or not to reject the null hypothesis* based on our p-value. This was based on a pre-specified level (known as $\alpha$), below which we would consider p-values to be **statistically significant.**   

This week, we will recap this process of hypothesis testing, before thinking about the ways in which this method might lead to error.  

## Walkthrough

### Example 1: Coin flip {-}    



<div class="red">
#### Research Question & Hypotheses {-} 

Is our coin biased? 
</div>

<br><br>
*Null hypothesis*: We're just as likely to get heads as tails when we flip the coin. We will denote by $p$ the probability of 'heads'.
$$H_0: p = 0.5$$
*Alternative hypothesis*: We're more likely to see either heads or tails when we flip the coin.
$$H_1: p \neq 0.5$$

#### Data collection {-} 

We flip the coin 90 times, and it lands on heads 55 times.  
$$H, T, H, H, T, T, T, H, H, H, H, T, H, H, T, T, H, T, H, H,...$$


#### Analysis {-}  

<div class="red">
<br>
**Steps**
<br> 

1. Calculate the sample statistic.  
1. Generate the null distribution.  
1. Calculate the probability of seeing our statistic (or one which is farther away from the null) if the null were true (this is the p-value).  
1. Compare the p-value to our pre-specified $\alpha$-level (for this example we will use the conventional 0.05).  

</div>  

**1. Calculate the sample statistic, $\hat{p}$**  
```{r}
p_hat <- 55/90
p_hat
```


**2. Generate the null distribution**  

<div class="noteBox">
Remember that the null distribution is what we would expect if the null hypothesis were true - it is how much the statistics computed from samples of size $n$ would vary if the null is true.  
In our case, this quantifies how much our statistic (the proportion of heads) in a sample of size 90 would vary if the true probability of the coin landing on heads were 1/2.
</div>

```{r warning=FALSE, message=FALSE}
# Specify our possible outcomes and their probabilities under the null
outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(1/2, 1/2)

# Generate samples under the null
samples <- rep_sample_n(outcomes, size = 90, replace = TRUE, reps = 1000, prob = prob)

# Calculate the statistics for each sample to create the null distribution
null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())
```

Now we can plot our null distribution: 
```{r eval=FALSE}
ggplot(null_distribution, aes(x = prop)) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = 'white', stackratio = 0.5) +
  labs(x = expr(hat(p)))
```

```{r echo=FALSE}
ggplot(null_distribution, aes(x = prop)) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, fill = 'white', stackratio = 0.5) +
  labs(x = expr(hat(p))) +
  geom_label(aes(x = 0.3, y = 0.75, label = "Each dot is the proportion of\nheads in a simulated sample\nof 90 flips"),
             hjust = 0, vjust = 0, lineheight = 0.8, colour = "#555555", fill=NA,
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.35, y = 0.75, xend = 0.42, yend = 0.35), 
                             colour = "#555555", 
                             size=0.5, 
                             curvature = 0.2,
                             arrow = arrow(length = unit(0.03, "npc")))
```


And plot the observed statistic on top, like we did last week: 

```{r eval=FALSE}
ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.61))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  geom_vline(xintercept = 0.61, color = 'tomato1', size = 1) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= 0.61))
```

```{r echo=FALSE}
ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.61))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  geom_vline(xintercept = 0.61, color = 'tomato1', size = 1) +
  labs(x = expr(hat(p)), fill = expr(hat(p) >= 0.61)) + 
  geom_label(aes(x = 0.53, y = 0.75, label = "These dots are at least as extreme\nas our observed statistic"),
             hjust = 0, vjust = 0.5, lineheight = 0.8, colour = "tomato1", fill="white",
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.65, y = 0.70, xend = 0.62, yend = 0.06), 
             colour = "tomato1", size=0.5, curvature = -0.2, 
             arrow = arrow(length = unit(0.03, "npc")))
```


**3. Calculate our p-value**  
<br>
How surprising is 55 heads in 90 coin flips? We can compare it against the null distribution.

`r msmbstyle::question_begin()`
What is our p-value?   

a. The proportion of statistics in the null distribution which are $\geq0.61$.
a. The proportion of statistics in the null distribution which are $\geq0.61$ or $\leq0.39$.  
a. Two times the proportion of statistics in the null distribution which are $\geq0.61$.

`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
If we perform an hypothesis test for the two-sided alternative $p \neq 0.5$, this is the proportion of statistics *at least as extreme in either direction*, so we're looking at both tails of the distribution (i.e., answer B above).
<br>
However, we can assume the distribution to be symmetric, so we can simply multiply the proportion of the null distribution in one tail by two (i.e., answer C above).

```{r echo=FALSE}
ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.61 | prop <=0.39))) +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.5) +
  scale_fill_manual(values = c('white', 'tomato1')) +
  geom_vline(xintercept = 0.61, color = 'tomato1', size = 1) +
  labs(x = expr(hat(p)), fill = expr(abs(hat(p)-0.5) >= 0.11))+
  geom_label(aes(x = 0.53, y = 0.75, label = "These dots are at least as extreme\nas our observed statistic"),
             hjust = 0, vjust = 0.5, lineheight = 0.8, colour = "tomato1", fill="white",
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.65, y = 0.70, xend = 0.62, yend = 0.06),
             colour = "tomato1", size=0.5, curvature = -0.2,
             arrow = arrow(length = unit(0.03, "npc"))) +
  geom_label(aes(x = 0.35, y = 0.95, label = "These dots are at least as extreme\nin the other direction"),
             hjust = 0, vjust = 0.5, lineheight = 0.8, colour = "tomato1", fill="white",
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.35, y = 0.9, xend = 0.38, yend = 0.06),
             colour = "tomato1", size=0.5, curvature = 0.2,
             arrow = arrow(length = unit(0.03, "npc")))
```

Both answers (b) and (c) are correct. As you can see, both methods return very similar estimates of the p-values, with the estimates getting closer and closer as the number of simulated samples increases.  

```{r}
pvalue <- null_distribution %>%
  summarise(
    pvalue_bothtails = sum(prop <= 0.39 | prop>=0.61) / n(),
    pvalue_2righttail = 2 * sum(prop>=0.61) / n()
  )

pvalue
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin()`
At a significance level of $\alpha = 0.05$, what is our formal decision about the hypotheses? 

1. Not enough evidence to reject $H_0$.<br>($H_0:$ We're just as likely to get heads as tails when we flip the coin.)
1. Evidence against $H_0$.<br>

`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
According to the p-value previously found,  0.04 $\leq$ 0.05, we have reason to reject the null hypothesis that our coin will land on heads and tails equally often. 
`r msmbstyle::solution_end()`


### An analogy in law {-}

Might we have made a mistake here? Is 55 out of 90 coin flips *surprising enough* for us to reject the hypothesis that the coin is fair?  
<br>
Last week we discussed an analogy in law, in which a person on trial is presumed innocent until proven guilty. Similarly, we presume $H_0$ to be true until there is strong evidence to reject it. How strong must the evidence be? How do we avoid wrongly convicting an innocent person? (i.e., wrongly rejecting an hypothesis which is actually true?)  


### Two different types of errors {-}

|                          | Person is innocent<br>$H_0$ is True | Person is guilty<br>$H_0$ is False        |
|:-------------------------|:------------------------------------:|:------------------------:|
|**Verdict = Innocent** <br> **Test fails to reject $H_0$**  | Correct decision       | Criminal goes free <br> Type II Error <br> $\beta$      |
|**Verdict =  Guilty** <br> **Test rejects $H_0$**          | Wrongful conviction <br> Type I Error <br> $\alpha$   | Correct decision         |
  
  
<!-- |                          | $H_0$ is True              | $H_0$ is False             | -->
<!-- |:-------------------------|:--------------------------:|:--------------------------:| -->
<!-- |Test doesn't reject $H_0$ | Correct decision           | Type II Error<br>($\beta$) | -->
<!-- |Test rejects $H_0$        | Type I Error<br>($\alpha$) | Correct decision           | -->


### Type I errors {-}  

If the null hypothesis is true, then the **sampling distribution** of our statistic follows the null distribution which we constructed above, and we will reject (incorrectly) any observed statistic which has a corresponding p-value of less than or equal to our $\alpha$ level (e.g., the 0.05 we set earlier). 
<br>
So the probability of making this error is equal to the $\alpha$ level which we set.  
<br>
In other words, when the null hypothesis is true, 0.05 (or 5%) of the random samples we could take would result in us rejecting it.   

`r msmbstyle::question_begin()`
If there are 100 researchers, each testing a fair coin at $\alpha = 0.05$, how many of them will incorrectly reject their null hypothesis that the coin is equally likely to land on heads or tails ($p = 0.5$).
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
We would expect 0.05 (5%) of the 100 researchers' random samples (sequence of coin flips) to have a proportion of heads extreme enough for them to reject the null hypothesis.<br>
We would expect 5 researchers to make a Type I Error.
`r msmbstyle::solution_end()`



### Type II Errors and Power {-}

The other kind of error we might make is a **Type II Error**, and is denoted by $\beta$.<br>
This happens when $H_0$ is false, but we do not have enough evidence to reject it.  
  
In our table, the columns specify the possible states of the world ($H_0$ is either True or False).   
<br>
In each of the possible states of the world, there are two potential outcomes of conducting a statistical test (reject $H_0$ or don't reject $H_0$).  
<br>
We have seen that: 

+ *If $H_0$ is true*, then the probability of incorrectly rejecting $H_0$ is $\alpha$ (often set at 0.05), and the probability of correctly retaining (not rejecting) $H_0$ is 0.95.  
+ *If $H_0$ is false*, then the probability of incorrectly failing to reject $H_0$ is $\beta$, and the probability of correctly rejecting $H_0$ is $1-\beta$. This is known as the **statistical power** of our test.  

|                          | $H_0$ is True          | $H_0$ is False             |
|:-------------------------|:----------------------:|:--------------------------:|
|**Test doesn't reject $H_0$** | Correct<br>**$1 - \alpha$**  | Type II Error <br> $\beta$ |
|**Test rejects $H_0$**        | Type I Error <br> $\alpha$ | Correct<br>**Power** <br> **$1 - \beta$**|

<div class="def">
#### Power of a statistical test {-}

The power of a statistical test ($1-\beta$) is the probability of rejecting the null hypothesis when the alternative hypothesis is true.

In other words, the power of a test is the probability that the test finds an effect if there is an effect to be found.
</div> 

### Example 2: A biased coin {-}

I have a trick coin which is weighted so that it lands on heads 60% of the time (rather than the usual 50% for a normal fair coin). 
<br>
Oh no! Tom has noticed that whenever we flip a coin, I always call heads, and I often seem to win. He accuses me of cheating by using a trick coin which is biased to land on heads!  
I make him an offer: he can flip the coin 50 times in order to decide whether or not it really is a trick coin. 


`r msmbstyle::question_begin()`
Tom's null hypothesis is that he is just as likely to get heads as tails when he flips the coin. 
$$H_0: p = 0.5$$  
Where $p$ is the probability of 'heads', what is his alternative hypothesis?
  
a. $H_1: p \neq 0.5$
a. $H_1: p > 0.5$
a. $H_1: p < 0.5$

`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
Given that Tom thinks the coin is biased towards landing on heads, we will use (b) as his alternative hypothesis: $H_1: p > 0.5$. 
`r msmbstyle::solution_end()`

<div class="red">
<br>
**To calculate the power of Tom's 50 flips to detect whether or not I'm using a trick coin we need to:**
<br> 

1. Generate the null distribution for Tom's test.  
1. Calculate the **critical value** (the minimum number of heads in 50 flips which would lead Tom to reject his null hypothesis).  
1. Calculate the actual probability of seeing a statistic larger than the critical value, given that we know the true bias of the coin. This is the statistical power of Tom's test.  

</div>  

**1. Generate Tom's null distribution**  

In Tom's 50 coin flips, at a significance level of 0.05, what proportion of heads would lead him to the conclusion that the coin is a trick coin? The first step is to generate the null distribution: 
```{r message=FALSE,warning=FALSE}
# Specify our possible outcomes and their probabilities under the null
outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(0.5, 0.5)

# Generate samples under the null
samples <- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob)

# Calculate the statistics for each sample to create the null distribution
null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())
```

**2. Calculating Tom's critical value:**  

Now that we have generated the null distribution which Tom will use to test his observed statistic, we need to work out the values at which he will reject the null.<br>
In other words, we need to work out where the top 5% of the null distribution is.<br>
<br>
Note that we are only looking at the top 5% because Tom thinks the coin is biased *towards heads*. So his alternative hypothesis ($H_1$) is $p > 0.5$, and he will reject $H_0$ if his observed statistic falls in the top end of the distribution.
<br>
To do this, we're going to use a new function called `quantile()`, which will give us the value for which a given percentage of the distribution of simulated sample statistics is to the left (e.g., smaller).  
```{r warning=FALSE,message=FALSE}
crit_val <- 
  null_distribution %>%
  summarise(crit95 = quantile(prop, 0.95)) %>%
  pull(crit95)

crit_val
```
If Tom flips the coin 50 times, under the null hypothesis, he would need to get a sample statistic ($\hat{p}$) of greater than or equal to 0.62.

**3. Calculating Tom's power**

Things we know so far:  

+ The coin is rigged to land on heads on 60% of flips - the true probability of heads is 0.6. 
+ If 62% or more of Tom's 50 coin flips come up heads, then he will reject his null hypothesis (that the coin is fair).

What's the probability that Tom's 50 flips will come up with 62% or more heads using the biased coin? 
<br>
In other words, what is the *power* of his test?

We can do this by generating the sampling distribution for when the coin is biased towards heads 60% of the time (which we know is actually true).
```{r message=FALSE, warning=FALSE}
outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(0.6, 0.4)

samples <- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob)

# Calculate the statistics for each sample to create the true distribution
true_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())

tom_power <- 
  true_distribution %>%
  summarise(
    prob_crit = sum(prop >= crit_val) / n()
  )
tom_power
```

Tom's 50 flips has `r tom_power*100`% power.<br>
What does this mean?<br>
It means that he has a `r tom_power` probability of concluding that my biased coin is biased!  
<br>
It might help if we plotted the two distributions which we just generated.  
We'll start with Tom's null distribution (proportions of heads in 1000 simulated samples of size 50, assuming the probability of heads = 0.5). It's a histogram, with the dotplots on top just to show their equivalence.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(null_distribution, aes(x = prop)) +
  geom_histogram(binwidth = 0.01, fill="grey70") +
  geom_dotplot(binwidth = 0.01, dotsize = 0.5, stackratio = 0.6, fill="white") +
  labs(title = "Tom's null distribution", x = expr(hat(p)), fill = expr(hat(p) >= 0.62))+
  xlim(0.25,0.85)
```

This shows the critical value and region for Tom's one-tailed test at $\alpha = 0.05$.
```{r echo=FALSE, warning=FALSE,message=FALSE}
tnull<- ggplot(null_distribution, aes(x = prop, fill = (prop >= 0.62))) +
  geom_histogram(binwidth = 0.01) +
  scale_fill_manual(values = c('grey70', 'tomato1')) +
  theme(legend.position = "none") +
  xlim(0.25,0.85) +
  labs(title = "Tom's null distribution", x = expr(hat(p))) +
  geom_label(aes(x = 0.4, y = 125, label = "Critical value: Any observed statistic equal to or\ngreater than this, Tom will reject his null hypothesis"),hjust = 0, vjust = 0.5, lineheight = 0.8, fill="white",
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.62, y = 120, xend = 0.62, yend = 0),
             size=0.5, curvature = 0,
             arrow = arrow(length = unit(0.03, "npc")))+
  geom_label(aes(x = 0.25, y = 85, label = "Critical region: These dots are the top 5% of the distribution"),
             hjust = 0, vjust = 0.5, lineheight = 0.8, fill="white", colour = "tomato1",
             label.size = NA, size = 3) +
  geom_curve(aes(x = 0.56, y = 80, xend = 0.65, yend = 16),
             colour = "tomato1", size=0.5, curvature = -0.2,
             arrow = arrow(length = unit(0.03, "npc")))
  

tnull
```

Finally, let's add to that the *true distribution* which we generated based on our knowledge that the coin is actually biased towards landing on heads 60% of the time. Here it is (in green):
```{r echo=FALSE, warning=FALSE,message=FALSE}
tnull + 
  geom_histogram(inherit.aes=FALSE, data=true_distribution, aes(x=prop), binwidth = 0.01, fill="darkolivegreen4", alpha=.4) +
  labs(title = "Tom's null distribution & the true distribution")

```
  
Think about what this shows. For a coin which is biased towards landing on heads 60% of the time, more often than not, the proportion of heads in 50 flips will fall below 0.62 (which is Tom's critical value - i.e., the minimum value which would result in him rejecting the null hypothesis).<br>
In fact, the probability of 50 flips of the trick coin resulting in a proportion of heads large enough for Tom to reject the null is about `r tom_power*100`%.<br>
This is the proportion of the true sampling distribution (green, above) which is equal to or greater than the critical value (black vertical line).
  

### Overlapping distributions {-}

We've just seem a visualisation of two different possible sampling distributions, one reflecting the null hypothesis, and one reflecting an alternative (in this case, it reflected the *true* bias of the coin). It helps to think of the statistical power of a test in terms of the *overlap* of two such distributions. 

`r msmbstyle::question_begin()`
How will our choice of $\alpha$ influence our statistical power? (Try explaining it in terms of the two distributions we saw above)
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
A lower $\alpha$ means that we need stronger evidence to reject the null hypothesis.<br>
We have less power when we have a lower $\alpha$ because the critical value is further from the center of the null distribution (i.e., more extreme). Less of the alternative distribution will be equal or greater than a more extreme critical value.  
`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`
How will our sample size influence our statistical power? (Try explaining it in terms of the two distributions we saw above)
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
A bigger sample means a smaller standard error (see [Week 11](#chap-sampling-distributions)).<br>
This would make both sampling distributions (that under the null hypothesis, and that under an alternative hypothesis) *narrower*, and therefore overlap less, leading to more power. 
`r msmbstyle::question_end()`

`r msmbstyle::question_begin()`
How would it influence our statistical power if we assumed the coin was actually rigged to land 70% of the time on heads? (Try explaining it in terms of the two distributions we saw above)
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
When there is greater difference between the null distribution and an hypothesised distribution, the two distributions will overlap less, leading to more power. 
`r msmbstyle::question_end()`

A nice visualisation of power can be found here: https://rpsychologist.com/d3/NHST/ 

### Generalising it {-}  

In our example, we *knew* the true bias of the coin. But Tom didn't!   
<br>
What Tom *could* do, is calculate the power of his test *assuming* a given value for $p$. He might have thought to himself:<br>
"Hmm, that coin seems to land on heads about 3/4 of the time. If that is true, I want to know what the probability of me being able to correctly reject the null hypothesis is when I flip the coin 50 times".   
  
<small>Had Tom used the R code above he might have called his distribution `hypothesised_distribution` rather than `true_distribution`!</small>   
<br>

<div class="red">
  
<br>

+ When we conduct NHST (Null Hypothesis Significance Testing), we set $\alpha$.  
+ In setting $\alpha$, we define a **critical region** under the null distribution. The **critical value** is the value of the statistic which defines the start of this region. Any statistic more extreme than this will result in rejecting the null hypothesis.  
+ If the null hypothesis is false, the probability that we reject the null hypothesis depends on a) our $\alpha$ level, b) how far away from the null distribution the assumed state of the world is, and c) our sample size.  
+ This means that, for an assumed **effect size** (i.e., difference from the null hypothesis) - or *set of* effect sizes - we can compute the power of a test for a given sample size.  
We can then do useful things such as reframe this to find out what the minimum sample size is that would be required to achieve a given level of power to detect a given effect size.  
  
</div>


## Summary

Today, we have recapped what we learned last week about hypothesis testing, and introduced some key ideas.
<br>
We learned that when we reject a *true* null hypothesis because we happen (by random chance) to have a sample statistic which is unlikely under the null, we commit a Type I Error. We saw how the probability of this happening is equal to our $\alpha$ level.  
<br>
We also learned that failing to reject the null hypothesis when it is actually false, is known as a Type II Error, and the probability of this depends on how far away from the null parameter the true parameter is (i.e., effect size), as well as how big our sample is.  
<br>
We simulated different distributions under different hypotheses, using the code below: 
```{r, eval=FALSE}
# Our possible outcomes
outcomes <- tibble(vals = factor(c('Heads', 'Tails'))) 

# Our probabilities (reflecting the hypothesis)
prob <- c(0.5, 0.5)

# Our simulated samples of size 50
samples <- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob)

# Our distribution
distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())
```

We calculated the critical value (for a one-sided test) for a null distribution by using the `quantile()` function, to find the values at which 95% of the distribution of simulated sample statistics are to the left.
```{r, eval=FALSE}
crit_val <- 
  null_distribution %>%
  summarise(crit95 = quantile(prop, 0.95)) %>%
  pull(crit95)
```

For a two-sided test at $\alpha = 0.05$, we can simply extend this to ask for the values where 2.5% of the distribution is to the left, and where 97.5% of the distribution is to the left (so 2.5% is to the right). In doing so, we divide our critical region in to the two tails of the distribution.
```{r}
null_distribution %>%
  summarise(
    crit025 = quantile(prop, 0.025),
    crit975 = quantile(prop, 0.975)
  )
```

We can find the proportion of a different theorised distribution which fell beyond a critical value, giving our power:
```{r eval=FALSE}
another_distribution %>%
  summarise(
    prob_crit = sum(prop >= crit_val) / n()
  )
```

---

## Lab

### Exercise 1: Rock Paper Scissors!

If you haven't ever played Rock, Paper, Scissors before, then play it now against someone nearby! 
<br>
A [paper in 2009](https://www.jstor.org/stable/25653686?) found that novice players tend to avoid choosing scissors. We are going to test this.  
<br>

<div class="red">
#### Research Question {-}
Are novice players of Rock, Paper, Scissors biased to choosing scissors less often?
</div>


`r msmbstyle::question_begin(header = "&#x25BA; Question 1")`
Write out your null and alternative hypotheses. Is our alternative hypothesis one-sided or two-sided?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
Null hypothesis: Novice players are equally likely to choose Scissors as they are to choose Rock or Paper. 
Alternative hypothesis: Novice players are less likely to choose Scissors as they are to choose Rock or Paper. 
<br>
Where $p$ denotes the proportion of games in which players choose scissors:  
$$H_0: p = 1/3$$
$$H_1: p < 1/3$$
  
Our alternative hypothesis is one-sided, because we are suggesting that the proportion of games in which players choose scissors is *less than* 1/3. 
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(header = "&#x25BA; Question 2")`
Assuming that novice players actually choose Scissors only 1/5 of the time ($p = 1/5$), calculate the power to detect this if we conduct a test (at $\alpha = 0.05$) based on 75 games of Rock, Paper, Scissors with novice players. 
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
We need to generate our null distribution:
```{r, message=FALSE, warning=FALSE}
outcomes <- tibble(vals = factor(c('Rock','Paper','Scissors')))
prob <- c(1/3, 1/3, 1/3)

# GENERATE THE NULL
samples <- rep_sample_n(outcomes, size = 75, replace = TRUE, reps = 1000, prob = prob)

null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Scissors') / n())
```

And calculate the critical value (note that because the alternative is $p < 1/3$, we are looking for the bottom 5%, rather the top 5%):
```{r}
# FIND CRITICAL VALUE
crit_val <- 
  null_distribution %>%
  summarise(crit95 = quantile(prop, 0.05)) %>%
  pull(crit95)
```

Construct an alternative distribution:
```{r}
# GENERATE ALTERNATIVE
outcomes <- tibble(vals = factor(c('Rock','Paper','Scissors')))
prob <- c(0.4, 0.4, 0.2)

samples <- rep_sample_n(outcomes, size = 75, replace = TRUE, reps = 1000, prob = prob)

hypothesised_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Scissors') / n())
```

And calculate power.
```{r}
# CALCULATE POWER
hypothesised_distribution %>%
  summarise(
    prob_crit = sum(prop <= crit_val) / n()
  )
```
`r msmbstyle::solution_end()`


`r msmbstyle::question_begin(header = "&#x25BA; Question 3")`
Calculate the power if we were to conduct the same test at $\alpha = 0.01$ instead.
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
We would have a different critical value:
```{r}
crit_val <- 
  null_distribution %>%
  summarise(crit95 = quantile(prop, 0.01)) %>%
  pull(crit95)
```
And our power would therefore change accordingly:
```{r}
# CALCULATE POWER
hypothesised_distribution %>%
  summarise(
    prob_crit = sum(prop <= crit_val) / n()
  )
```
`r msmbstyle::solution_end()`

### Exercise 2: Calculating power for a different coin.

`r msmbstyle::question_begin(header = "&#x25BA; Question 4")`
If my trick coin was actually weighted so that it landed on heads 65% of the time, what would the power of Tom's test (50 flips) be? 
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
We already know the critical value at which Tom will reject the null hypothesis based on his 50 flips. 
We need to:
<br>
Generate the theoretical distribution of $\hat(p) = 0.65$. 
```{r message=FALSE, warning=FALSE}
#Remember that Tom's critical value is:
crit_val <- 0.62

outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(0.65, 0.35)

samples <- rep_sample_n(outcomes, size = 50, replace = TRUE, reps = 1000, prob = prob)

theoretical_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())
```
Calculate what proportion of this distribution is greater than his critical value. This will be the power.
```{r}
theoretical_distribution %>%
  summarise(
    prob_crit = sum(prop >= crit_val) / n()
  )
```
`r msmbstyle::solution_end()`


## Extra Exercises

### Exercise 3: Sample size (number of flips) and power  

`r msmbstyle::question_begin(header = "&#x25BA; Question 5")`
Assuming the coin to be biased towards landing on heads 60% of the time, calculate the power of Tom's statistical test for when he flips the coin 75 times, 100 times, and 200 times. Which sample size should Tom use if he wants at least 80% power?
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
We can run this chunk changing the `nr_flips` value each time, to calculate power at different sample sizes.  
```{r message=FALSE, warning=FALSE}
nr_flips <- 50

outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(0.5, 0.5)

# GENERATE THE NULL
samples <- rep_sample_n(outcomes, size =nr_flips, replace = TRUE, reps = 1000, prob = prob)

null_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())

# FIND CRITICAL VALUE
crit_val <- 
  null_distribution %>%
  summarise(crit95 = quantile(prop, 0.95)) %>%
  pull(crit95)

# GENERATE THEORISED ALTERNATIVE
outcomes <- tibble(vals = factor(c('Heads', 'Tails')))
prob <- c(0.6, 0.4)

samples <- rep_sample_n(outcomes, size = nr_flips, replace = TRUE, reps = 1000, prob = prob)

hypothesised_distribution <- samples %>% 
  group_by(replicate) %>%
  summarise(prop = sum(vals == 'Heads') / n())

# CALCULATE POWER
hypothesised_distribution %>%
  summarise(
    prob_crit = sum(prop >= crit_val) / n()
  )
```
`r msmbstyle::solution_end()`

`r msmbstyle::question_begin(header = "&#x25BA; Question 6")`
List the three things which affect the power of a statistical test. 
`r msmbstyle::question_end()`
`r msmbstyle::solution_begin(hidden=FALSE)`
  
+ Alpha level
+ Sample size
+ Effect size

`r msmbstyle::solution_end()`


<div class="noteBox"> 
#### A thought experiment {-} 

+ There are 20 researchers. 
+ Each researcher has a perfectly balanced/fair coin.  
+ Each researcher conducts a statistical test at $\alpha = 0.05$ to evaluate whether their coin is fair (lands on heads equally as often as it lands on tails).

How many of the researchers' tests would we expect to result in a Type I Error?  
<small>**Remember:** The probability of making a Type I error is the probability of getting an unlikely sample statistic simply due to chance sampling variation (i.e., we just happen to get a random sample with an unlikely statistic).</small>
`r msmbstyle::solution_begin(hidden=FALSE)`

This is similar to asking "What is the probability of observing at least one significant result due to chance sampling variation alone?". We can work this out..
<br>  

+ For one researcher, if their null hypothesis is true, the probability that they get a significant result is 0.05, and the probability that they get a non-significant result (p-value $> 0.05$) is 0.95.

+ If there are 20 researchers, the probability of them *all* getting *non-significant* results when their null hypotheses are all true is $0.95^{20} = 0.358$.  

+ This means that the probability of the opposite - *at least one* of them gets a significant result even though all their nulls are true - is $1 - 0.358 = 0.642$.  

`r msmbstyle::solution_end()`
</div>



## Glossary

- *Type I Error.* Rejecting the null hypothesis when it is actually true (false positive). Its probability is denoted by $\alpha$.
- *Type II Error.* Failing to reject a null hypothesis that is actually false (false negative). Its probability is denoted by $\beta$.
- *Power ($1 - \beta$).* The probability of (correctly) rejecting the null hypothesis when it is false.
- *Critical Region*. The area of the null distribution in which an observed sample statistic would lead to rejecting the null hypothesis. The area of the critical region corresponds to $\alpha$% of the null distribution.
- *Critical value*. The value of the statistic which defines the start of the critical region. Any statistic more extreme than this will result in rejecting the null hypothesis.
- *Effect size.* The distance from the value of the parameter under the null hypothesis. When we calculate power, we often do so for an *assumed* effect size. When we take a sample and calculate an observed statistic, the distance from that observed statistic to the parameter under the null is our observed effect size.



## References  

- Eyler, D., Shalla, Z., Doumaux, A., & McDevitt, T. (2009). Winning at Rock-Paper-Scissors. *The College Mathematics Journal*, 40(2), 125-128.
- Tintle, N., Chance, B. L., Cobb, G. W., Rossman, A. J., Roy, S., Swanson, T., & VanderStoep, J. (2015). *Introduction to statistical investigations.* New York: Wiley.
