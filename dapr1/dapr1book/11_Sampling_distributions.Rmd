```{r, echo=FALSE}
HIDDEN_SOLS=FALSE

TOGGLE_WALK=TRUE
set.seed(1)
```



# Sampling distributions {#chap-sampling-distributions}

<div class = "lo">
#### Learning outcomes {-}

**LO1.** Understand the difference between a population parameter and a sample statistic.

**LO2.** Understand that a sampling distribution shows how sample statistics vary from sample to sample.

**LO3.** Understand the effect of sample size on the sampling distribution, and how to quantify the variability of a statistic. 

#### Reading {-}
This week's reading is [Chapter 7](https://moderndive.com/7-sampling.html) of the book by Chester Ismay and Albert Y. Kim. *Statistical Inference via Data Science: A ModernDive into R and the Tidyverse*. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/
</div>


## Recap

```{r data-analysis-pipeline, echo=FALSE, fig.align='center', out.width='75%', fig.cap='The data analysis pipeline.'}
knitr::include_graphics('https://d33wubrfki0l68.cloudfront.net/86cc45e87bb755a3bcecce462a6524e68d13a466/67469/images/r4ds/data_science_pipeline.png')
```

In Semester 1 (weeks 1-10), we started walking through the required steps of a statistical investigation, conveniently summarized in Figure \@ref(fig:data-analysis-pipeline).

Suppose that you are interested in a research question that can be answered by collecting data on some statistical units. 
Once collected, you (a) import/load the data into R; 
(b) tidy them so that each column corresponds to a single variable of interest; 
(c) transform variables if needed; 
(d) visualise your data and inspect for unusual values; 
(e) fit statistical models to the data; and 
(f) communicate your results and conclusions to the wider community.

`r msmbstyle::question_begin(header = "&#x25BA; Note")`
The inner cycle (c-d-e) might need to be re-iterated a few times.
`r msmbstyle::question_end()`

In the first semester we saw:

(a) how to import data into R;
(b) how to tidy datasets (for example by making sure that some variables are factors);
(c) how to transform variables (e.g. standardizing via `scale()`, log-transforming data, ...).
(d) how to visualise the data using the `ggplot2` package.

In this semester we will focus on

(e) modelling, and 
(f) communicating our findings to the wider community.



## Population vs sample {#population-sample}

Typically, it is either infeasible in terms of time or cost to perform an exhaustive data collection on the entire population of interest (also known as **census**). This is why, for instance, the UK Office for National Statistics only performs a census every 10 years and the next one will be in 2021.
To save time and money, we typically record the variables of interest on a smaller subset of the entire population, also known as a **sample**.
In order to make sure that any conclusions we draw from the sample are **generalizable** to the wider population, this sample needs to be taken at **random**.
This avoids **representation bias**, where some units are less represented than others, which would lead to wrong conclusions for the entire population.

`r msmbstyle::question_begin(header = "&#x25BA; Example")`
**Average montly salary in Sweden** \label(average-salary-sweden)

Suppose you are interested in the average monthly salary of people working in Sweden. Unfortunately, you neither have the time nor the money to go to Sweden and ask each single person his/her own salary.
Hence, you decide to ask some people at random.

What are the problems of the following sample selection criteria?

1. Asking 500 random people from Facebook that live in Sweden.
2. Asking 1000 random people from the web that live in Sweden.
3. Calling 200 phone numbers from the telephone directory.
4. Asking 500 people working near the central bank of Sweden.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=FALSE, toggle=TOGGLE_WALK)`
All four criteria lead to samples that are not representative of the entire population. This is because some people will have higher chance of being included in the sample than others, leading to representation bias. Because of this, any conclusion we might make from the sample is not generalizable from the whole population.

1. People that do not have Facebook are not considered in the sample.
2. People that do not use internet are not represented.
3. People without a landline phone are not included.
4. People working in this area will very likely have higher salaries than the rest of the country. This sample does not represent fairly people working in farming or other sectors.
`r msmbstyle::solution_end()`


In [Week 8](#week-8) of Semester 1, we discussed how to obtain sample data from a bigger population of interest; this is known as data collection.
We now take the opposite direction as we try to use the information from the sample data to draw conclusions about the entire population.
Furthermore, we will spend some time assessing how accurate our conclusions about the entire population are.


<div class = "def">
#### Statistical inference {-}

**Statistical inference** is the process of using the sample data to draw conclusions about the entire population.
</div>

```{r, echo=FALSE, fig.align='center', out.width = "75%", fig.cap = "Data collection vs statistical inference"}
knitr::include_graphics("./images/statistical_inference.png")
```


`r msmbstyle::question_begin(header = "&#x25BA; Caution")`
The inferential process **assumes** that the sample is randomly drawn from the population of interest.
Any sample selection method that is biased, i.e. leading to samples which are not representative of the entire population, will mean that the results we obtain from the data in the sample can not be generalized to the entire population.
`r msmbstyle::question_end()`

In the average salary of Sweden example, none of the four sample selection strategies consisted in random sampling.
They systematically excluded some people from the samples, leading to samples that were not repersentative of the population.
For this reason, the average salary computed on any sample could not have been used to draw conclusions about the average salary of swedish people.



## Population parameter vs sample statistic {#parameter-statistic}

To make it easier to understand whether we are referring to the entire population or a sample, we use the term *parameter* when referring to a numerical summary of the entire population, and the term *statistic* for a numerical summary of the sample.

<div class = "def">
#### Parameter vs statistic {-}

- A **parameter** is a number describing some aspect of the population.
- A **statistic** is a number that is computed from the data in a sample.
</div>

`r msmbstyle::question_begin(header = "&#x25BA; Example")`
**Average price of goods sold by ACME Corporation**

Suppose you work for a company that is interested in buying ACME Corporation^[You might remember it from the cartoon Wile E. Coyote and the Road Runner.] and your boss wants to know within the next hour what is the average price of goods sold by that company and how different the prices are.

Since ACME Corportation has such a big mail ordering catalogue, see Figure \@ref(fig:acme), we will assume that the company sells many products. Furthermore, we only have the catalogue in paper-form and no website or online list of prices is available.
```{r acme, out.width='50%', fig.align='center', echo=FALSE, fig.cap="Product catalogue of ACME corporation"}
knitr::include_graphics('https://i.pinimg.com/474x/e5/bc/52/e5bc52b78be204a8d6d1f4932492ea3d--bugs-bunny-coyote.jpg')
```

1. Identify the population of interest and the population parameters.
2. Can we compute the parameters within the next hour?
3. How would you proceed in estimating the population parameters if you just had time to read through 100 item descriptions? Would you pick the first 100 items or would you pick 100 random page numbers?
4. State which statistics you would use to estimate the population parameters.
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin(hidden=FALSE, toggle=TOGGLE_WALK)`
1. The population of interest is all products sold by ACME Corportation. The population parameters are the mean price $\mu$ and the standard deviation $\sigma$.
2. Because the catalogue has so many pages, we can not compute the population parameters within the next hour.
3. We must estimate the population mean and standard deviation from a sample of size $n = 100$. We should choose the items entering the sample at random, to avoid sampling bias. If we were to choose 100 consecutive items, we might end up with a very good estimate of the average price for the category those contecutive items belong to (e.g. gardening). However, this would not be a good estimate of the overall price across the multiple categories of products sold.
4. Our best guess of the population mean would be the sample mean, $\bar{x}$, and our guess of the population standard deviation would be the sample standard deviation, denoted $s$ or $\hat{\sigma}$.
`r msmbstyle::solution_end()`

From this example, you can see that the population parameter and the sample statistic generally have the same name.
However, these are often written with different symbols to convey with just one letter:

1. what *feature* they represent;
2. if it is a *population* quantity or a quantity *computed on a sample*.

The following table summarizes standard notation for some population parameters, typically unknown, and the corresponding "best guesses" computed on a sample.


|                   | Population parameter   | Sample statistic         |
|:------------------|:----------------------:|:------------------------:|
|Mean               | $\mu$                  | $\bar{x}$ or $\hat{\mu}$ |
|Standard deviation | $\sigma$               | $s$ or $\hat{\sigma}$    |
|Proportion         | $p$                    | $\hat{p}$                |

Table: Notation for common parameters and statistics.


- The greek letter $\mu$ (mu) is used as a parameter to denote the population mean/average, while $\bar{x}$ or $\hat{\mu}$ (mu-hat) as a statistic for the mean computed on a sample.
- The greek letter $\sigma$ (sigma) is used as a parameter to denote the population standard deviation, while $s$ or $\hat{\sigma}$ (sigma-hat) as a statistic for the standard deviation of the collected sample.
- The letter $p$ is used as a parameter to denote the population proportion, while $\hat{p}$ (p-hat) as a statistic for the sample proportion.



`r msmbstyle::question_begin(header = "&#x25BA; Example")`
**Proportion of UK people aged between 25 and 34 with a Bachelor's degree or higher**

The last UK Census, done in 2011, reports that 40% of people aged 25 to 34 years had a degree-level or above qualification.
Suppose that in a random sample of $n = 200$ UK residents who are between 25 and 34 years old, 58 of them have a Bachelor's degree or higher. 
Using the appropriate notation, state what is the population parameter and what is the sample statistic.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=FALSE, toggle=TOGGLE_WALK)`

- The population parameter is the proportion of *all* UK people aged between 25 to 34 years old with a Bachelor's degree or higher: $p = 0.4$.
- The sample statistic is the proportion with a Bachelor's degree or higher *for those in the sample*: $\hat{p} = 58/200 = 0.29$.

`r msmbstyle::solution_end()`


As discussed, it is generally infeasible to be able to know the value of the population parameter *exactly*. This would require collecting data for the entire population and then computing the required quantity.
Instead, we typically select a random sample from the population, and then compute the quantity of interest for the sample data.
We then use this sample statistic as a *(point) estimate* or *best guess* of the population parameter.


## Sampling distributions {#sec-sampling-distributions}

A population parameter is typically considered to be a **fixed** value.
A sample statistic, however, **varies from sample to sample**.
This is because a sample statistic, i.e. a quantity computed on a sample, will depend on which units are selected to enter the sample.
This can be seen as a downside to sampling, but we must remember that time and resources often prevent us from finding out the true value of a population parameter, whereas it is comparatively easy to collect a sample and compute a statistic.

A fundamental question that arises when we estimate an unknown population parameter by a sample statistic is: how accurate do we believe our best guess to be?

Remembering that the parameter is fixed, while the statistics varies from sample to sample, we might proceed in answering this question by looking at how the computed statistic varies depending on the sample.


`r msmbstyle::question_begin(header = "&#x25BA; Example")`
**Average yearly salary of American National Football League (NFL) players**

We will read a file containing the yearly salaries (in millions of dollars) for all players being paid at the start of 2015 by a National Football League (NFL) team.
This entire dataset represents the population of all National Football League players in 2015.^[Of course a population might change over time, as people can enter or leave at any time, so you might wonder why did we say that a population parameter is fixed. Because of the large number of units in the entire population, it is reasonable to assume that the addition of comparatively few units to the entirety leads to a negligible change in the population parameter.]

We are interested in the following question: what is the average yearly salary of a NFL player in 2015?

In this particular example, we actually **do** know the population parameter, because we have data on the whole population. We resort to sampling, however, to show how the value of a statistic varies when computed on different samples.

1. Read in the data and state, with appropriate notation, what is the population parameter.
2. Select a random sample of $n = 50$ players and compute the average yearly salary for the players in the sample. How does your statistic compare to the population parameter?
3. Take another sample of size $n = 50$ players and compute the average salary for the new sample. How does it compare with the mean from the previous sample?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=FALSE, toggle=TOGGLE_WALK)`
Do not worry if some of these functions are new to you, try to follow along and understand what each does.

**Question 1.**

Let us start by loading the data, inspecting the initial rows of the tibble, and checking the dimensions:
```{r, message=FALSE}
library(tidyverse)

nfl <- read_tsv('https://edin.ac/2TexAFA')
head(nfl)
dim(nfl)
```

The data comprise 5 measurements on 2099 units. We have data on the player's name, position, the team name, the total money they received while playing on the NFL (cumulative over multiple years) and the yearly salary.

Since we are only interested in the yearly salary of the players, we can select only the relevant columns from the data:
```{r}
nfl <- nfl %>% 
  select(Player, YearlySalary)

head(nfl)
```


We now calculate the average yearly salary for all players:
```{r}
nfl_mean <- nfl %>%
  summarise(avg = mean(YearlySalary))

nfl_mean
```

The population parameter is the average yearly salary in 2015, $\mu =$ `r nfl_mean %>% pull(avg) %>% round(2)`.


**Question 2.**

In order to randomly sample players, we will use the package `moderndive`. If you do not have it installed, run the following command:

```{r, eval=FALSE}
install.packages("moderndive")
```

We then load the package and create a sample of size $n = 50$ players:
```{r, message = FALSE, warning=FALSE}
library(moderndive)

nfl_sample_1 <- nfl %>%
  rep_sample_n(size = 50)

nfl_sample_1
```

This returns a tibble of size $50 \times 3$. The second column shows the names of the players included in the sample. The additional first column is equal to 1 for all cases in the sample. This indicates that the 50 rows all belong to the first sample. You are also invited to interactively scroll through the sample by typing the command `View(nfl_sample_1)`.

Let's now compute the average salary for the players in the sample:
```{r}
nfl_sample_1_mean <- nfl_sample_1 %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_1_mean
```

We can see that the average salary in our sample is $\bar{x} =$ `r nfl_sample_1_mean %>% pull(avg) %>% round(2)` million dollars.

The sample mean, `r nfl_sample_1_mean %>% pull(avg) %>% round(2)`, is close to the population mean, `r nfl_mean %>% pull(avg) %>% round(2)`, even if not exactly the same. We are not surprised of this result: we do not expect the mean of every sample to be exactly equal to the population mean, but we do hope that they are somewhat close.


**Question 3.**

Let's take another random sample of size 50 and compute the mean salary:
```{r}
nfl_sample_2 <- nfl %>%
  rep_sample_n(size = 50)

nfl_sample_2
```

```{r}
nfl_sample_2_mean <- nfl_sample_2 %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_2_mean
```

The statistic computed on the second sample is $\bar{x} =$ `r nfl_sample_2_mean %>% pull(avg) %>% round(2)`. Again, this is similar to the population parameter, $\mu =$ `r nfl_mean %>% pull(avg) %>% round(2)`.
We also note that the mean computed on the second sample is different from the mean computed on the first sample.

We could have immediately obtained two samples, each of size $n = 50$ units. This requires that we repeat/replicate two times the activity of sampling 50 units from the entire population. Every replicated sampling of 50 units from the population is done "from scratch", so that the population is always unchanged from replicate to replicate. 
This is done in the function `rep_sample_n(size = 50)` by including the extra argument `reps = 2`:

```{r}
nfl_samples <- nfl %>%
  rep_sample_n(size = 50, reps = 2)

nfl_samples
```

If you explore this tibble, it has $50 \times 2 = 100$ rows. The colum replicate takes value 1 for the first 50 rows, and the value 2 for the next 50 rows.
This indicates that the players in rows 1 to 50 are selected to be in the first sample, while the players in rows 51 to 100 are those selected to be in the second sample.

We can now compute the mean yearly salary for each of the two samples. The salaries of the 50 players in the first sample (replicate = 1) will be summarised by a single mean value, and the salaries of the 50 players in the second sample (replicate = 2) will be summarised into another mean value. In the end we will have a tibble with just two rows:
```{r}
nfl_sample_means <- nfl_samples %>%
  group_by(replicate) %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_means
```

We see that both are close to the population parameter which, we remind, is equal to `r nfl_mean %>% pull(avg) %>% round(2)`.
`r msmbstyle::solution_end()`



Clearly, we can extend the repeated sampling procedure to more than just two samples of size = 50.
Let us now obtain 2000 replicated samples, all of size 50, from the same NFL population:
```{r}
nfl_samples <- nfl %>%
  rep_sample_n(size = 50, reps = 2000)

nfl_samples
```

This tibble has $50 \times 2000 = 100,000$ rows. The first 50 players are part of the 1st sample, the next 50 belong to the 2nd sample, and so on...

We can now compute the mean of each of the 2000 samples of size 50, obtaining a tibble of 2000 sample means:
```{r}
nfl_sample_means <- nfl_samples %>%
  group_by(replicate) %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_means
```

Let us plot the distribution of the sample mean for 2000 random samples, and superimpose in red the true value of the population parameter $\mu$:
```{r sampling-distribution, fig.cap="Sampling distribution of the mean for $n = 50$ with population mean $\\mu$ marked by a red vertical line."}
ggplot(nfl_sample_means, aes(x = avg)) +
  geom_histogram(color = "white") +
  geom_vline(xintercept = pull(nfl_mean, avg), color = "red", size = 1) +
  labs(x = expression(bar(x)))
```

Figure \@ref(fig:sampling-distribution) shows the values of the sample mean computed from sample to sample. Hence, it shows the variability of the sample mean induced by sampling variation.
Such a plot is fundamental in statistical inference, and it is called **sampling distribution**. Each of these dots is one of the sample means in the tibble `nfl_sample_means`.

<div class = "def">
#### Sampling distribution {-}

The **sampling distribution** shows the distribution of the statistic for different samples of the same size from the same population.
</div>

Clearly, we can compute sampling distributions for other statistics too: the proportion, the standard deviation, ...

This requires the following steps:

1. Obtaining multiple samples, all of the same size, from the same population;
2. For each sample, calculate the value of the statistic;
3. Plot the distribution of the computed statistics.


In Figure \@ref(fig:sampling-distribution), the distribution of sample means is centred around the population mean $\mu$ and has a shape that is similar to a bell. This can be generalised further:


<div class = "def">
#### Centre and shape of a sampling distribution {-}

- **Centre:** If samples are randomly selected, the sampling distribution will be centred around the population parameter.
- **Shape:** For most of the statistics we consider, if the sample size is large enough, the sampling distribution will be symmetric and bell-shaped. (**Central Limit Theorem**)

</div>


## The standard error of a statistic {#standard-error}

The variability, or spread, of the sampling distribution shows how much the sample statistics tend to vary from sample to sample.
This is key in understanding how accurate our estimate of the population parameter, based on just one sample, will be.

In Semester 1 you saw how to summarize the variability of data using the standard deviation: `sd()`.
So, the variability of a statistic can be quantified by calculating the standard deviation of its sampling distribution.

Technically, this is not different from an ordinary standard deviation as the code and formula is the same. However, the spread of a sample statistic is so fundamental to have its own name: **the standard error of the statistic**.

In other words, we use:

- **standard deviation** to denote the variability among the values in a particular sample;
- **standard error** to denote the variability of the statistics computed on many samples.


<div class = "def">
#### Standard error {-}

The **standard error** of a statistic, denoted $SE$, is the standard deviation of its sampling distribution.
</div>

The standard error measures the *typical error* when estimating the population parameter with the sample statistic. You can think of the SE as a "typical distance" of a sample statistic from the population parameter.


## The effect of sample size on the sampling distribution {#sample-size-standard-error}

It is of interest to see how the sampling distribution of the mean salary, shown in Figure \@ref(fig:sampling-distribution) for a sample size $n = 50$, changes with the sample size.


In the following code chunk we compute the sampling distribution of the mean for samples of size $n = 50$, $n = 100$, and $n = 500$, always repeating the sampling from the population 2000 times:

```{r}
nfl_sample_means_n_50 <- nfl %>%
  rep_sample_n(size = 50, reps = 2000) %>%
  group_by(replicate) %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_means_n_100 <- nfl %>%
  rep_sample_n(size = 100, reps = 2000) %>%
  group_by(replicate) %>%
  summarise(avg = mean(YearlySalary))

nfl_sample_means_n_500 <- nfl %>%
  rep_sample_n(size = 500, reps = 2000) %>%
  group_by(replicate) %>%
  summarise(avg = mean(YearlySalary))
```


We now combine the datasets for different sample sizes into a unique tibble, adding a column with the sample size, and then plot the distributions for different sample sizes.
Remember that `mutate()` takes a tibble and adds or changes a column. The function `bind_rows()` takes multiple tibbles and stacks them under each other.
```{r sampling-distribution-vary-n, fig.height=8, fig.align='center', fig.cap="Three sampling distributions of the mean, with population mean $\\mu$ marked by a red vertical line."}
nfl_sample_means_vary_n <- bind_rows(
  nfl_sample_means_n_50 %>% mutate(n = 50),
  nfl_sample_means_n_100 %>% mutate(n = 100),
  nfl_sample_means_n_500 %>% mutate(n = 500)
)

ggplot(nfl_sample_means_vary_n, aes(x = avg)) +
  geom_histogram(color = "white") +
  geom_vline(xintercept = pull(nfl_mean, avg), color = "red", size = 1) +
  facet_grid(rows = vars(n), labeller = label_both) +
  labs(x = expression(bar(x)), title = "Distribution of the sample mean for samples of size 50, 100, 500")
```

Figure \@ref(fig:sampling-distribution-vary-n) shows that as the sample size increases, the variability of the sampling distributions decreases, hence **the standard error of the statistic decreases as the sample size increases**.

We can create a tibble that shows, for each sample size, the standard error of the sample mean:
```{r}
nfl_sample_means_vary_n %>%
  group_by(n) %>%
  summarise(SE = sd(avg))
```

As we discussed, the tibble shows that as the sample size $n$ increases, the standard error $SE$ decreases.

The larger the sample size, the lower the typical error of our estimate will be, and for every sample we will obtain a calculated statistic that is more similar to the population parameter.



## Lab: Hollywood movies

The following code chunk reads in data from over 900 Hollywood movies produced between 2007 and 2013. Consider it as the entire population of movies produced in Hollywood in that time period.

```{r, message=FALSE}
hollywood <- read_tsv('https://edin.ac/2N9yHms')
hollywood
```

Among the recorded variables, three will be of interest

- Movie: the movie title;
- Genre: which genre the movie belongs to;
- Budget: budget to produce the movie.



`r msmbstyle::question_begin()`
**Extracting relevant variables**

Extract from the `hollywood` tibble the 3 variables of interest and keep the movies for which we have all information (no missing entries).
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
We can extract variables using the function `select()`, while to keep the rows for which we have all measurements we use `na.omit()`:
```{r}
hollywood <- hollywood %>%
  select(Movie, Genre, Budget) %>%
  na.omit
hollywood
```
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Proportion of comedy movies**

What is the population proportion of comedy movies?
What is an estimate of the proportion of comedy movies using a sample of size 20?
Using the appropriate notation, report your results in one or two sentences.
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
prop_comedy <- hollywood %>%
  summarise(prop = mean(Genre == "Comedy")) %>% 
  pull(prop)
prop_comedy

sample_prop_comedy <- hollywood %>%
  rep_sample_n(size = 20) %>%
  summarise(prop = mean(Genre == "Comedy")) %>%
  pull(prop)
sample_prop_comedy
```

The population proportion of comedy movies is $p =$ `r prop_comedy %>% round(2)`, while the proportion of comedy movies in the sample is $\hat{p} =$ `r sample_prop_comedy %>% round(2)`.
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Sampling distributions**

What is a sampling distribution?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
The sampling distribution is the distribution of the values that a statistic takes on different samples of the same size and from the same population.
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Sampling distribution of the proportion**

Compute the sampling distribution of the proportion of comedy movies for sample size $n = 20$, using 1000 replicates. Is it centred at the population value?
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r fig.cap="Sampling distribution of the proportion for $n = 20$ with population parameter $p$ marked by a red vertical line."}
sample_props <- hollywood %>%
  rep_sample_n(size = 20, reps = 1000) %>% 
  group_by(replicate) %>%
  summarise(prop = mean(Genre == 'Comedy'))

ggplot(sample_props, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy, color = 'red', size = 1) +
  labs(x = expression(hat(p)))
```

Yes, the distribution is almost bell-shaped and centred at the population parameter.
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Standard error**

Using the replicated samples from the previous question, what is the standard error of the sample proportion of comedy movies?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`

The standard error of the sample proportion is simply the standard deviation of the distribution of sample proportions for many samples. Since we have already computed the proportions for 1000 samples in the previous question, we just have to compute their variability using the standard deviation:
```{r}
se_prop <- sample_props %>%
  summarise(SE = sd(prop)) %>% 
  pull(SE)

se_prop
```

The standard error of the sample proportion for sample size $n = 20$, based on 1000 replicated samples, is $SE(\hat{p}) =$ `r se_prop %>% round(2)`.
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**The effect of sample size on the standard error of the sample proportion**

How does the sample size affect the standard error of the sample proportion?
Compute the sampling distribution for the proportion of comedy movies using samples of size $n = 20$, $n = 50$, $n = 200$ respectively and 1000 replicated samples.
`r msmbstyle::question_end()`


`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r, fig.height=8, fig.align='center', fig.cap="Three sampling distributions of the proportion, with population parameter $p$ marked by a vertical red line."}
sample_props_20 <- hollywood %>%
  rep_sample_n(size = 20, reps = 1000) %>% 
  group_by(replicate) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_50 <- hollywood %>%
  rep_sample_n(size = 50, reps = 1000) %>% 
  group_by(replicate) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_200 <- hollywood %>%
  rep_sample_n(size = 200, reps = 1000) %>% 
  group_by(replicate) %>%
  summarise(prop = mean(Genre == 'Comedy'))

sample_props_vary_n <- bind_rows(
  sample_props_20 %>% mutate(n = 20),
  sample_props_50 %>% mutate(n = 50),
  sample_props_200 %>% mutate(n = 200)
)

ggplot(sample_props_vary_n, aes(x = prop)) +
  geom_histogram(color = 'white') +
  geom_vline(xintercept = prop_comedy, color = 'red', size = 1) +
  facet_grid(rows = vars(n), labeller = label_both) +
  labs(x = expression(hat(p)), title = "Sampling distribution of the proportion for samples of size 20, 50, 200")
```

From this plot we can see that, as the sample size increases, the standard error of the sample proportion decreases. Increasing the sample size, the spread of the statistic values is reduced.
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Comparing the budget for action and comedy movies**

What is the population average budget (in millions of dollars) allocated for making action vs comedy movies? And the standard deviation?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
budgets <- hollywood %>%
  filter(Genre == 'Action' | Genre == 'Comedy') %>%
  group_by(Genre) %>%
  summarise(avg_budget = mean(Budget),
            sd_budget = sd(Budget))
budgets
```

From the above tibble we see that action movies have been allocated a higher budget ($\mu_{Action} =$ `r budgets$avg_budget[1] %>% round(1)`) than comedy movies ($\mu_{Comedy} =$ `r budgets$avg_budget[2] %>% round(1)`). At the same time, action movies have a higher variability of budgets around the mean value ($\sigma_{Action} =$ `r budgets$sd_budget[1] %>% round(1)` vs $\sigma_{Comedy} =$ `r budgets$sd_budget[2] %>% round(1)`).
`r msmbstyle::solution_end()`



`r msmbstyle::question_begin()`
**Estimate the average difference in budget for action and comedy movies**

Estimate the average difference in budget for action vs comedy movies, using a sample of size 100. How does it compare to the population difference?
`r msmbstyle::question_end()`

`r msmbstyle::solution_begin(hidden=HIDDEN_SOLS)`
```{r}
sample_mean_budget <- hollywood %>%
  filter(Genre == 'Action' | Genre == 'Comedy') %>%
  rep_sample_n(size = 100) %>%
  group_by(Genre) %>%
  summarise(avg_budget = mean(Budget))
sample_mean_budget

est_diff <- pull(sample_mean_budget, avg_budget)[1] - pull(sample_mean_budget, avg_budget)[2]
est_diff
```

The estimated difference in mean budget is $\bar{x}_{Action} - \bar{x}_{Comedy} =$ `r sample_mean_budget$avg_budget[1] %>% round(1)` - `r sample_mean_budget$avg_budget[2] %>% round(1)` = `r (sample_mean_budget$avg_budget[1] - sample_mean_budget$avg_budget[2]) %>% round(1)`.
The population difference in budget can be calculated from the previous question, and is equal to $\mu_{Action} - \mu_{Comedy} =$ `r budgets$avg_budget[1] %>% round(1)` - `r budgets$avg_budget[2] %>% round(1)` = `r (budgets$avg_budget[1] - budgets$avg_budget[2]) %>% round(1)`.
The two values are not exactly equal, as we have estimated the parameter with a sample of size 100.
`r msmbstyle::solution_end()`



## Summary

In Section \@ref(parameter-statistic) we have reviewed the difference between a population parameter and a statistic computed on a sample [**LO1**]. 
Because of the sampling criteria, which select units at random, those included in the sample vary when the sampling procedure is repeated. 
The distribution of the values that a sample statistics takes on the different samples is called sampling distribution, and has been defined in Section \@ref(sec-sampling-distributions) [**LO2**]. The spread of a statistic gives an idea of how close our estimate of the unknown population parameter is to the population value. Hence, lower variability means a better guess. We quantified the variability of a statistic with its standard error, defined as the standard deviation of the statistic's sampling distribution, and in Section \@ref(sample-size-standard-error) we saw that as the sample size increases, the standard error decreases [**LO3**].



## Glossary

- *Statistical inference.* The process of drawing conclusions about the population from the data collected in a sample.
- *Population.* The entire collection of units of interest.
- *Sample.* A subset of the entire population.
- *Random sample.* A subset of the entire population, picked at random, so that any conclusion made from the sample data can be generalized to the entire population.
- *Representation bias.* Happens when some units of the population are systematically underrepresented in samples.
- *Generalizability.* When information from the sample can be used to draw conclusions about the entire population. This is only possible if the sampling procedure leads to samples that are representative of the entire population (such as drawn at random).
- *Parameter.* A fixed but typically unknown quantity describing the population.
- *Statistic.* A quantity computed on a sample.
- *Sampling distribution.* The distribution of the values that a statistic takes on different samples of the same size and from the same population.
- *Standard error.* The standard error of a statistic is the standard deviation of the sampling distribution of the statistic.


## References

- Chester Ismay and Albert Y. Kim. *Statistical Inference via Data Science: A ModernDive into R and the Tidyverse*. Chapman and Hall/CRC, 2019. Freely available online at: https://moderndive.com/

- Robin H. Lock et al. *Statistics: unlocking the power of data*. Wiley, 2013.
